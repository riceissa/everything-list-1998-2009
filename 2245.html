<?xml version="1.0" encoding="us-ascii"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii" />
<meta name="generator" content="hypermail 2.3.0, see http://www.hypermail-project.org/" />
<title>Fwd: Re: PhD-thesis on Observational Selection Effects from Nick Bostrom on 2000-10-19 (everything)</title>
<meta name="Author" content="Nick Bostrom (nick.domain.name.hidden)" />
<meta name="Subject" content="Fwd: Re: PhD-thesis on Observational Selection Effects" />
<meta name="Date" content="2000-10-19" />
<style type="text/css">
/*<![CDATA[*/
/* To be incorporated in the main stylesheet, don't code it in hypermail! */
body {color: black; background: #ffffff}
dfn {font-weight: bold;}
pre { background-color:inherit;}
.head { border-bottom:1px solid black;}
.foot { border-top:1px solid black;}
th {font-style:italic;}
table { margin-left:2em;}map ul {list-style:none;}
#mid { font-size:0.9em;}
#received { float:right;}
address { font-style:inherit ;}
/*]]>*/
.quotelev1 {color : #990099}
.quotelev2 {color : #ff7700}
.quotelev3 {color : #007799}
.quotelev4 {color : #95c500}
.period {font-weight: bold}
</style>
</head>
<body>
<div class="head">
<h1>Fwd: Re: PhD-thesis on Observational Selection Effects</h1>
<!-- received="Thu Oct 19 17:31:16 2000" -->
<!-- isoreceived="20001020003116" -->
<!-- sent="Thu, 19 Oct 2000 20:14:18 -0400" -->
<!-- isosent="20001020001418" -->
<!-- name="Nick Bostrom" -->
<!-- email="nick.domain.name.hidden" -->
<!-- subject="Fwd: Re: PhD-thesis on Observational Selection Effects" -->
<!-- id="4.3.2.7.2.20001019201336.04893db8.domain.name.hidden" -->
<!-- charset="us-ascii" -->
<!-- expires="-1" -->
<map id="navbar" name="navbar">
<ul class="links">
<li>
<dfn>This message</dfn>:
[ <a href="#start2245" name="options1" id="options1" tabindex="1">Message body</a> ]
 [ More options (<a href="#options2">top</a>, <a href="#options3">bottom</a>) ]
</li>
<li>
<dfn>Related messages</dfn>:
<!-- unext="start" -->
[ <a href="2246.html" accesskey="d" title="Marchal: &quot;Re: PhD-thesis on Observational Selection Effects&quot;">Next message</a> ]
[ <a href="2244.html" title="Nick Bostrom: &quot;Fwd: Re: PhD-thesis on Observational Selection Effects&quot;">Previous message</a> ]
<!-- unextthread="start" -->
<!-- ureply="end" -->
</li>
</ul>
</map>
<ul class="links">
<li><a name="options2" id="options2"></a><dfn>Contemporary messages sorted</dfn>: [ <a href="date.html#msg2245" title="Contemporary messages by date">by date</a> ] [ <a href="index.html#msg2245" title="Contemporary discussion threads">by thread</a> ] [ <a href="subject.html#msg2245" title="Contemporary messages by subject">by subject</a> ] [ <a href="author.html#msg2245" title="Contemporary messages by author">by author</a> ] [ <a href="attachment.html" title="Contemporary messages by attachment">by messages with attachments</a> ]</li>
</ul>
</div>
<!-- body="start" -->
<div class="mail">
<address class="headers">
<span id="from">
<dfn>From</dfn>: Nick Bostrom &lt;<a href="mailto:nick.domain.name.hidden?Subject=Re%3A%20Fwd%3A%20Re%3A%20PhD-thesis%20on%20Observational%20Selection%20Effects">nick.domain.name.hidden</a>&gt;
</span><br />
<span id="date"><dfn>Date</dfn>: Thu, 19 Oct 2000 20:14:18 -0400</span><br />
</address>
<br />
Jacques wrote:
<br />
<br /><em class="quotelev2">&gt;&gt;From: &quot;Nick Bostrom&quot; &lt;nick.bostrom.domain.name.hidden&gt;
</em><br />
<em class="quotelev2">&gt;&gt;Jacques Mallah wrote some time back:
</em><br />
<em class="quotelev2">&gt;&gt;[see chapter 8 of <a href="http://www.anthropic-principle.com/phd/">http://www.anthropic-principle.com/phd/</a>]
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev1">&gt;&gt; &gt;&gt;&gt;If he believes the MWI however, then he knows there is a branch
</em><br />
<em class="quotelev1">&gt;&gt; &gt;&gt;&gt;where a deer turns up and one where it doesn't.  He knows that for a 
</em><br />
<em class="quotelev2">&gt;&gt; usual day the effective probability of a deer turning up is about 1%.
</em><br />
<em class="quotelev2">&gt;&gt;Since he has no reason to believe the laws of nature are specially 
</em><br />
<em class="quotelev2">&gt;&gt;configured to correlate the amplitude of the wavefunction's branches with 
</em><br />
<em class="quotelev2">&gt;&gt;his actions (especially since such correlation would be hard to reconcile 
</em><br />
<em class="quotelev2">&gt;&gt;with QM), he will still believe the effective probability of a deer 
</em><br />
<em class="quotelev2">&gt;&gt;turning up is just 1%.
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev4">&gt;&gt; &gt;&gt;You may be confusing subjective and objective probability.
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev3">&gt;&gt; &gt;    Nope.  Why would you think I might do so?
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt;Adam might have reason to think that the objective chance of Deer is 1%, 
</em><br />
<em class="quotelev2">&gt;&gt;and nonetheless have reason assign a 90% (say) credence to Deer. Compare 
</em><br />
<em class="quotelev2">&gt;&gt;the situation to the case of a coin which has just been tossed in what 
</em><br />
<em class="quotelev2">&gt;&gt;you think is a fair manner; so you think there was a 50% chance of Heads. 
</em><br />
<em class="quotelev2">&gt;&gt;But suppose you have cought a brief glimps of the coin after it landed, 
</em><br />
<em class="quotelev2">&gt;&gt;and it looked like Tails, although you aren't absolutely sure. So your 
</em><br />
<em class="quotelev2">&gt;&gt;subjective credence may be 95%.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;    Yeah no shit.  Of course I know about Bayesian probabilities.
</em><br />
<em class="quotelev1">&gt;    You still haven't suggested any reason as to why you might have 
</em><br />
<em class="quotelev1">&gt; thought that I was confusing them.
</em><br />
<em class="quotelev1">&gt;    Remember that effective probability is the fraction of observers, in a 
</em><br />
<em class="quotelev1">&gt; given situation, that see that outcome.  For QM this is equal to the sum 
</em><br />
<em class="quotelev1">&gt; of the squares of the amplitudes of those branches consistent with that 
</em><br />
<em class="quotelev1">&gt; outcome, as long as no observers are created or destroyed and the 
</em><br />
<em class="quotelev1">&gt; measurement is definitely made.
</em><br />
<br />Even assuming that Adam is 100% certain about the truth of MWI (which, of 
<br />
course, there is absolutely no need for me to suppose), and even assuming 
<br />
he knows that for a typical day the effective probability of Deer is 1%, it 
<br />
is still not in general the case that he should think that there is a 1% 
<br />
probability of Deer that morning. The reason why I was suspecting that you 
<br />
were confusing objective and subjective probabilities is that I thought you 
<br />
were claiming that Adam should assign a credence of 1% to deer turning up. 
<br />
If you admit that Adam should believe that the probability of Deer (giving 
<br />
him forming the right intentions) is very great even though he knows that 
<br />
the effective probability of Deer on a typical day is only 1%, then my 
<br />
ground for suspecting this confusion vanishes.
<br />
<br /><br /><em class="quotelev3">&gt;&gt; &gt;    In both cases I have so far discussed, his Bayesian probaility
</em><br />
<em class="quotelev3">&gt;&gt; &gt;distribution for the objective probability is sharply peaked about &gt;a 
</em><br />
<em class="quotelev2">&gt;&gt; (stochastic xor effective) probability of 1%.
</em><br />
<em class="quotelev3">&gt;&gt; &gt;    A common way this might occur is if he has observed, over the
</em><br />
<em class="quotelev2">&gt;&gt;course of 10 years (3652 days) that about 37 times a deer has turned up.
</em><br />
<em class="quotelev2">&gt;&gt;If he assumes that there is a fixed probability p, and initially has a 
</em><br />
<em class="quotelev2">&gt;&gt;uniform Bayesian distribution for p on (0,1), then his final distribution 
</em><br />
<em class="quotelev2">&gt;&gt;will be sharply peaked about 1%.
</em><br />
<em class="quotelev3">&gt;&gt; &gt;    The point, here, is that in such a case he *can't* suddenly assume 
</em><br />
<em class="quotelev2">&gt;&gt; &quot;today is different, so while on a usual day p=.01, I'll just have a 
</em><br />
<em class="quotelev2">&gt;&gt; uniform Bayesian prior for p_today.&quot;, and then apply the &quot;Adam paradox&quot;.
</em><br />
<em class="quotelev2">&gt;&gt;So, in both the non-MWI and the MWI case, p~=.01 is his prior probability 
</em><br />
<em class="quotelev2">&gt;&gt;before he considers his own situation regarding reproduction, but the 
</em><br />
<em class="quotelev2">&gt;&gt;effect of the
</em><br />
<em class="quotelev3">&gt;&gt; &gt;latter is different.  So far I think you agree with that.
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt;Yes, I think that's right so far. But prior probability is not the same 
</em><br />
<em class="quotelev2">&gt;&gt;as objective probability. If there is a wounded deer in the
</em><br />
<em class="quotelev2">&gt;&gt;neighborhood, then the objective chance may be quite high (say, 78%).
</em><br />
<em class="quotelev2">&gt;&gt;But Adam doesn't know whether there is such a deer in the neighboorhood, 
</em><br />
<em class="quotelev2">&gt;&gt;so his subjective credence that the objective chance is that high, is low 
</em><br />
<em class="quotelev2">&gt;&gt;(say 0.1%). So he shouldn't think that a deer will turn up; that has a 
</em><br />
<em class="quotelev2">&gt;&gt;low prior probability. But when he forms the appropriate reproductive 
</em><br />
<em class="quotelev2">&gt;&gt;intentions, then (assuming SSA with a universal reference class), he 
</em><br />
<em class="quotelev2">&gt;&gt;obtains reason to think that the objective chance is actually quite high. 
</em><br />
<em class="quotelev2">&gt;&gt;This also results in his subjective credence in Deer increases.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;    That's only without the MWI.  (i.e. True only for stochastic 
</em><br />
<em class="quotelev1">&gt; probabilities, not for effective probabilities.)
</em><br />
<br />No, it's true whether those objective probabilities are given my MWI or 
<br />
some other physical theory. Nowhere in what I was saying was I presupposing 
<br />
that the objective probabilities come from MWI.
<br />
<br /><br /><em class="quotelev2">&gt;&gt;(Note that I'm talking about an objective chance here which varies from 
</em><br />
<em class="quotelev2">&gt;&gt;day to day, depending on what the deer in the region are up to. The more 
</em><br />
<em class="quotelev2">&gt;&gt;deer nearby, the greater the objective physical chance that some will 
</em><br />
<em class="quotelev2">&gt;&gt;pass by Adam's cave within a certain time interval.)
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;    In the MWI, btw, there would be little such variation since one must 
</em><br />
<em class="quotelev1">&gt; sum over the branches desribing the various deer activities.
</em><br />
<br />Over time, the objective probabilities might get smeared out into a general 
<br />
deer-fog over the whole region. But at the beginning of the world, the 
<br />
objective deer-probability will be concentrated where the initial 
<br />
conditions say that the deer start out.
<br />
<br /><br /><em class="quotelev4">&gt;&gt; &gt;&gt;And I do find the premise that he can be certain that no
</em><br />
<em class="quotelev4">&gt;&gt; &gt;&gt;type of MWI can be true hard to swallow.
</em><br />
<em class="quotelev4">&gt;&gt; &gt;&gt;
</em><br />
<em class="quotelev4">&gt;&gt; &gt;&gt;He wouldn't have to be certain about that.
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev3">&gt;&gt; &gt;    Well, he would certainly have to assign a ludicrously high
</em><br />
<em class="quotelev3">&gt;&gt; &gt;probability (e.g. 50%) to the idea that the MWI might be false.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev2">&gt;&gt;2. Adam might not know contemporary physics.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;    Irrelevant.
</em><br />
<br />No, that's very relevant, because if Adam does not know contemporary 
<br />
physics then he has no reason to think that MWI is true, and then he would 
<br />
have no reason not to assign such a &quot;ludicrously high&quot; probability as 50% 
<br />
to the idea that the MWI might be false.
<br />
<br /><br /><em class="quotelev3">&gt;&gt; &gt;    As I see it, it is a priori possible that I could have been any
</em><br />
<em class="quotelev3">&gt;&gt; &gt;observer.  Thus all observers must be included, by definition.
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt;I find the &quot;I could have been you&quot; talk quite suspicious and murky. It's 
</em><br />
<em class="quotelev2">&gt;&gt;not clear to me that this is the way to cast light on the situtation.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;    Let me try to enlighten you a little then.  Think of it like this: I 
</em><br />
<em class="quotelev1">&gt; know I'm an observer-moment (or thought, if you like); that much I can 
</em><br />
<em class="quotelev1">&gt; assume a priori.  Now I (or my brain, which is the computer actually 
</em><br />
<em class="quotelev1">&gt; carrying out the calculations, with observer-moments like me &quot;along for 
</em><br />
<em class="quotelev1">&gt; the ride&quot;) want to compare two possible models of the universe, so I need 
</em><br />
<em class="quotelev1">&gt; to calculate the Bayesian probability that each model is true.
</em><br />
<em class="quotelev1">&gt;    (In my view, first I must get the prior for this from Occam's razor, 
</em><br />
<em class="quotelev1">&gt; or &quot;exp(-complexity)&quot;; pretend for this exercise that it results in just 
</em><br />
<em class="quotelev1">&gt; two models with significant prior probability and that each of these is 
</em><br />
<em class="quotelev1">&gt; roughly 50% likely a priori.  Even if you don't like that, in any case 
</em><br />
<em class="quotelev1">&gt; assume I have two competing models of equal a priori probability.  I can 
</em><br />
<em class="quotelev1">&gt; always just find the conditional Bayesian probability that model #1 is 
</em><br />
<em class="quotelev1">&gt; true given that either #1 or #2 is true; that's more or less what 
</em><br />
<em class="quotelev1">&gt; scientists traditionally do, since they neglect the simplest [AUH] models.)
</em><br />
<em class="quotelev1">&gt;    For that I need to know the Bayesian probability that, if a given 
</em><br />
<em class="quotelev1">&gt; model is true, the other information that I take as known would also be true.
</em><br />
<em class="quotelev1">&gt;This other information takes the form &quot;I see x&quot;.  Since
</em><br />
<em class="quotelev1">&gt;this is not used as a priori information, it will allow me to update my 
</em><br />
<em class="quotelev1">&gt;prior.  But I need the Bayesian probabilities that the information would 
</em><br />
<em class="quotelev1">&gt;be true if each model were true.
</em><br />
<em class="quotelev1">&gt;    So what is the Bayesian probability that I would see x if model #1 
</em><br />
<em class="quotelev1">&gt; were true?  In order to get it, I do not first assume that I see x, since 
</em><br />
<em class="quotelev1">&gt; then I would get 1 and that's not what I need.  So the only information I 
</em><br />
<em class="quotelev1">&gt; assume is the a priori information, I am an observer-moment.  That is the 
</em><br />
<em class="quotelev1">&gt; only other thing I know about myself, other than the observation that I 
</em><br />
<em class="quotelev1">&gt; see x.  So if model #1 predicts the existance of N observer-moments, m of 
</em><br />
<em class="quotelev1">&gt; whom see x, I have no a priori reason to say that any of them is more 
</em><br />
<em class="quotelev1">&gt; likely to be me than the others.  So the Bayesian probability that &quot;I 
</em><br />
<em class="quotelev1">&gt; would see x&quot; if model #1 were true is the effective probability, m / N.
</em><br />
<br />Well, if we assume that it is a priori knowledge that I am an 
<br />
observer-moment, then why should this a priori knowledge not be used in 
<br />
evaluating hypotheses? For example by saying: it would be more probable 
<br />
that I should exit if many observer-moments came into existence? (This is 
<br />
the Self-Indication Assumption, which as you know I reject. But I would be 
<br />
interested in hearing your story of why it should be rejected.)
<br />
<br />Second, suppose I say to you: Not only &quot;I exist (am an observer-moment).&quot; 
<br />
is a priori, but &quot;I exist and I'm currently thinking about something 
<br />
related to anthropic reasoning.&quot; is also a priori. And for the same reason: 
<br />
you couldn't possibly have found out otherwise. Then by reasoning you 
<br />
describe, the reference class would not consist of all observer-moments but 
<br />
instead of all observer-moments thinking about something related to 
<br />
anthropic reasoning. What do you say about that? (Incidentally, I am 
<br />
recently drawn to a definition of the reference class which might look 
<br />
rather like that.)
<br />
<br /><br /><br />Dr. Nick Bostrom
<br />
Department of Philosophy
<br />
Yale University
<br />
Homepage: <a href="http://www.nickbostrom.com">http://www.nickbostrom.com</a>
<br />
<span id="received"><dfn>Received on</dfn> Thu Oct 19 2000 - 17:31:16 PDT</span>
</div>
<!-- body="end" -->
<div class="foot">
<map id="navbarfoot" name="navbarfoot" title="Related messages">
<ul class="links">
<li><dfn>This message</dfn>: [ <a href="#start2245">Message body</a> ]</li>
<!-- lnext="start" -->
<li><dfn>Next message</dfn>: <a href="2246.html" title="Next message in the list">Marchal: "Re: PhD-thesis on Observational Selection Effects"</a></li>
<li><dfn>Previous message</dfn>: <a href="2244.html" title="Previous message in the list">Nick Bostrom: "Fwd: Re: PhD-thesis on Observational Selection Effects"</a></li>
<!-- lnextthread="start" -->
<!-- lreply="end" -->
</ul>
<ul class="links">
<li><a name="options3" id="options3"></a><dfn>Contemporary messages sorted</dfn>: [ <a href="date.html#msg2245" title="Contemporary messages by date">by date</a> ] [ <a href="index.html#msg2245" title="Contemporary discussion threads">by thread</a> ] [ <a href="subject.html#msg2245" title="Contemporary messages by subject">by subject</a> ] [ <a href="author.html#msg2245" title="Contemporary messages by author">by author</a> ] [ <a href="attachment.html" title="Contemporary messages by attachment">by messages with attachments</a> ]</li>
</ul>
</map>
</div>
<!-- trailer="footer" -->
<p><small><em>
This archive was generated by <a href="http://www.hypermail-project.org/">hypermail 2.3.0</a>
: Fri Feb 16 2018 - 13:20:07 PST
</em></small></p>
</body>
</html>
