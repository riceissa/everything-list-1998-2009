<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1" />
<meta name="generator" content="hypermail 2.3.0, see http://www.hypermail-project.org/" />
<title>Re: How would a computer know if it were conscious? from David Nyman on 2007-06-29 (everything)</title>
<meta name="Author" content="David Nyman (david.nyman.domain.name.hidden)" />
<meta name="Subject" content="Re: How would a computer know if it were conscious?" />
<meta name="Date" content="2007-06-29" />
<style type="text/css">
/*<![CDATA[*/
/* To be incorporated in the main stylesheet, don't code it in hypermail! */
body {color: black; background: #ffffff}
dfn {font-weight: bold;}
pre { background-color:inherit;}
.head { border-bottom:1px solid black;}
.foot { border-top:1px solid black;}
th {font-style:italic;}
table { margin-left:2em;}map ul {list-style:none;}
#mid { font-size:0.9em;}
#received { float:right;}
address { font-style:inherit ;}
/*]]>*/
.quotelev1 {color : #990099}
.quotelev2 {color : #ff7700}
.quotelev3 {color : #007799}
.quotelev4 {color : #95c500}
.period {font-weight: bold}
</style>
</head>
<body>
<div class="head">
<h1>Re: How would a computer know if it were conscious?</h1>
<!-- received="Fri Jun 29 11:18:02 2007" -->
<!-- isoreceived="20070629181802" -->
<!-- sent="Fri, 29 Jun 2007 16:17:38 +0100" -->
<!-- isosent="20070629151738" -->
<!-- name="David Nyman" -->
<!-- email="david.nyman.domain.name.hidden" -->
<!-- subject="Re: How would a computer know if it were conscious?" -->
<!-- id="b0b263660706290817i50062cdayf9fd7a2ab943156c.domain.name.hidden" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="ccaef73f0064aa9557c4563eba72baf1.domain.name.hidden" -->
<!-- expires="-1" -->
<map id="navbar" name="navbar">
<ul class="links">
<li>
<dfn>This message</dfn>:
[ <a href="#start13681" name="options1" id="options1" tabindex="1">Message body</a> ]
 [ More options (<a href="#options2">top</a>, <a href="#options3">bottom</a>) ]
</li>
<li>
<dfn>Related messages</dfn>:
<!-- unext="start" -->
[ <a href="13682.html" accesskey="d" title="Jason: &quot;Justifying the Theory of Everything&quot;">Next message</a> ]
[ <a href="13680.html" title="LauLuna: &quot;Re: Penrose and algorithms&quot;">Previous message</a> ]
[ <a href="13679.html" title="Bruno Marchal: &quot;Re: How would a computer know if it were conscious?&quot;">In reply to</a> ]
<!-- unextthread="start" -->
[ <a href="13449.html" accesskey="t" title="Colin Hales: &quot;Re: How would a computer know if it were conscious?&quot;">Next in thread</a> ]
<!-- ureply="end" -->
</li>
</ul>
</map>
<ul class="links">
<li><a name="options2" id="options2"></a><dfn>Contemporary messages sorted</dfn>: [ <a href="date.html#msg13681" title="Contemporary messages by date">by date</a> ] [ <a href="index.html#msg13681" title="Contemporary discussion threads">by thread</a> ] [ <a href="subject.html#msg13681" title="Contemporary messages by subject">by subject</a> ] [ <a href="author.html#msg13681" title="Contemporary messages by author">by author</a> ] [ <a href="attachment.html" title="Contemporary messages by attachment">by messages with attachments</a> ]</li>
</ul>
</div>
<!-- body="start" -->
<div class="mail">
<address class="headers">
<span id="from">
<dfn>From</dfn>: David Nyman &lt;<a href="mailto:david.nyman.domain.name.hidden?Subject=Re%3A%20How%20would%20a%20computer%20know%20if%20it%20were%20conscious%3F">david.nyman.domain.name.hidden</a>&gt;
</span><br />
<span id="date"><dfn>Date</dfn>: Fri, 29 Jun 2007 16:17:38 +0100</span><br />
</address>
<br />
On 29/06/07, Bruno Marchal &lt;marchal.domain.name.hidden&gt; wrote:
<br />
BM:  I am not sure that in case of disagreement (like our &quot;disagreement&quot;
<br />
with Torgny), changing the vocabulary is a good idea. This will not
<br />
make the problem going away, on the contrary there is a risk of
<br />
introducing obscurity.
<br />
<br />DN:  Yes. this seems to be the greater risk.  OK, in general I'll try to
<br />
avoid it where possible.  I've taken note of the correspondences you
<br />
provided for the senses of 'consciousness' I listed, and the additional one.
<br />
<br /><br />BM:  Actually the elementary grasp are decomposable (into number relations)
<br />
in the comp setting.
<br />
<br />DN:  Then are you saying that 'action' can occur without 'sense' - i.e. that
<br />
'zombies' are conceivable?  This is what I hoped was avoided in the
<br />
intuition that 'sense' and 'action' are, respectively, 1-p and 3-p aspects
<br />
abstracted from a 0-p decomposable self-relation. The zombie then becomes
<br />
merely a category error.  I thought that in COMP, number relations would be
<br />
identified with this decomposable self-relation.  Ah.....but by
<br />
'decomposable', I think perhaps you mean that there are of course
<br />
*different* number relations, so that this would then entail that there is a
<br />
set of such fundamental relations such that *each* relation is individually
<br />
decomposable, yes?
<br />
<br />BM:  OK, but the machine cannot know that. As we cannot know that).
<br />
<br />DN:  Do you mean that the machine can't know for sure the correspondence
<br />
between its conscious world and the larger environment in which this is
<br />
embedded and to which it putatively relates? Then I agree of course, and as
<br />
you say, neither can we, for the sufficient reasons you have articulated.
<br />
So what I meant was that it would simply be in the same position that we
<br />
are, which seems self-evident.
<br />
<br />Anyway, as I said, the original post was probably ill advised, and I retract
<br />
my quibbles about your terminology.
<br />
<br />As to my point about whether such an outcome is likely vis-a-vis an AI
<br />
program, it wasn't of course because you made any claims on this topic, but
<br />
stimulated by another thread.  My thought goes as follows.  I seem to have
<br />
convinced myself that, on the COMP assumption that *I* am such a machine, it
<br />
is possible for other machines to instantiate conscious computations.
<br />
Therefore it would be reasonable for me to attribute consciousness to a
<br />
machine that passed certain critical tests, though not such that I could
<br />
definitely know or prove that it was conscious.  Nonetheless, such quibbles
<br />
don't stop us from undertaking some empirical effort to develop machines
<br />
with consciousness.  Two ways of doing this seem apparent.  First, to copy
<br />
an existing such system (e.g. a human) at an appropriate substitution level
<br />
(as in your notorious gedanken experiment).  Second, to arrange for some
<br />
initial system to undergo a process of 'psycho-physical' evolution (as
<br />
humans have done) such that its 'sense' and 'action' narratives
<br />
'self-converge' on a consistent 1p-3p interface, as in our own case.
<br />
<br />In either of these cases, 'sense' and 'action' narratives 'self-converge',
<br />
rather than being 'engineered', and any imputation of consciousness ( i.e.
<br />
the attribution of semantics to the computation) continues to be 1p
<br />
*self-attribution*, not a provable or definitely knowable 3p one.  The
<br />
problem then seems to be: is there in fact a knowable method to 'design' all
<br />
this into a system from the outside: i.e. a way to start from an external
<br />
semantic attribution (e.g. an AI program) and then 'engineer' the sense and
<br />
action syntactics of the instantiation in such a way that they converge on a
<br />
consistent semantic interpretation from either 1p or 3p pov?  IOW, so that a
<br />
system thus engineered would be capable of passing the same critical tests
<br />
achievable by the first two types.  I can't see that we possess even a
<br />
theory of how this could be done, and as somebody once said, there's nothing
<br />
so practical as a good theory.  This is why I expressed doubt in the
<br />
empirical outcome of any AI programme approached in this manner.  ISTM that
<br />
references to Moore's Law etc. in this context are at present not much more
<br />
than promissory notes written in invisible ink on transparent paper.
<br />
<br />David.
<br />
<br />Le 28-juin-07, à 17:56, David Nyman a écrit :
<br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev2">&gt; &gt; On 28/06/07, Bruno Marchal &lt; marchal.domain.name.hidden&gt; wrote:
</em><br />
<em class="quotelev2">&gt; &gt;
</em><br />
<em class="quotelev2">&gt; &gt; Hi Bruno
</em><br />
<em class="quotelev2">&gt; &gt;
</em><br />
<em class="quotelev2">&gt; &gt; The remarks you comment on are certainly not the best-considered or
</em><br />
<em class="quotelev2">&gt; &gt; most cogently expressed of my recent posts.  However, I'll try to
</em><br />
<em class="quotelev2">&gt; &gt; clarify if you have specific questions.  As to why I said I'd rather
</em><br />
<em class="quotelev2">&gt; &gt; not use the term 'consciousness', it's because of some recent
</em><br />
<em class="quotelev2">&gt; &gt; confusion and circular disputes ( e.g. with Torgny, or about whether
</em><br />
<em class="quotelev2">&gt; &gt; hydrogen atoms are 'conscious').
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; I am not sure that in case of disagreement (like our &quot;disagreement&quot;
</em><br />
<em class="quotelev1">&gt; with Torgny), changing the vocabulary is a good idea. This will not
</em><br />
<em class="quotelev1">&gt; make the problem going away, on the contrary there is a risk of
</em><br />
<em class="quotelev1">&gt; introducing obscurity.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev2">&gt; &gt; Some of the sometimes confused senses (not by you, I hasten to add!)
</em><br />
<em class="quotelev2">&gt; &gt; seem to be:
</em><br />
<em class="quotelev2">&gt; &gt;
</em><br />
<em class="quotelev2">&gt; &gt; 1) The fact of possessing awareness
</em><br />
<em class="quotelev2">&gt; &gt; 2) The fact of being aware of one's awareness
</em><br />
<em class="quotelev2">&gt; &gt; 3) the fact of being aware of some content of one's awareness
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; So just remember that in a first approximation I identify this with
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; 1) being conscious              (Dt?)   .... for those who have
</em><br />
<em class="quotelev1">&gt; followed the modal posts. (Dx is for ~ Beweisbar (~x))
</em><br />
<em class="quotelev1">&gt; 2) being self-conscious      (DDt?)
</em><br />
<em class="quotelev1">&gt; 3) being conscious of #      (Dp?)
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; You can also have:
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; 4) being self-conscious of something  (DDp?).
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Dp is really an abbreviation of the arithmetical proposition
</em><br />
<em class="quotelev1">&gt; ~beweisbar ( '~p').     'p' means the godel number describing p in the
</em><br />
<em class="quotelev1">&gt; language of the machine (by default it is the first order arithmetic
</em><br />
<em class="quotelev1">&gt; language).
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev2">&gt; &gt;
</em><br />
<em class="quotelev2">&gt; &gt; So now I would prefer to talk about self-relating to a 1-personal
</em><br />
<em class="quotelev2">&gt; &gt; 'world', where previously I might have said 'I am conscious', and that
</em><br />
<em class="quotelev2">&gt; &gt; such a world mediates or instantiates 3-personal content.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; This is ambiguous. The word 'world' is a bit problematic in my setting.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev2">&gt; &gt;  I've tried to root this (in various posts) in a logically or
</em><br />
<em class="quotelev2">&gt; &gt; semantically primitive notion of self-relation that could underly 0,
</em><br />
<em class="quotelev2">&gt; &gt; 1, or 3-person narratives, and to suggest that such self-relation
</em><br />
<em class="quotelev2">&gt; &gt; might be intuited as 'sense' or 'action' depending on the narrative
</em><br />
<em class="quotelev2">&gt; &gt; selected.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; OK.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev2">&gt; &gt; But crucially such nuances would merely be partial takes on the
</em><br />
<em class="quotelev2">&gt; &gt; underlying self-relation, a 'grasp' which is not decomposable.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Actually the elementary grasp are decomposable (into number relations)
</em><br />
<em class="quotelev1">&gt; in the comp setting.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev2">&gt; &gt;
</em><br />
<em class="quotelev2">&gt; &gt; So ISTM that questions should attempt to elicit the machine's
</em><br />
<em class="quotelev2">&gt; &gt; self-relation to such a world and its contents: i.e. it's 'grasp' of a
</em><br />
<em class="quotelev2">&gt; &gt; reality analogous to our own.  And ISTM the machine could also ask
</em><br />
<em class="quotelev2">&gt; &gt; itself such questions, just as we can, if indeed such a world existed
</em><br />
<em class="quotelev2">&gt; &gt; for it.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; OK, but the machine cannot know that. As we cannot know that).
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev2">&gt; &gt;
</em><br />
<em class="quotelev2">&gt; &gt; I realise of course that it's fruitless to try to impose my jargon on
</em><br />
<em class="quotelev2">&gt; &gt; anyone else, but I've just been trying to see whether I could become
</em><br />
<em class="quotelev2">&gt; &gt; less confused by expressing things in this way.  Of course, a
</em><br />
<em class="quotelev2">&gt; &gt; reciprocal effect might just be to make others more confused!
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; It is the risk indeed.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Best regards,
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Bruno
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev2">&gt; &gt;
</em><br />
<em class="quotelev2">&gt; &gt; David
</em><br />
<em class="quotelev3">&gt; &gt;&gt;
</em><br />
<em class="quotelev3">&gt; &gt;&gt;
</em><br />
<em class="quotelev3">&gt; &gt;&gt; Le 21-juin-07, à 01:07, David Nyman a écrit :
</em><br />
<em class="quotelev3">&gt; &gt;&gt;
</em><br />
<em class="quotelev4">&gt; &gt;&gt; &gt;
</em><br />
<em class="quotelev4">&gt; &gt;&gt; &gt; On Jun 5, 3:12 pm, Bruno Marchal &lt; marc....domain.name.hidden&gt; wrote:
</em><br />
<em class="quotelev4">&gt; &gt;&gt; &gt;
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; Personally I don' think we can be *personally* mistaken about our
</em><br />
<em class="quotelev3">&gt; &gt;&gt; own
</em><br />
<em class="quotelev1">&gt; &gt;&gt;  &gt;&gt; consciousness even if we can be mistaken about anything that
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; consciousness could be about.
</em><br />
<em class="quotelev4">&gt; &gt;&gt; &gt;
</em><br />
<em class="quotelev4">&gt; &gt;&gt; &gt; I agree with this, but I would prefer to stop using the term
</em><br />
<em class="quotelev4">&gt; &gt;&gt; &gt; 'consciousness' at all.
</em><br />
<em class="quotelev3">&gt; &gt;&gt;
</em><br />
<em class="quotelev3">&gt; &gt;&gt;
</em><br />
<em class="quotelev3">&gt; &gt;&gt; Why?
</em><br />
<em class="quotelev3">&gt; &gt;&gt;
</em><br />
<em class="quotelev3">&gt; &gt;&gt;
</em><br />
<em class="quotelev3">&gt; &gt;&gt;
</em><br />
<em class="quotelev4">&gt; &gt;&gt; &gt; To make a decision (to whatever degree of
</em><br />
<em class="quotelev4">&gt; &gt;&gt; &gt; certainty) about whether a machine possessed a 1-person pov
</em><br />
<em class="quotelev3">&gt; &gt;&gt; analogous
</em><br />
<em class="quotelev4">&gt; &gt;&gt; &gt; to a human one, we would surely ask it the same sort of questions
</em><br />
<em class="quotelev3">&gt; &gt;&gt; one
</em><br />
<em class="quotelev4">&gt; &gt;&gt; &gt; would ask a human.  That is: questions about its personal 'world' -
</em><br />
<em class="quotelev4">&gt; &gt;&gt; &gt; what it sees, hears, tastes (and perhaps extended non-human
</em><br />
<em class="quotelev4">&gt; &gt;&gt; &gt; modalitiies); what its intentions are, and how it carries them into
</em><br />
<em class="quotelev4">&gt; &gt;&gt; &gt; practice.  From the machine's point-of-view, we would expect it to
</em><br />
<em class="quotelev4">&gt; &gt;&gt; &gt; report such features of its personal world as being immediately
</em><br />
<em class="quotelev4">&gt; &gt;&gt; &gt; present (as ours are), and that it be 'blind' to whatever 'rendering
</em><br />
<em class="quotelev4">&gt; &gt;&gt; &gt; mechanisms' may underlie this (as we are).
</em><br />
<em class="quotelev4">&gt; &gt;&gt; &gt;
</em><br />
<em class="quotelev4">&gt; &gt;&gt; &gt; If it passed these tests, it would be making similar claims on a
</em><br />
<em class="quotelev4">&gt; &gt;&gt; &gt; personal world as we do, and deploying this to achieve similar ends.
</em><br />
<em class="quotelev4">&gt; &gt;&gt; &gt; Since in this case it could ask itself the same questions that we
</em><br />
<em class="quotelev3">&gt; &gt;&gt; can,
</em><br />
<em class="quotelev4">&gt; &gt;&gt; &gt; it would have the same grounds for reaching the same conclusion.
</em><br />
<em class="quotelev4">&gt; &gt;&gt; &gt;
</em><br />
<em class="quotelev4">&gt; &gt;&gt; &gt; However, I've argued in the other bit of this thread against the
</em><br />
<em class="quotelev4">&gt; &gt;&gt; &gt; possibility of a computer in practice being able to instantiate
</em><br />
<em class="quotelev3">&gt; &gt;&gt; such a
</em><br />
<em class="quotelev4">&gt; &gt;&gt; &gt; 1-person world merely in virtue of 'soft' behaviour (i.e.
</em><br />
<em class="quotelev4">&gt; &gt;&gt; &gt; programming).  I suppose I would therefore have to conclude that no
</em><br />
<em class="quotelev4">&gt; &gt;&gt; &gt; machine could actually pass the tests I describe above - whether
</em><br />
<em class="quotelev3">&gt; &gt;&gt; self-
</em><br />
<em class="quotelev4">&gt; &gt;&gt; &gt; administered or not - purely in virtue of running some AI program,
</em><br />
<em class="quotelev4">&gt; &gt;&gt; &gt; however complex.  This is an empirical prediction, and will have to
</em><br />
<em class="quotelev4">&gt; &gt;&gt; &gt; await an empirical outcome.
</em><br />
<em class="quotelev3">&gt; &gt;&gt;
</em><br />
<em class="quotelev3">&gt; &gt;&gt;
</em><br />
<em class="quotelev3">&gt; &gt;&gt; Now I have big problems to understand this post. I must think ... (and
</em><br />
<em class="quotelev3">&gt; &gt;&gt; go).
</em><br />
<em class="quotelev3">&gt; &gt;&gt;
</em><br />
<em class="quotelev3">&gt; &gt;&gt; Bye,
</em><br />
<em class="quotelev3">&gt; &gt;&gt;
</em><br />
<em class="quotelev3">&gt; &gt;&gt; Bruno
</em><br />
<em class="quotelev3">&gt; &gt;&gt;
</em><br />
<em class="quotelev3">&gt; &gt;&gt;
</em><br />
<em class="quotelev3">&gt; &gt;&gt;
</em><br />
<em class="quotelev4">&gt; &gt;&gt; &gt;
</em><br />
<em class="quotelev4">&gt; &gt;&gt; &gt;
</em><br />
<em class="quotelev4">&gt; &gt;&gt; &gt; On Jun 5, 3:12 pm, Bruno Marchal &lt;marc....domain.name.hidden&gt; wrote:
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; Le 03-juin-07, à 21:52, Hal Finney a écrit :
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt;
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt;
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt;
</em><br />
<em class="quotelev2">&gt; &gt;&gt; &gt;&gt;&gt; Part of what I wanted to get at in my thought experiment is the
</em><br />
<em class="quotelev2">&gt; &gt;&gt; &gt;&gt;&gt; bafflement and confusion an AI should feel when exposed to human
</em><br />
<em class="quotelev2">&gt; &gt;&gt; &gt;&gt;&gt; ideas
</em><br />
<em class="quotelev2">&gt; &gt;&gt;  &gt;&gt;&gt; about consciousness.  Various people here have proffered their
</em><br />
<em class="quotelev3">&gt; &gt;&gt; own
</em><br />
<em class="quotelev2">&gt; &gt;&gt; &gt;&gt;&gt; ideas, and we might assume that the AI would read these
</em><br />
<em class="quotelev3">&gt; &gt;&gt; suggestions,
</em><br />
<em class="quotelev2">&gt; &gt;&gt; &gt;&gt;&gt; along with many other ideas that contradict the ones offered here.
</em><br />
<em class="quotelev2">&gt; &gt;&gt; &gt;&gt;&gt; It seems hard to escape the conclusion that the only logical
</em><br />
<em class="quotelev3">&gt; &gt;&gt; response
</em><br />
<em class="quotelev2">&gt; &gt;&gt; &gt;&gt;&gt; is for the AI to figuratively throw up its hands and say that it
</em><br />
<em class="quotelev3">&gt; &gt;&gt; is
</em><br />
<em class="quotelev2">&gt; &gt;&gt; &gt;&gt;&gt; impossible to know if it is conscious, because even humans cannot
</em><br />
<em class="quotelev2">&gt; &gt;&gt; &gt;&gt;&gt; agree
</em><br />
<em class="quotelev2">&gt; &gt;&gt; &gt;&gt;&gt; on what consciousness is.
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt;
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; Augustin said about (subjective) *time* that he knows perfectly
</em><br />
<em class="quotelev3">&gt; &gt;&gt; what
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; it
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; is, but that if you ask him to say what it is, then he admits being
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; unable to say anything. I think that this applies to
</em><br />
<em class="quotelev3">&gt; &gt;&gt; &quot;consciousness&quot;.
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; We know what it is, although only in some personal and
</em><br />
<em class="quotelev3">&gt; &gt;&gt; uncommunicable
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; way.
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; Now this happens to be true also for many mathematical concept.
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; Strictly speaking we don't know how to define the natural numbers,
</em><br />
<em class="quotelev3">&gt; &gt;&gt; and
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; we know today that indeed we cannot define them in a communicable
</em><br />
<em class="quotelev3">&gt; &gt;&gt; way,
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; that is without assuming the auditor knows already what they are.
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt;
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; So what can we do. We can do what mathematicians do all the time.
</em><br />
<em class="quotelev3">&gt; &gt;&gt; We
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; can abandon the very idea of *defining* what consciousness is, and
</em><br />
<em class="quotelev3">&gt; &gt;&gt; try
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; instead to focus on principles or statements about which we can
</em><br />
<em class="quotelev3">&gt; &gt;&gt; agree
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; that they apply to consciousness. Then we can search for
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; (mathematical)
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; object obeying to such or similar principles. This can be made
</em><br />
<em class="quotelev3">&gt; &gt;&gt; easier
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; by admitting some theory or realm for consciousness like the idea
</em><br />
<em class="quotelev3">&gt; &gt;&gt; that
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; consciousness could apply to *some* machine or to some
</em><br />
<em class="quotelev3">&gt; &gt;&gt; *computational
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; events&quot; etc.
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt;
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; We could agree for example that:
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; 1) each one of us know what consciousness is, but nobody can prove
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; he/she/it is conscious.
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; 2) consciousness is related to inner personal or self-referential
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; modality
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; etc.
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt;
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; This is how I proceed in &quot;Conscience et Mécanisme&quot;.  (&quot;conscience&quot;
</em><br />
<em class="quotelev3">&gt; &gt;&gt; is
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; the french for consciousness, &quot;conscience morale&quot; is the french for
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; the
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; english &quot;conscience&quot;).
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt;
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt;
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt;
</em><br />
<em class="quotelev2">&gt; &gt;&gt; &gt;&gt;&gt; In particular I don't think an AI could be expected to claim that
</em><br />
<em class="quotelev3">&gt; &gt;&gt; it
</em><br />
<em class="quotelev2">&gt; &gt;&gt; &gt;&gt;&gt; knows that it is conscious, that consciousness is a deep and
</em><br />
<em class="quotelev2">&gt; &gt;&gt; &gt;&gt;&gt; intrinsic
</em><br />
<em class="quotelev2">&gt; &gt;&gt; &gt;&gt;&gt; part of itself, that whatever else it might be mistaken about it
</em><br />
<em class="quotelev2">&gt; &gt;&gt; &gt;&gt;&gt; could
</em><br />
<em class="quotelev2">&gt; &gt;&gt; &gt;&gt;&gt; not be mistaken about being conscious.  I don't see any logical
</em><br />
<em class="quotelev3">&gt; &gt;&gt; way
</em><br />
<em class="quotelev2">&gt; &gt;&gt; &gt;&gt;&gt; it
</em><br />
<em class="quotelev2">&gt; &gt;&gt; &gt;&gt;&gt; could reach this conclusion by studying the corpus of writings on
</em><br />
<em class="quotelev3">&gt; &gt;&gt; the
</em><br />
<em class="quotelev2">&gt; &gt;&gt; &gt;&gt;&gt; topic.  If anyone disagrees, I'd like to hear how it could happen.
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt;
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; As far as a machine is correct, when she introspects herself, she
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; cannot not discover a gap between truth (p) and provability (Bp).
</em><br />
<em class="quotelev3">&gt; &gt;&gt; The
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; machine can discover correctly (but not necessarily in a completely
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; communicable way) a gap between provability (which can potentially
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; leads to falsities, despite correctness) and the incorrigible
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; knowability or knowledgeability (Bp &amp; p), and then the gap between
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; those notions and observability (Bp &amp; Dp) and sensibility (Bp &amp; Dp
</em><br />
<em class="quotelev3">&gt; &gt;&gt; &amp;
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; p). Even without using the conventional name of &quot;consciousness&quot;,
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; machines can discover semantical fixpoint playing the role of non
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; expressible but true statements.
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; We can *already* talk with machine about those true unnameable
</em><br />
<em class="quotelev3">&gt; &gt;&gt; things,
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; as have done Tarski, Godel, Lob, Solovay, Boolos, Goldblatt, etc.
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt;
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt;
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt;
</em><br />
<em class="quotelev2">&gt; &gt;&gt; &gt;&gt;&gt; And the corollary to this is that perhaps humans also cannot
</em><br />
<em class="quotelev2">&gt; &gt;&gt; &gt;&gt;&gt; legitimately
</em><br />
<em class="quotelev2">&gt; &gt;&gt;  &gt;&gt;&gt; make such claims, since logically their position is not so
</em><br />
<em class="quotelev3">&gt; &gt;&gt; different
</em><br />
<em class="quotelev2">&gt; &gt;&gt; &gt;&gt;&gt; from that of the AI.  In that case the seemingly axiomatic
</em><br />
<em class="quotelev3">&gt; &gt;&gt; question
</em><br />
<em class="quotelev2">&gt; &gt;&gt; &gt;&gt;&gt; of
</em><br />
<em class="quotelev2">&gt; &gt;&gt; &gt;&gt;&gt; whether we are conscious may after all be something that we could
</em><br />
<em class="quotelev3">&gt; &gt;&gt; be
</em><br />
<em class="quotelev2">&gt; &gt;&gt; &gt;&gt;&gt; mistaken about.
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt;
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; This is an inference from &quot;I cannot express p&quot; to &quot;I can express
</em><br />
<em class="quotelev3">&gt; &gt;&gt; not
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; p&quot;. Or from ~Bp to B~p.  Many atheist reason like that about the
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; concept of &quot;unameable&quot; reality, but it is a logical error.
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; Even for someone who is not willing to take the comp hyp into
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; consideration, it is a third person communicable fact that
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; self-observing machines can discover and talk about many non
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; 3-provable
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; and sometimes even non 3-definable true &quot;statements&quot; about them.
</em><br />
<em class="quotelev3">&gt; &gt;&gt; Some
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; true statements can only be interrogated.
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; Personally I don' think we can be *personally* mistaken about our
</em><br />
<em class="quotelev3">&gt; &gt;&gt; own
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; consciousness even if we can be mistaken about anything that
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; consciousness could be about.
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt;
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; Bruno
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt;
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt;&gt; <a href="http://iridia.ulb.ac.be/~marchal/">http://iridia.ulb.ac.be/~marchal/</a>&lt;<a href="http://iridia.ulb.ac.be/%7Emarchal/">http://iridia.ulb.ac.be/%7Emarchal/</a>&gt;
</em><br />
<em class="quotelev4">&gt; &gt;&gt; &gt;
</em><br />
<em class="quotelev4">&gt; &gt;&gt; &gt;
</em><br />
<em class="quotelev1">&gt; &gt;&gt; &gt; &gt;
</em><br />
<em class="quotelev4">&gt; &gt;&gt; &gt;
</em><br />
<em class="quotelev3">&gt; &gt;&gt; <a href="http://iridia.ulb.ac.be/~marchal/">http://iridia.ulb.ac.be/~marchal/</a> &lt;<a href="http://iridia.ulb.ac.be/%7Emarchal/">http://iridia.ulb.ac.be/%7Emarchal/</a>&gt;
</em><br />
<em class="quotelev3">&gt; &gt;&gt;
</em><br />
<em class="quotelev1">&gt; &gt;&gt;  &gt;&gt;
</em><br />
<em class="quotelev3">&gt; &gt;&gt;
</em><br />
<em class="quotelev1">&gt; <a href="http://iridia.ulb.ac.be/~marchal/">http://iridia.ulb.ac.be/~marchal/</a> &lt;<a href="http://iridia.ulb.ac.be/%7Emarchal/">http://iridia.ulb.ac.be/%7Emarchal/</a>&gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev2">&gt; &gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<br />--~--~---------~--~----~------------~-------~--~----~
<br />
You received this message because you are subscribed to the Google Groups &quot;Everything List&quot; group.
<br />
To post to this group, send email to everything-list.domain.name.hidden
<br />
To unsubscribe from this group, send email to everything-list-unsubscribe.domain.name.hidden
<br />
For more options, visit this group at <a href="http://groups.google.com/group/everything-list?hl=en">http://groups.google.com/group/everything-list?hl=en</a>
<br />
-~----------~----~----~----~------~----~------~--~---
<br />
<span id="received"><dfn>Received on</dfn> Fri Jun 29 2007 - 11:18:02 PDT</span>
</div>
<!-- body="end" -->
<div class="foot">
<map id="navbarfoot" name="navbarfoot" title="Related messages">
<ul class="links">
<li><dfn>This message</dfn>: [ <a href="#start13681">Message body</a> ]</li>
<!-- lnext="start" -->
<li><dfn>Next message</dfn>: <a href="13682.html" title="Next message in the list">Jason: "Justifying the Theory of Everything"</a></li>
<li><dfn>Previous message</dfn>: <a href="13680.html" title="Previous message in the list">LauLuna: "Re: Penrose and algorithms"</a></li>
<li><dfn>In reply to</dfn>: <a href="13679.html" title="Message to which this message replies">Bruno Marchal: "Re: How would a computer know if it were conscious?"</a></li>
<!-- lnextthread="start" -->
<li><dfn>Next in thread</dfn>: <a href="13449.html" title="Next message in this discussion thread">Colin Hales: "Re: How would a computer know if it were conscious?"</a></li>
<!-- lreply="end" -->
</ul>
<ul class="links">
<li><a name="options3" id="options3"></a><dfn>Contemporary messages sorted</dfn>: [ <a href="date.html#msg13681" title="Contemporary messages by date">by date</a> ] [ <a href="index.html#msg13681" title="Contemporary discussion threads">by thread</a> ] [ <a href="subject.html#msg13681" title="Contemporary messages by subject">by subject</a> ] [ <a href="author.html#msg13681" title="Contemporary messages by author">by author</a> ] [ <a href="attachment.html" title="Contemporary messages by attachment">by messages with attachments</a> ]</li>
</ul>
</map>
</div>
<!-- trailer="footer" -->
<p><small><em>
This archive was generated by <a href="http://www.hypermail-project.org/">hypermail 2.3.0</a>
: Fri Feb 16 2018 - 13:20:14 PST
</em></small></p>
</body>
</html>
