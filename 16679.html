<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1" />
<meta name="generator" content="hypermail 2.3.0, see http://www.hypermail-project.org/" />
<title>Re: Consciousness is information? from Bruno Marchal on 2009-05-23 (everything)</title>
<meta name="Author" content="Bruno Marchal (marchal.domain.name.hidden)" />
<meta name="Subject" content="Re: Consciousness is information?" />
<meta name="Date" content="2009-05-23" />
<style type="text/css">
/*<![CDATA[*/
/* To be incorporated in the main stylesheet, don't code it in hypermail! */
body {color: black; background: #ffffff}
dfn {font-weight: bold;}
pre { background-color:inherit;}
.head { border-bottom:1px solid black;}
.foot { border-top:1px solid black;}
th {font-style:italic;}
table { margin-left:2em;}map ul {list-style:none;}
#mid { font-size:0.9em;}
#received { float:right;}
address { font-style:inherit ;}
/*]]>*/
.quotelev1 {color : #990099}
.quotelev2 {color : #ff7700}
.quotelev3 {color : #007799}
.quotelev4 {color : #95c500}
.period {font-weight: bold}
</style>
</head>
<body>
<div class="head">
<h1>Re: Consciousness is information?</h1>
<!-- received="Sun May 24 07:54:29 2009" -->
<!-- isoreceived="20090524145429" -->
<!-- sent="Sun, 24 May 2009 07:54:29 +0200" -->
<!-- isosent="20090524055429" -->
<!-- name="Bruno Marchal" -->
<!-- email="marchal.domain.name.hidden" -->
<!-- subject="Re: Consciousness is information?" -->
<!-- id="B86F30AA-4631-4598-B27D-E020D196B3D2.domain.name.hidden" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="c8d958e90905231456x2d01d256m6df66a8926dc1c57.domain.name.hidden" -->
<!-- expires="-1" -->
<map id="navbar" name="navbar">
<ul class="links">
<li>
<dfn>This message</dfn>:
[ <a href="#start16679" name="options1" id="options1" tabindex="1">Message body</a> ]
 [ More options (<a href="#options2">top</a>, <a href="#options3">bottom</a>) ]
</li>
<li>
<dfn>Related messages</dfn>:
<!-- unext="start" -->
[ <a href="16680.html" accesskey="d" title="Brent Meeker: &quot;Re: Consciousness is information?&quot;">Next message</a> ]
[ <a href="16678.html" title="Kelly: &quot;Re: Consciousness is information?&quot;">Previous message</a> ]
[ <a href="16677.html" title="Kelly Harmon: &quot;Re: Consciousness is information?&quot;">In reply to</a> ]
<!-- unextthread="start" -->
[ <a href="16681.html" accesskey="t" title="Brent Meeker: &quot;Re: Consciousness is information?&quot;">Next in thread</a> ]
 [ <a href="#replies">Replies</a> ]
<!-- ureply="end" -->
</li>
</ul>
</map>
<ul class="links">
<li><a name="options2" id="options2"></a><dfn>Contemporary messages sorted</dfn>: [ <a href="date.html#msg16679" title="Contemporary messages by date">by date</a> ] [ <a href="index.html#msg16679" title="Contemporary discussion threads">by thread</a> ] [ <a href="subject.html#msg16679" title="Contemporary messages by subject">by subject</a> ] [ <a href="author.html#msg16679" title="Contemporary messages by author">by author</a> ] [ <a href="attachment.html" title="Contemporary messages by attachment">by messages with attachments</a> ]</li>
</ul>
</div>
<!-- body="start" -->
<div class="mail">
<address class="headers">
<span id="from">
<dfn>From</dfn>: Bruno Marchal &lt;<a href="mailto:marchal.domain.name.hidden?Subject=Re%3A%20Consciousness%20is%20information%3F">marchal.domain.name.hidden</a>&gt;
</span><br />
<span id="date"><dfn>Date</dfn>: Sun, 24 May 2009 07:54:29 +0200</span><br />
</address>
<br />
OK. So, now, Kelly, just to understand what you mean by your theory, I  
<br />
have to ask you what your theory predicts in case of self- 
<br />
multiplication.
<br />
You have to see that, personally, I don't have a theory other than the  
<br />
assumption that the brain is emulable by a Turing machine, and by  
<br />
brain I mean any portion of my local neighborhood needed for surviving  
<br />
the comp functional substitution. This is the comp hypothesis.
<br />
<br />Because we are both modal realist(*), and, true, worlds (histories)  
<br />
with white rabbit exists, and from inside are as actual as our present  
<br />
state. But then, I say that, as a consequence of the comp hyp, there  
<br />
is a relative probability or credibility measure on those histories.  
<br />
To see where does those probabilities come from, you have to  
<br />
understand that 1) you can be multiplied (that is read, copy (cut) and  
<br />
pasted in Washington AND Moscow (say)), and 2) you are multiplied (by  
<br />
2^aleph_zero, at each instant, with a comp definition of instant not  
<br />
related in principle with any form of physical time).
<br />
<br />What does your theory predicts concerning your expectation in such an  
<br />
experience/experiment.
<br />
<br />The fact is that your explanation, that we are in an typical universe,  
<br />
because those exist as well, just does not work with the comp hyp. It  
<br />
does not work, because it does not explain why we REMAIN in that  
<br />
typical worlds. It seems to me that, as far as I can put meaning on  
<br />
your view, the probability I will see a white rabbit in two seconds is  
<br />
as great than the probability I will see anything else, and this is in  
<br />
contradiction with the fact. What makes us staying in apparent lawful  
<br />
histories?
<br />
<br />What does you theory predict about agony and death, from the first  
<br />
person point of view? This is an extreme case where comp is sensibly  
<br />
in opposition with &quot;Aristotelian naturalism&quot;.
<br />
<br />May be you could study the UDA, and directly tell me at which step  
<br />
your &quot;theory&quot; departs from the comp hyp. It has to depart, because you  
<br />
say below that we are in a quantum reality by chance, where the comp  
<br />
hyp explains why we have to be (even after death) in a quantum reality.
<br />
<br />Bruno
<br />
<br />(*) Once and for all, when I say I am a modal realist, I really mean  
<br />
this &quot;I have an argument showing that the comp theory imposes modal  
<br />
realism&quot;.  I am really not defending any theory. I am just showing  
<br />
that the comp theory leads to precise and verifiable/refutable facts.  
<br />
I am a logician: all what I show to people is that IF you believe this  
<br />
THEN you have to believe that. It is part of my personal religion that  
<br />
my personal religion is personal and private (and evolvable).
<br />
<br /><br /><br />On 23 May 2009, at 23:56, Kelly Harmon wrote:
<br />
<br /><em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; On Sat, May 23, 2009 at 8:47 AM, Bruno Marchal &lt;marchal.domain.name.hidden&gt;  
</em><br />
<em class="quotelev1">&gt; wrote:
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev3">&gt;&gt;&gt; To repeat my
</em><br />
<em class="quotelev3">&gt;&gt;&gt; earlier Chalmers quote, &quot;Experience is information from the inside;
</em><br />
<em class="quotelev3">&gt;&gt;&gt; physics is information from the outside.&quot;  It is this subjective
</em><br />
<em class="quotelev3">&gt;&gt;&gt; experience of information that provides meaning to the otherwise
</em><br />
<em class="quotelev3">&gt;&gt;&gt; completely abstract &quot;platonic&quot; symbols.
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt; I insist on this well before Chalmers. We are agreeing on this.
</em><br />
<em class="quotelev2">&gt;&gt; But then you associate consciousness with the experience of  
</em><br />
<em class="quotelev2">&gt;&gt; information.
</em><br />
<em class="quotelev2">&gt;&gt; This is what I told you. I can understand the relation between
</em><br />
<em class="quotelev2">&gt;&gt; consciousness and information content.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Information.  Information content.  Hmmmmmmmm.  Well, I'm not entirely
</em><br />
<em class="quotelev1">&gt; sure what you're saying here.  Maybe I don't have a problem with this,
</em><br />
<em class="quotelev1">&gt; but maybe I do.  Maybe we're really saying the same thing here, but
</em><br />
<em class="quotelev1">&gt; maybe we're not.  Hmmmmm.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev3">&gt;&gt;&gt; Note that I don't have Bruno's fear of white rabbits.
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt; Then you disagree with all reader of David Lewis, including David
</em><br />
<em class="quotelev2">&gt;&gt; lewis himself who recognizes this inflation of to many realities as a
</em><br />
<em class="quotelev2">&gt;&gt; weakness of its modal realism. My point is that the comp constraints
</em><br />
<em class="quotelev2">&gt;&gt; leads to a solution of that problem, indeed a solution close to the
</em><br />
<em class="quotelev2">&gt;&gt; quantum Everett solution. But the existence of white rabbits, and  
</em><br />
<em class="quotelev2">&gt;&gt; thus
</em><br />
<em class="quotelev2">&gt;&gt; the correctness of comp remains to be tested.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; True, Lewis apparently saw it as a cost, BUT not so high a cost as to
</em><br />
<em class="quotelev1">&gt; abandon modal realism.  I don't even see it as a high cost, I see it
</em><br />
<em class="quotelev1">&gt; as a logical consequence.  Again, it's easy to imagine a computer
</em><br />
<em class="quotelev1">&gt; simulation/virtual reality in which a conscious observer would see
</em><br />
<em class="quotelev1">&gt; disembodied talking heads and flying pigs.  So it certainly seems
</em><br />
<em class="quotelev1">&gt; possible for a conscious being to be in a state of observing an
</em><br />
<em class="quotelev1">&gt; unattached talking head.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Given that it's possible, why wouldn't it be actual?
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; The only reason to think that it wouldn't be actual is that our
</em><br />
<em class="quotelev1">&gt; external objectively existing physical universe doesn't have physical
</em><br />
<em class="quotelev1">&gt; laws that can lead easily to the existance of such talking heads to be
</em><br />
<em class="quotelev1">&gt; observed.  But once you've abandoned the external universe and
</em><br />
<em class="quotelev1">&gt; embraced platonism, then where does the constraint against observing
</em><br />
<em class="quotelev1">&gt; talking heads come from?
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Assuming platonism, I can explain why &quot;I&quot; don't see talking heads:
</em><br />
<em class="quotelev1">&gt; because every possible Kelly is realized, and that includes a Kelly
</em><br />
<em class="quotelev1">&gt; who doesn't observe disembodied talking heads and who doesn't know
</em><br />
<em class="quotelev1">&gt; anyone who has ever seen such a head.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; So given that my observations aren't in conflict with my theory, I
</em><br />
<em class="quotelev1">&gt; don't see a problem.  The fact that nothing that I could observe would
</em><br />
<em class="quotelev1">&gt; ever conflict with my theory is also not particularly troubling to me
</em><br />
<em class="quotelev1">&gt; because I didn't arrive at my theory as means of explaining any
</em><br />
<em class="quotelev1">&gt; particular observed fact about the external universe.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; My theory isn't intended to explain the contingent details of what I
</em><br />
<em class="quotelev1">&gt; observe.  It's intended to explain the fact THAT I subjectively
</em><br />
<em class="quotelev1">&gt; observe anything at all.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Given that it seems theoretically possible to create a computer
</em><br />
<em class="quotelev1">&gt; simulation that would manifest any imaginable conscious being
</em><br />
<em class="quotelev1">&gt; observing any imaginable &quot;world&quot;, including schizophrenic beings
</em><br />
<em class="quotelev1">&gt; observing psychodelic realities, I don't see why you are trying to
</em><br />
<em class="quotelev1">&gt; constrain the platonic realities that can be experienced to those that
</em><br />
<em class="quotelev1">&gt; are extremely similar to ours.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev2">&gt;&gt; It is just a question of testing a theory. You seem to say something
</em><br />
<em class="quotelev2">&gt;&gt; like &quot;if the theory predict that water under fire will typically  
</em><br />
<em class="quotelev2">&gt;&gt; boil,
</em><br />
<em class="quotelev2">&gt;&gt; and that experience does not confirm that typicality (water froze
</em><br />
<em class="quotelev2">&gt;&gt; regularly) then it means we are just very unlucky&quot;. But then all
</em><br />
<em class="quotelev2">&gt;&gt; theories are correct.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; I say there is no water.  There is just our subjective experience of
</em><br />
<em class="quotelev1">&gt; observing water.  Trying to constrain a Platonic theory of
</em><br />
<em class="quotelev1">&gt; consciousness so that it matches a particular observed physical
</em><br />
<em class="quotelev1">&gt; reality seems like a mistake to me.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Is there a limit to what we could experience in a computer simulated
</em><br />
<em class="quotelev1">&gt; reality?  If not, why would there be a limit to what we could
</em><br />
<em class="quotelev1">&gt; experience in Platonia?
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev3">&gt;&gt;&gt; The double-aspect principle stems from the observation that there  
</em><br />
<em class="quotelev3">&gt;&gt;&gt; is a
</em><br />
<em class="quotelev3">&gt;&gt;&gt; direct isomorphism between certain physically embodied information
</em><br />
<em class="quotelev3">&gt;&gt;&gt; spaces and certain phenomenal (or experiential) information spaces.
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt; This can be shown false in Quantum theory without collapse, and more
</em><br />
<em class="quotelev2">&gt;&gt; easily with the comp assumption.
</em><br />
<em class="quotelev2">&gt;&gt; No problem if you tell me that you reject both Everett and comp.
</em><br />
<em class="quotelev2">&gt;&gt; Chalmers seems in some place to accept both Everett and comp, indeed.
</em><br />
<em class="quotelev2">&gt;&gt; He explains to me that he stops at step 3. He believes that after a
</em><br />
<em class="quotelev2">&gt;&gt; duplication you feel to be simultaneously at the both place, even
</em><br />
<em class="quotelev2">&gt;&gt; assuming comp. I think and can argue that this is non sense. Nobody
</em><br />
<em class="quotelev2">&gt;&gt; defends this on the list. Are you defending an idea like that?
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; I included the Chalmers quote because I think it provides a good image
</em><br />
<em class="quotelev1">&gt; of how abstract information seems to supervene on physical systems.
</em><br />
<em class="quotelev1">&gt; BUT by quoting the passage I'm not saying that I think that this
</em><br />
<em class="quotelev1">&gt; appearance of supervenience is the source of consciousness.  I still
</em><br />
<em class="quotelev1">&gt; buy into the putnam mapping view that there is no 1-to-1 mapping from
</em><br />
<em class="quotelev1">&gt; information or computation to any physical system, which of course
</em><br />
<em class="quotelev1">&gt; makes physicalism untenable as an explanation for consciousness.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; As for Everettian MWI, I don't think that quantum mechanics has
</em><br />
<em class="quotelev1">&gt; anything to do with conscious experience.  The fact that we see a
</em><br />
<em class="quotelev1">&gt; world which is apparently quantum mechanical in nature is a
</em><br />
<em class="quotelev1">&gt; coincidence.  A fluke.  In keeping with an unconstrained platonic
</em><br />
<em class="quotelev1">&gt; theory of consciousness, I would expect that there are other conscious
</em><br />
<em class="quotelev1">&gt; observers who experience other very different worlds where they make
</em><br />
<em class="quotelev1">&gt; observations that are not consistent with quantum mechanics.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev2">&gt;&gt; Perhaps. I don't see the relevance. It is quite coherent with comp
</em><br />
<em class="quotelev2">&gt;&gt; that some form of meaning can be approached in this or similar ways.
</em><br />
<em class="quotelev2">&gt;&gt; Assuming comp, what can be considered as lacking is the self- 
</em><br />
<em class="quotelev2">&gt;&gt; reference
</em><br />
<em class="quotelev2">&gt;&gt; of the universal machine involved in the attribution of meaning.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; I included the LSA discussion because I think it gives a good image of
</em><br />
<em class="quotelev1">&gt; how I see information being structured in a platonic sense, as
</em><br />
<em class="quotelev1">&gt; relationships between symbols, and also because it said some
</em><br />
<em class="quotelev1">&gt; interesting things about the symbol grounding problem.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev2">&gt;&gt; With comp, those other &quot;sensory modalities&quot; are coded before being
</em><br />
<em class="quotelev2">&gt;&gt; processed by the brain, or the universal machine under consideration.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; I agree, other sensory modalities are just more ungrounded tokenized
</em><br />
<em class="quotelev1">&gt; information that is included in the web of relationships which
</em><br />
<em class="quotelev1">&gt; ultimately, when consciously experienced &quot;from the inside&quot;, provides
</em><br />
<em class="quotelev1">&gt; meaning to the otherwise purely abstract &quot;ungrounded&quot; platonic
</em><br />
<em class="quotelev1">&gt; symbols.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev2">&gt;&gt; Kelly, the question is: do we disagree?
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; I've wondered that too.  It could be that we only differ on a few
</em><br />
<em class="quotelev1">&gt; relatively minor points.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev2">&gt;&gt; I criticize your statement
</em><br />
<em class="quotelev2">&gt;&gt; &quot;consciousness = information&quot; for vagueness, but only BECAUSE you  
</em><br />
<em class="quotelev2">&gt;&gt; have
</em><br />
<em class="quotelev2">&gt;&gt; oppose it to the computationalist hypothesis,
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; I don't deny that there are computational/arithmetical descriptions of
</em><br />
<em class="quotelev1">&gt; how instances of consciousness can be related.  We agree on that.  I
</em><br />
<em class="quotelev1">&gt; just question what role, OTHER than describing the possible
</em><br />
<em class="quotelev1">&gt; relationships between sets of information, that computation plays.
</em><br />
<em class="quotelev1">&gt; Given that many algorithms can produce the same output from the same
</em><br />
<em class="quotelev1">&gt; input, I am inclined to say that it's the output that matters for
</em><br />
<em class="quotelev1">&gt; consciousness, not the algorithm.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; It seems to me that connections between instances of consciousness are
</em><br />
<em class="quotelev1">&gt; implied, but that there's nothing &quot;real&quot; actually binding these
</em><br />
<em class="quotelev1">&gt; instances together other than the subjective feelings of continuity
</em><br />
<em class="quotelev1">&gt; arising from the memory each instant has of previous instances.  But
</em><br />
<em class="quotelev1">&gt; &quot;memory&quot; would seem to be a informational/data related concept I'd
</em><br />
<em class="quotelev1">&gt; think, not an algorithmic one.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; If an algorithm results in the overwriting or erasure of memory, then
</em><br />
<em class="quotelev1">&gt; there is no longer the flow of conscious experience.  The algorithm
</em><br />
<em class="quotelev1">&gt; doesn't provide that &quot;subjective&quot; connection between instances of
</em><br />
<em class="quotelev1">&gt; consciousness.  The information stored in memory does.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev2">&gt; &gt;
</em><br />
<br /><a href="http://iridia.ulb.ac.be/~marchal/">http://iridia.ulb.ac.be/~marchal/</a>
<br />
<br /><br /><br /><br />--~--~---------~--~----~------------~-------~--~----~
<br />
You received this message because you are subscribed to the Google Groups &quot;Everything List&quot; group.
<br />
To post to this group, send email to everything-list.domain.name.hidden
<br />
To unsubscribe from this group, send email to everything-list+unsubscribe.domain.name.hidden
<br />
For more options, visit this group at <a href="http://groups.google.com/group/everything-list?hl=en">http://groups.google.com/group/everything-list?hl=en</a>
<br />
-~----------~----~----~----~------~----~------~--~---
<br />
<span id="received"><dfn>Received on</dfn> Sun May 24 2009 - 07:54:29 PDT</span>
</div>
<!-- body="end" -->
<div class="foot">
<map id="navbarfoot" name="navbarfoot" title="Related messages">
<ul class="links">
<li><dfn>This message</dfn>: [ <a href="#start16679">Message body</a> ]</li>
<!-- lnext="start" -->
<li><dfn>Next message</dfn>: <a href="16680.html" title="Next message in the list">Brent Meeker: "Re: Consciousness is information?"</a></li>
<li><dfn>Previous message</dfn>: <a href="16678.html" title="Previous message in the list">Kelly: "Re: Consciousness is information?"</a></li>
<li><dfn>In reply to</dfn>: <a href="16677.html" title="Message to which this message replies">Kelly Harmon: "Re: Consciousness is information?"</a></li>
<!-- lnextthread="start" -->
<li><dfn>Next in thread</dfn>: <a href="16681.html" title="Next message in this discussion thread">Brent Meeker: "Re: Consciousness is information?"</a></li>
<li><a name="replies" id="replies"></a>
<dfn>Reply</dfn>: <a href="16681.html" title="Message sent in reply to this message">Brent Meeker: "Re: Consciousness is information?"</a></li>
<li><dfn>Reply</dfn>: <a href="16682.html" title="Message sent in reply to this message">Kelly Harmon: "Re: Consciousness is information?"</a></li>
<!-- lreply="end" -->
</ul>
<ul class="links">
<li><a name="options3" id="options3"></a><dfn>Contemporary messages sorted</dfn>: [ <a href="date.html#msg16679" title="Contemporary messages by date">by date</a> ] [ <a href="index.html#msg16679" title="Contemporary discussion threads">by thread</a> ] [ <a href="subject.html#msg16679" title="Contemporary messages by subject">by subject</a> ] [ <a href="author.html#msg16679" title="Contemporary messages by author">by author</a> ] [ <a href="attachment.html" title="Contemporary messages by attachment">by messages with attachments</a> ]</li>
</ul>
</map>
</div>
<!-- trailer="footer" -->
<p><small><em>
This archive was generated by <a href="http://www.hypermail-project.org/">hypermail 2.3.0</a>
: Fri Feb 16 2018 - 13:20:15 PST
</em></small></p>
</body>
</html>
