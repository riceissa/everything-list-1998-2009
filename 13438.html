<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1" />
<meta name="generator" content="hypermail 2.3.0, see http://www.hypermail-project.org/" />
<title>Re: How would a computer know if it were conscious? from Brent Meeker on 2007-06-02 (everything)</title>
<meta name="Author" content="Brent Meeker (meekerdb.domain.name.hidden)" />
<meta name="Subject" content="Re: How would a computer know if it were conscious?" />
<meta name="Date" content="2007-06-02" />
<style type="text/css">
/*<![CDATA[*/
/* To be incorporated in the main stylesheet, don't code it in hypermail! */
body {color: black; background: #ffffff}
dfn {font-weight: bold;}
pre { background-color:inherit;}
.head { border-bottom:1px solid black;}
.foot { border-top:1px solid black;}
th {font-style:italic;}
table { margin-left:2em;}map ul {list-style:none;}
#mid { font-size:0.9em;}
#received { float:right;}
address { font-style:inherit ;}
/*]]>*/
.quotelev1 {color : #990099}
.quotelev2 {color : #ff7700}
.quotelev3 {color : #007799}
.quotelev4 {color : #95c500}
.period {font-weight: bold}
</style>
</head>
<body>
<div class="head">
<h1>Re: How would a computer know if it were conscious?</h1>
<!-- received="Sat Jun  2 17:57:10 2007" -->
<!-- isoreceived="20070603005710" -->
<!-- sent="Sat, 02 Jun 2007 14:57:06 -0700" -->
<!-- isosent="20070602215706" -->
<!-- name="Brent Meeker" -->
<!-- email="meekerdb.domain.name.hidden" -->
<!-- subject="Re: How would a computer know if it were conscious?" -->
<!-- id="4661E7B2.30308.domain.name.hidden" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="200706022336.18516.allcolor.domain.name.hidden" -->
<!-- expires="-1" -->
<map id="navbar" name="navbar">
<ul class="links">
<li>
<dfn>This message</dfn>:
[ <a href="#start13438" name="options1" id="options1" tabindex="1">Message body</a> ]
 [ More options (<a href="#options2">top</a>, <a href="#options3">bottom</a>) ]
</li>
<li>
<dfn>Related messages</dfn>:
<!-- unext="start" -->
[ <a href="13439.html" accesskey="d" title="Pete Carlton: &quot;Re: Asifism&quot;">Next message</a> ]
[ <a href="13437.html" title="Quentin Anciaux: &quot;Re: How would a computer know if it were conscious?&quot;">Previous message</a> ]
[ <a href="13437.html" title="Quentin Anciaux: &quot;Re: How would a computer know if it were conscious?&quot;">In reply to</a> ]
<!-- unextthread="start" -->
[ <a href="13440.html" accesskey="t" title="Jason Resch: &quot;Re: How would a computer know if it were conscious?&quot;">Next in thread</a> ]
<!-- ureply="end" -->
</li>
</ul>
</map>
<ul class="links">
<li><a name="options2" id="options2"></a><dfn>Contemporary messages sorted</dfn>: [ <a href="date.html#msg13438" title="Contemporary messages by date">by date</a> ] [ <a href="index.html#msg13438" title="Contemporary discussion threads">by thread</a> ] [ <a href="subject.html#msg13438" title="Contemporary messages by subject">by subject</a> ] [ <a href="author.html#msg13438" title="Contemporary messages by author">by author</a> ] [ <a href="attachment.html" title="Contemporary messages by attachment">by messages with attachments</a> ]</li>
</ul>
</div>
<!-- body="start" -->
<div class="mail">
<address class="headers">
<span id="from">
<dfn>From</dfn>: Brent Meeker &lt;<a href="mailto:meekerdb.domain.name.hidden?Subject=Re%3A%20How%20would%20a%20computer%20know%20if%20it%20were%20conscious%3F">meekerdb.domain.name.hidden</a>&gt;
</span><br />
<span id="date"><dfn>Date</dfn>: Sat, 02 Jun 2007 14:57:06 -0700</span><br />
</address>
<br />
Quentin Anciaux wrote:
<br />
<em class="quotelev1">&gt; I'd like to add a &quot;definition&quot; of consciousness.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Consciousness is the inner narative composed of sounds/images/feelings which 
</em><br />
<em class="quotelev1">&gt; present itself as 'I'. What is (the origin/meaning) of 'I', I don't know, 
</em><br />
<em class="quotelev1">&gt; but 'I' is the consciousness.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Quentin
</em><br />
<em class="quotelev1">&gt;   
</em><br />
<br />John McCarthy notes that consciousness is not a single thing.  He has 
<br />
written some essays on what it would mean to create a conscious 
<br />
artificial intelligence:
<br />
<br /><a href="http://www-formal.stanford.edu/jmc/consciousness.html">http://www-formal.stanford.edu/jmc/consciousness.html</a>
<br />
<a href="http://www-formal.stanford.edu/jmc/zombie.pdf">http://www-formal.stanford.edu/jmc/zombie.pdf</a>
<br />
<br />Brent Meeker
<br />
<br /><em class="quotelev1">&gt; On Saturday 02 June 2007 22:13:30 Hal Finney wrote:
</em><br />
<em class="quotelev1">&gt;   
</em><br />
<em class="quotelev2">&gt;&gt; Various projects exist today aiming at building a true Artificial
</em><br />
<em class="quotelev2">&gt;&gt; Intelligence.  Sometimes these researchers use the term AGI, Artificial
</em><br />
<em class="quotelev2">&gt;&gt; General Intelligence, to distinguish their projects from mainstream AI
</em><br />
<em class="quotelev2">&gt;&gt; which tends to focus on specific tasks.  A conference on such projects
</em><br />
<em class="quotelev2">&gt;&gt; will be held next year, agi-08.org.
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt; Suppose one of these projects achieves one of the milestone goals of
</em><br />
<em class="quotelev2">&gt;&gt; such efforts; their AI becomes able to educate itself by reading books
</em><br />
<em class="quotelev2">&gt;&gt; and reference material, rather than having to have facts put in by
</em><br />
<em class="quotelev2">&gt;&gt; the developers.  Perhaps it requires some help with this, and various
</em><br />
<em class="quotelev2">&gt;&gt; questions and ambiguities need to be answered by humans, but still this is
</em><br />
<em class="quotelev2">&gt;&gt; a huge advancement as the AI can now in principle learn almost any field.
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt; Keep in mind that this AI is far from passing the Turing test; it is able
</em><br />
<em class="quotelev2">&gt;&gt; to absorb and digest material and then answer questions or perhaps even
</em><br />
<em class="quotelev2">&gt;&gt; engage in a dialog about it.  But its complexity is, we will suppose,
</em><br />
<em class="quotelev2">&gt;&gt; substantially less than the human brain.
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt; Now at some point the AI reads about the philosophy of mind, and the
</em><br />
<em class="quotelev2">&gt;&gt; question is put to it: are you conscious?
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt; How might an AI program go about answering a question like this?
</em><br />
<em class="quotelev2">&gt;&gt; What kind of reasoning would be applicable?  In principle, how would
</em><br />
<em class="quotelev2">&gt;&gt; you expect a well-designed AI to decide if it is conscious?  And then,
</em><br />
<em class="quotelev2">&gt;&gt; how or why is the reasoning different if a human rather than an AI is
</em><br />
<em class="quotelev2">&gt;&gt; answering them?
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt; Clearly the AI has to start with the definition.  It needs to know what
</em><br />
<em class="quotelev2">&gt;&gt; consciousness is, what the word means, in order to decide if it applies.
</em><br />
<em class="quotelev2">&gt;&gt; Unfortunately such definitions usually amount to either a list of
</em><br />
<em class="quotelev2">&gt;&gt; synonyms for consciousness, or use the common human biological heritage
</em><br />
<em class="quotelev2">&gt;&gt; as a reference.  From the Wikipedia: &quot;Consciousness is a quality of the
</em><br />
<em class="quotelev2">&gt;&gt; mind generally regarded to comprise qualities such as subjectivity,
</em><br />
<em class="quotelev2">&gt;&gt; self-awareness, sentience, sapience, and the ability to perceive the
</em><br />
<em class="quotelev2">&gt;&gt; relationship between oneself and one's environment.&quot;  Here we have four
</em><br />
<em class="quotelev2">&gt;&gt; synonyms and one relational description which would arguably apply to
</em><br />
<em class="quotelev2">&gt;&gt; any computer system that has environmental sensors, unless &quot;perceive&quot;
</em><br />
<em class="quotelev2">&gt;&gt; is also merely another synonym for conscious perception.
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt; It looks to me like AIs, even ones much more sophisticated than I am
</em><br />
<em class="quotelev2">&gt;&gt; describing here, are going to have a hard time deciding whether they
</em><br />
<em class="quotelev2">&gt;&gt; are conscious in the human sense.  Since humans seem essentially unable
</em><br />
<em class="quotelev2">&gt;&gt; to describe consciousness in any reasonable operational terms, there
</em><br />
<em class="quotelev2">&gt;&gt; doesn't seem any acceptable way for an AI to decide whether the word
</em><br />
<em class="quotelev2">&gt;&gt; applies to itself.
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt; And given this failure, it calls into question the ease with which
</em><br />
<em class="quotelev2">&gt;&gt; humans assert that they are conscious.  How do we really know that
</em><br />
<em class="quotelev2">&gt;&gt; we are conscious?  For example, how do we know that what we call
</em><br />
<em class="quotelev2">&gt;&gt; consciousness is what everyone else calls consciousness?  I am worried
</em><br />
<em class="quotelev2">&gt;&gt; that many people believe they are conscious simply because as children,
</em><br />
<em class="quotelev2">&gt;&gt; they were told they were conscious.  They were told that consciousness
</em><br />
<em class="quotelev2">&gt;&gt; is the difference between being awake and being asleep, and assume on
</em><br />
<em class="quotelev2">&gt;&gt; that basis that when they are awake they are conscious.  Then all those
</em><br />
<em class="quotelev2">&gt;&gt; other synonyms are treated the same way.
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt; Yet most humans would not admit to any doubt that they are conscious.
</em><br />
<em class="quotelev2">&gt;&gt; For such a slippery and seemingly undefinable concept, it seems odd
</em><br />
<em class="quotelev2">&gt;&gt; that people are so sure of it.  Why, then, can't an AI achieve a similar
</em><br />
<em class="quotelev2">&gt;&gt; degree of certainty?  Do you think a properly programmed AI would ever
</em><br />
<em class="quotelev2">&gt;&gt; say, yes, I am conscious, because I have subjectivity, self-awareness,
</em><br />
<em class="quotelev2">&gt;&gt; sentience, sapience, etc., and I know this because it is just inherent in
</em><br />
<em class="quotelev2">&gt;&gt; my artificial brain?  Presumably we could program the AI to say this,
</em><br />
<em class="quotelev2">&gt;&gt; and to believe it (in whatever sense that word applies), but is it
</em><br />
<em class="quotelev2">&gt;&gt; something an AI could logically conclude?
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt; Hal
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt;     
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev2">&gt; &gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;   
</em><br />
<br /><br />--~--~---------~--~----~------------~-------~--~----~
<br />
You received this message because you are subscribed to the Google Groups &quot;Everything List&quot; group.
<br />
To post to this group, send email to everything-list.domain.name.hidden
<br />
To unsubscribe from this group, send email to everything-list-unsubscribe.domain.name.hidden
<br />
For more options, visit this group at <a href="http://groups.google.com/group/everything-list?hl=en">http://groups.google.com/group/everything-list?hl=en</a>
<br />
-~----------~----~----~----~------~----~------~--~---
<br />
<span id="received"><dfn>Received on</dfn> Sat Jun 02 2007 - 17:57:10 PDT</span>
</div>
<!-- body="end" -->
<div class="foot">
<map id="navbarfoot" name="navbarfoot" title="Related messages">
<ul class="links">
<li><dfn>This message</dfn>: [ <a href="#start13438">Message body</a> ]</li>
<!-- lnext="start" -->
<li><dfn>Next message</dfn>: <a href="13439.html" title="Next message in the list">Pete Carlton: "Re: Asifism"</a></li>
<li><dfn>Previous message</dfn>: <a href="13437.html" title="Previous message in the list">Quentin Anciaux: "Re: How would a computer know if it were conscious?"</a></li>
<li><dfn>In reply to</dfn>: <a href="13437.html" title="Message to which this message replies">Quentin Anciaux: "Re: How would a computer know if it were conscious?"</a></li>
<!-- lnextthread="start" -->
<li><dfn>Next in thread</dfn>: <a href="13440.html" title="Next message in this discussion thread">Jason Resch: "Re: How would a computer know if it were conscious?"</a></li>
<!-- lreply="end" -->
</ul>
<ul class="links">
<li><a name="options3" id="options3"></a><dfn>Contemporary messages sorted</dfn>: [ <a href="date.html#msg13438" title="Contemporary messages by date">by date</a> ] [ <a href="index.html#msg13438" title="Contemporary discussion threads">by thread</a> ] [ <a href="subject.html#msg13438" title="Contemporary messages by subject">by subject</a> ] [ <a href="author.html#msg13438" title="Contemporary messages by author">by author</a> ] [ <a href="attachment.html" title="Contemporary messages by attachment">by messages with attachments</a> ]</li>
</ul>
</map>
</div>
<!-- trailer="footer" -->
<p><small><em>
This archive was generated by <a href="http://www.hypermail-project.org/">hypermail 2.3.0</a>
: Fri Feb 16 2018 - 13:20:14 PST
</em></small></p>
</body>
</html>
