<?xml version="1.0" encoding="iso-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" />
<meta name="generator" content="hypermail 2.3.0, see http://www.hypermail-project.org/" />
<title>RE: Am I a token or a type? from Colin Hales on 2002-08-07 (everything)</title>
<meta name="Author" content="Colin Hales (colin.domain.name.hidden)" />
<meta name="Subject" content="RE: Am I a token or a type?" />
<meta name="Date" content="2002-08-07" />
<style type="text/css">
/*<![CDATA[*/
/* To be incorporated in the main stylesheet, don't code it in hypermail! */
body {color: black; background: #ffffff}
dfn {font-weight: bold;}
pre { background-color:inherit;}
.head { border-bottom:1px solid black;}
.foot { border-top:1px solid black;}
th {font-style:italic;}
table { margin-left:2em;}map ul {list-style:none;}
#mid { font-size:0.9em;}
#received { float:right;}
address { font-style:inherit ;}
/*]]>*/
.quotelev1 {color : #990099}
.quotelev2 {color : #ff7700}
.quotelev3 {color : #007799}
.quotelev4 {color : #95c500}
.period {font-weight: bold}
</style>
</head>
<body>
<div class="head">
<h1>RE: Am I a token or a type?</h1>
<!-- received="Wed Aug  7 17:38:28 2002" -->
<!-- isoreceived="20020808003828" -->
<!-- sent="Thu, 8 Aug 2002 10:19:33 +1000" -->
<!-- isosent="20020808001933" -->
<!-- name="Colin Hales" -->
<!-- email="colin.domain.name.hidden" -->
<!-- subject="RE: Am I a token or a type?" -->
<!-- id="000801c23e71$45b8a2f0$0100a8c0.domain.name.hidden" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="005101c23bdc$a2795620$da9c143e.domain.name.hidden" -->
<!-- expires="-1" -->
<map id="navbar" name="navbar">
<ul class="links">
<li>
<dfn>This message</dfn>:
[ <a href="#start3900" name="options1" id="options1" tabindex="1">Message body</a> ]
 [ More options (<a href="#options2">top</a>, <a href="#options3">bottom</a>) ]
</li>
<li>
<dfn>Related messages</dfn>:
<!-- unext="start" -->
[ <a href="3901.html" accesskey="d" title="Bruno Marchal: &quot;RE: Rationality of free will in the multiverse&quot;">Next message</a> ]
[ <a href="3899.html" title="Bruno Marchal: &quot;Re: Time, causality, posets&quot;">Previous message</a> ]
[ <a href="3893.html" title="Lennart Nilsson: &quot;Re: Am I a token or a type?&quot;">In reply to</a> ]
<!-- unextthread="start" -->
[ <a href="3894.html" accesskey="t" title="Lennart Nilsson: &quot;Re: Am I a token or a type?&quot;">Next in thread</a> ]
<!-- ureply="end" -->
</li>
</ul>
</map>
<ul class="links">
<li><a name="options2" id="options2"></a><dfn>Contemporary messages sorted</dfn>: [ <a href="date.html#msg3900" title="Contemporary messages by date">by date</a> ] [ <a href="index.html#msg3900" title="Contemporary discussion threads">by thread</a> ] [ <a href="subject.html#msg3900" title="Contemporary messages by subject">by subject</a> ] [ <a href="author.html#msg3900" title="Contemporary messages by author">by author</a> ] [ <a href="attachment.html" title="Contemporary messages by attachment">by messages with attachments</a> ]</li>
</ul>
</div>
<!-- body="start" -->
<div class="mail">
<address class="headers">
<span id="from">
<dfn>From</dfn>: Colin Hales &lt;<a href="mailto:colin.domain.name.hidden?Subject=RE%3A%20Am%20I%20a%20token%20or%20a%20type%3F">colin.domain.name.hidden</a>&gt;
</span><br />
<span id="date"><dfn>Date</dfn>: Thu, 8 Aug 2002 10:19:33 +1000</span><br />
</address>
<br />
Lennart wrote:
<br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; If what you say means that what is possible for a sentient being given
</em><br />
<em class="quotelev1">&gt; sufficiently advanced technique to percieve is what can
</em><br />
<em class="quotelev1">&gt; possibly exist,including the feeling
</em><br />
<em class="quotelev1">&gt; we have what it is like being us, I'm with you. The description and the
</em><br />
<em class="quotelev1">&gt; descibed belong to the same dimension. The border between them becomes
</em><br />
relative,
<br />
<em class="quotelev1">&gt; just like the Now in relativity theory.
</em><br />
<em class="quotelev1">&gt; So what is the uppermost level for AI?
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Lennart
</em><br />
<em class="quotelev1">&gt;
</em><br />
<br />In answer to Lennart and as commentary to Bruno at al&#x2026;..
<br />
<br />This issue resonated a bit with my current thinking in various areas. I have
<br />
taken the time to &#x2018;dump it&#x2019; to paper, so that I may get on to more practical
<br />
matters &#x2013; to let it leave me alone! The following meandering word dump is
<br />
what happened, as copied from my design/journal/book/whatever the hell it
<br />
is. It&#x2019;s rather long but I hope, worth a read.
<br />
<br /><em class="quotelev1">&gt;Am I a token or a type?
</em><br />
<em class="quotelev1">&gt;How can an abstraction be felt?
</em><br />
<em class="quotelev1">&gt;What is the uppermost level for an AI?
</em><br />
<br />Our culture is that 'observer' is automatically taken as 'outside' the
<br />
observed system. We characterise the observations with cognition itself
<br />
assumed and tacit. What I am trying to get people to do is to treat the
<br />
observer as part of the system - indeed created _of_ the same system. We
<br />
observe from within. Our symbols and mathematical idealisations become the
<br />
means of communicating observations from one observer to another, using the
<br />
common perception mechanisms of a given &#x2018;type&#x2019; of observer. This
<br />
mathematical description in no way communicates 'what it is like' to be an
<br />
observer and never will, just as any number of words describing flowers will
<br />
never capture what it is truly like to be a flower or even a human in the
<br />
presence of a flower.
<br />
=====================================
<br />
Imagine, {and with this opening word I have already fallen into the
<br />
traditional deception of the thought experiment!} that the
<br />
&#x2018;everything-listers&#x2019; rest on their backs on a grassy slope, musing the
<br />
clouds. Let&#x2019;s say in summer, near Heidelberg, Germany. The beer halls and
<br />
many Rhine valley wines beckon. (We may as well make this fun!).
<br />
Collectively we see the following in the sky:
<br />
<br />cloud cloud cloud
<br />
cloud cloud cloud
<br />
cloud
<br />
cloud
<br />
cloud cloud
<br />
cloud cloud
<br />
cloud
<br />
cloud
<br />
cloud cloud cloud
<br />
cloud cloud cloud
<br />
<br />The universe has randomly contrived to arrange a cloud that we all recognise
<br />
as the letter E. The fact of the resonance in the minds of us all &#x2013; the
<br />
&#x2018;meaning&#x2019; of the shape &#x2013; is just that: a resonance in the minds of us all as
<br />
computational entities &#x2013; observers - and no more. The E shape is a natural
<br />
spontaneous occurrence, the result of what we like to call the output of a
<br />
computational entity we call the universe. It has no meaning to the universe
<br />
that created it (see *** below) &#x2013; it is, of itself, no more meaningful than
<br />
any other cloud shape. The presence of the cloud itself is where the
<br />
computational expression ends, for the universe at the spatial/temporal
<br />
scale of the everything-list observers.
<br />
<br />In my previous post I proposed that we shift the whole viewpoint of
<br />
abstraction by recognising our true place as observers. &#x201c;Throw a net around
<br />
any chunk of any universe and then consider the observer &#x2013; the computational
<br />
entity &#x2013; thus created&#x201d;. The boundaries of a computational entity can be
<br />
found by growing the boundaries until the computational causality-modelling
<br />
capacity ceases to increase (unfortunately you need another observer to do
<br />
this, and so on &#x2013; lets just say there is one!). At that boundary you then
<br />
continue to increase the boundary to some arbitrary level, which is the
<br />
sensory interface to the rest of the universe. If we are talking about
<br />
construction of a human observer then the boundary of the computational
<br />
component is the brain and the sensory boundary is the external boundary of
<br />
a human (no tools or proprioception just yet, please). The only reason for
<br />
this boundary is that there are multiple observers (tokens) of &#x2018;type&#x2019; people
<br />
and they all interact (communicate) at that level .ie. communications have
<br />
been calibrated for an entity with that boundary (- an example being the
<br />
cloud E previously described an arbitrary shape mutually agreed as
<br />
representing something for as yet undefined purposes).
<br />
<br />In this way we &#x2018;are&#x2019; a chunk of the universe we observe. The same
<br />
computational rules that created the clouds created us. What we call
<br />
reality, when viewed like this, becomes the &#x2018;thoughts&#x2019; of the universe.
<br />
Subscribers to the many flavours of QM, MWI, multiverses will say that the
<br />
universe has many &#x2018;friends&#x2019; (&#x2018;tokens&#x2019; of &#x2018;type&#x2019; universe) to &#x2018;talk to&#x2019;. For
<br />
this discussion let us consider there is only one &#x2013; our position as
<br />
observers places us there even if we are the sporadic output of the UD
<br />
beasty (Thanks Hal for spelling it out!). *** (from above) We have it that
<br />
because the universe has no-one to talk to, its &#x2018;thoughts&#x2019; need have no
<br />
calibration &#x2013; standardisation. Within the universe the &#x2018;thoughts&#x2019; that are
<br />
humans have a uniformity &#x2013; a standardisation level- that engenders our
<br />
survival, as it is that very similarity and the communication between us
<br />
(from genes up) that allows the continued existence of &#x2018;us&#x2019; tokens of type
<br />
&#x2018;people&#x2019;.
<br />
<br />Clouds are just clouds: billowing fractals &#x2013;communicated between ourselves
<br />
in symbols collectively known as the &#x2018;abstraction&#x2019; called flow dynamics by
<br />
us observers - scudding across a summer sky. As computational entities they
<br />
are comparatively shallow and any notional perception is very &#x2018;dim&#x2019;.
<br />
<br />This is the practical reality for us and, with apologies, my selfish focus
<br />
of interest as an &#x2018;AI constructor&#x2019;.
<br />
<br />We are now able to, I think, get a little resolution on the &#x2018;type&#x2019;/&#x2019;token&#x2019;
<br />
distinction. What I have so far discussed is the agreement between a group
<br />
of observers (everything-listers on the grassy slope) that they are, to a
<br />
high degree of certainty, in the presence of &#x2018;E&#x2019;-ness. What exactly is
<br />
happening here?
<br />
<br />Light from the E-cloud hits the senses of each computational entity.
<br />
A-priori training, which occurs as a result of being a chunk of universe
<br />
with an appropriate level of causality modelling and by also co-existing
<br />
with other like entities has created a causality link to an internal
<br />
symbolic representation of E or E-ness. The awareness of E may then be
<br />
conveyed to other like entities via verbal communication: a mapping through
<br />
the air.
<br />
<br />Fine &#x2013; but what exactly is the causality modeller? Conceptually is it any
<br />
different to the cloud? Whereas the cloud is a constellation of water
<br />
droplets that just happens to have taken the E shape, in the mind we have
<br />
contrived &#x2013; learned &#x2013; a constellation of neural firings that has the same
<br />
significance &#x2013; a significance calibrated for the very purposes of
<br />
standardised communication and behaviour. We have also learned of (modelled)
<br />
the essential features of E-ness that have to be present &#x2013; the pattern of
<br />
neural firings &#x2013; that connects us to the awareness of the presence of
<br />
E-ness. The abstraction E &#x2013; a thing created by mutual agreement between
<br />
computational entities &#x2013; then controls/determines behaviour. E does not
<br />
exist. The matter that has been configured to represent E- ness exists.
<br />
<br />So here is where my assertion has surfaced. E-ness is an abstraction. It has
<br />
no reality of its own. The efficacy of this arrangement is profound. Large
<br />
scale and far reaching understanding of causality can be contained within a
<br />
small symbolic (abstracting) computational entity of sufficient
<br />
sophistication. The causality model that &#x2018;is&#x2019; the mind, structured of the
<br />
same matter as the rest of the universe, has created a personalised,
<br />
customised, set of relationships that, if characterised using symbolic
<br />
mathematics would represent a set of &#x2018;laws of the mind&#x2019; that are NOT laws of
<br />
physics. These laws operate to create our internal thoughts &#x2013; the equivalent
<br />
of the universe&#x2019;s laws of physics, that allow the universe to &#x2018;think&#x2019;
<br />
clouds. The byproduct of this is that we can think 'outside' the system -
<br />
imagine arbitrary instances of things that do not obey what we understand to
<br />
be normal causality in our universe.
<br />
<br />Some say that the universe is a massive cellular automata. Regardless of the
<br />
accuracy of this we can say with some certainty that a configuration of much
<br />
larger-scale cellular automata (neurons) runs a customised set of adaptive
<br />
symbol-manipulation 'equations' that is &#x2018;mind&#x2019; including what we describe as
<br />
human-level cognitive capacity. In the absence of any other processor
<br />
architecture, an AI with a near-human &#x2018;what it is like&#x2019; experience would
<br />
clearly be based optimally on cellular automata.
<br />
<br />So where does this lead? The single most significant thing is that if you
<br />
want to build an AI that displays human level cognition you do NOT create an
<br />
artificial processor manipulating the abstract symbols of the human mind.
<br />
This is what the computer scientist does when code-crashing &#x2018;AI&#x2019;. This is
<br />
like taking the E-cloud above and encoding it, such as I have done in my
<br />
email as a bunch of pixels, and then manipulating the pixels! What real
<br />
understanding of our universe can an AI gain from this? The AI is modelling
<br />
NOT our universe, but the causality between artificial symbols we have
<br />
created to communicate our understanding to each other in the real universe.
<br />
It&#x2019;s a whole level of indirection removed from reality. The absolute best
<br />
that can be obtained from this is clearly a simulation. A sufficiently
<br />
sophisticated AI of this type will sense and feel not our universe, but
<br />
whatever &#x2018;it is like&#x2019; to embody causal relationships between a set of human
<br />
generated artificial symbols!
<br />
<br />Role play what it would be like to be two of these simulations. Would they
<br />
be able to communicate to each other realistically about 'what it is like to
<br />
be human'? Absolutely not, except as notional 'race-callers'. They would
<br />
connect with each other at an intimate level 'what it is like' to be them.
<br />
This is something from which humans are excluded, just as 'what it is like
<br />
to be a dog or Deep-Blue' is excluded from our cognitive envelope.
<br />
<br />Here is an &quot;AI Rule #1&quot;: An AI with human level cognition of our universe
<br />
has to model the universe directly, through its own senses as an observer at
<br />
the same spatio-temporal level .ie. with a processing created &#x2013;of- the
<br />
universe it is supposed to model.
<br />
<br />This means that the only potential strong-AI will come from
<br />
a)	those AI workers starting from a robotics standpoint &#x2013; where the sensory
<br />
environment is implicitly part of the AI and where the AI is as physically
<br />
similar as practical.
<br />
b)	Cellular automata approach.
<br />
<br />This means that strong-AI will not come from
<br />
c)	AI without sophisticated sensory/actuation connectivity to our universe.
<br />
d)	Von-Neumann sequential symbolic processing. Whilst creating simulations
<br />
of cognition that may behave &#x2018;as if&#x2019; they were in our universe (to &#x2018;us&#x2019;
<br />
observers in that universe), simply cannot have a &#x2018;what it is like&#x2019;
<br />
experience that we have of our universe or any deep understanding of a human
<br />
&#x2019;s place in our universe. They will have their own &#x2018;what it is like&#x2019;
<br />
experience that we will never see or comprehend. An AI created like this
<br />
will be completely unable to comprehend the human context, as we will be
<br />
unable to comprehend the AI&#x2019;s context. It seems that this avenue would
<br />
indeed create a version of the legendary &#x2018;zombie&#x2019;.
<br />
<br />What about a hybrid? For example: A cellular automata based AI where each
<br />
cell is a mini-Von-Neumann architecture. This would get a little closer to
<br />
human equivalence.
<br />
<br />When you get to this level of consideration you have to look at what you are
<br />
trying to achieve. It becomes clearer that the only computational entity
<br />
with the same &#x2018;what it is like&#x2019; description as a human will be something
<br />
constructed identically to a human. A human! The question becomes more: What
<br />
cognitive capacity do you want to achieve, whilst making a useful
<br />
computational entity?
<br />
<br />For safety reasons &#x2013; the closer to human the better.
<br />
<br /><br />With this in mind a set of working hypotheses are as follows:
<br />
a)	Observers are computational entities constructed from the system being
<br />
observed with cognitive sophistication commensurate with computational
<br />
sophistication, defined as the capacity of a computational entity to model
<br />
the rest of the universe.
<br />
b)	That morphology matters: The more similarities there are between any two
<br />
computational entities, the easier it will be for each to cognitively handle
<br />
the other entity .ie. control behaviour to suit the needs of each
<br />
computational entity.
<br />
c)	That the sensory/actuation feeds that connect any two computational
<br />
entities allow communication and are also constructed from the system.
<br />
d)	&#x2018;Typing&#x2019; - the descriptions (symbols, which includes mathematical
<br />
abstractions) that flow between observers is only that, in no way conveys
<br />
'what it is like to be' the phenomenon thus described and finally &#x2013; in no
<br />
way does an abstraction (a &#x2018;typing&#x2019;) instantiate or run an abstraction.
<br />
<br />Answers&#x2026;&#x2026;
<br />
If you take the above on board, I hold that you are in a better position to
<br />
understand the role of 'mind' and 'subjectivity' in the scheme of things.
<br />
Answers lead some where:
<br />
<br />Am I a token or a type?
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If you can ask the question you are instantiated and you answer &#x2018;Token&#x2019; to
<br />
yourself. Another instantiated like type would answer &#x2018;Token&#x2019; (for you) as
<br />
well, if asked. (An abstraction, by definition, only exists in the mind of
<br />
an observer as symbology without instantiation, it is not possible to &#x2018;be&#x2019; a
<br />
type and view a token. Indeed this is the very reason the distinction is
<br />
useful.)
<br />
<br />How can an abstraction be felt?
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;An abstraction cannot be felt. It is a communication between observers
<br />
transmitted as symbols.
<br />
<br />What is the uppermost level for an AI?
<br />
There is no &#x2018;uppermost level&#x2019;, except that which we configure for our own
<br />
utility. If we need an AI to be as useful as a human then it needs to be at
<br />
least as computationally sophisticated and physically similar (in terms of
<br />
perception/actuation) to a human as possible.
<br />
<br />Is existence &#x2018;created&#x2019; in the mind of the observer?
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;No. The result of an observation is created in the mind of an observer.
<br />
<br />My daughter ate a teddy bear biscuit. The teddybear had a smile. Do you
<br />
think she tasted the smile?
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;No.
<br />
<br />Will &#x2018;AI&#x2019; like CYC or ALICEBOT ever have any real human cognition?
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;No.
<br />
<br />A lovely example of this is in &#x201c;Godel Escher Bach&#x201d; and the anteater&#x2019;s &#x201c;Aunt
<br />
Hillary&#x201d;. &#x201c;Aunt Hillary&#x201d; is a termite nest with which the anteater has a
<br />
primitive cognitive relationship. The ants are the cellular automata and
<br />
when a &#x2018;net&#x2019; is thrown over it a definite functional computational entity
<br />
can be delineated, courtesy of the biologists. Hofstader nicely explores the
<br />
way the communication occurs and the 'symbols' generated during the process.
<br />
<br />Lastly I have realised just what a tough job that philosophers have always
<br />
had in describing the mind. All they have to go by is language &#x2013; a set of
<br />
abstract symbols remapping brain states and thus losing all their ability to
<br />
communicate or indeed define the &#x2018;what it is like&#x2019; subjective experience, a
<br />
job of description which was, after all, doomed from the start!
<br />
=======================================================
<br />
stop stop, no more, you say.......
<br />
I hope you made it this far.....
<br />
<br />:-)
<br />
<br />Colin
<br />
<span id="received"><dfn>Received on</dfn> Wed Aug 07 2002 - 17:38:28 PDT</span>
</div>
<!-- body="end" -->
<div class="foot">
<map id="navbarfoot" name="navbarfoot" title="Related messages">
<ul class="links">
<li><dfn>This message</dfn>: [ <a href="#start3900">Message body</a> ]</li>
<!-- lnext="start" -->
<li><dfn>Next message</dfn>: <a href="3901.html" title="Next message in the list">Bruno Marchal: "RE: Rationality of free will in the multiverse"</a></li>
<li><dfn>Previous message</dfn>: <a href="3899.html" title="Previous message in the list">Bruno Marchal: "Re: Time, causality, posets"</a></li>
<li><dfn>In reply to</dfn>: <a href="3893.html" title="Message to which this message replies">Lennart Nilsson: "Re: Am I a token or a type?"</a></li>
<!-- lnextthread="start" -->
<li><dfn>Next in thread</dfn>: <a href="3894.html" title="Next message in this discussion thread">Lennart Nilsson: "Re: Am I a token or a type?"</a></li>
<!-- lreply="end" -->
</ul>
<ul class="links">
<li><a name="options3" id="options3"></a><dfn>Contemporary messages sorted</dfn>: [ <a href="date.html#msg3900" title="Contemporary messages by date">by date</a> ] [ <a href="index.html#msg3900" title="Contemporary discussion threads">by thread</a> ] [ <a href="subject.html#msg3900" title="Contemporary messages by subject">by subject</a> ] [ <a href="author.html#msg3900" title="Contemporary messages by author">by author</a> ] [ <a href="attachment.html" title="Contemporary messages by attachment">by messages with attachments</a> ]</li>
</ul>
</map>
</div>
<!-- trailer="footer" -->
<p><small><em>
This archive was generated by <a href="http://www.hypermail-project.org/">hypermail 2.3.0</a>
: Fri Feb 16 2018 - 13:20:07 PST
</em></small></p>
</body>
</html>
