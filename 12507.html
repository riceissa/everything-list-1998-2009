<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1" />
<meta name="generator" content="hypermail 2.3.0, see http://www.hypermail-project.org/" />
<title>Re: The Meaning of Life from Bruno Marchal on 2007-01-10 (everything)</title>
<meta name="Author" content="Bruno Marchal (marchal.domain.name.hidden)" />
<meta name="Subject" content="Re: The Meaning of Life" />
<meta name="Date" content="2007-01-10" />
<style type="text/css">
/*<![CDATA[*/
/* To be incorporated in the main stylesheet, don't code it in hypermail! */
body {color: black; background: #ffffff}
dfn {font-weight: bold;}
pre { background-color:inherit;}
.head { border-bottom:1px solid black;}
.foot { border-top:1px solid black;}
th {font-style:italic;}
table { margin-left:2em;}map ul {list-style:none;}
#mid { font-size:0.9em;}
#received { float:right;}
address { font-style:inherit ;}
/*]]>*/
.quotelev1 {color : #990099}
.quotelev2 {color : #ff7700}
.quotelev3 {color : #007799}
.quotelev4 {color : #95c500}
.period {font-weight: bold}
</style>
</head>
<body>
<div class="head">
<h1>Re: The Meaning of Life</h1>
<!-- received="Wed Jan 10 08:20:57 2007" -->
<!-- isoreceived="20070110162057" -->
<!-- sent="Wed, 10 Jan 2007 14:20:36 +0100" -->
<!-- isosent="20070110132036" -->
<!-- name="Bruno Marchal" -->
<!-- email="marchal.domain.name.hidden" -->
<!-- subject="Re: The Meaning of Life" -->
<!-- id="6a54d4c0bb9ce7ff91fc3bad1a91a895.domain.name.hidden" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="BAY124-W197F66655F01FF8C59CAF2D2BC0.domain.name.hidden" -->
<!-- expires="-1" -->
<map id="navbar" name="navbar">
<ul class="links">
<li>
<dfn>This message</dfn>:
[ <a href="#start12507" name="options1" id="options1" tabindex="1">Message body</a> ]
 [ More options (<a href="#options2">top</a>, <a href="#options3">bottom</a>) ]
</li>
<li>
<dfn>Related messages</dfn>:
<!-- unext="start" -->
[ <a href="12508.html" accesskey="d" title="John M: &quot;Re: Evil ? (was: Hypostases&quot;">Next message</a> ]
[ <a href="12506.html" title="Bruno Marchal: &quot;Re: The Meaning of [your] Life&quot;">Previous message</a> ]
[ <a href="12500.html" title="Stathis Papaioannou: &quot;RE: The Meaning of Life&quot;">In reply to</a> ]
<!-- unextthread="start" -->
[ <a href="12513.html" accesskey="t" title="Stathis Papaioannou: &quot;RE: The Meaning of Life&quot;">Next in thread</a> ]
<!-- ureply="end" -->
</li>
</ul>
</map>
<ul class="links">
<li><a name="options2" id="options2"></a><dfn>Contemporary messages sorted</dfn>: [ <a href="date.html#msg12507" title="Contemporary messages by date">by date</a> ] [ <a href="index.html#msg12507" title="Contemporary discussion threads">by thread</a> ] [ <a href="subject.html#msg12507" title="Contemporary messages by subject">by subject</a> ] [ <a href="author.html#msg12507" title="Contemporary messages by author">by author</a> ] [ <a href="attachment.html" title="Contemporary messages by attachment">by messages with attachments</a> ]</li>
</ul>
</div>
<!-- body="start" -->
<div class="mail">
<address class="headers">
<span id="from">
<dfn>From</dfn>: Bruno Marchal &lt;<a href="mailto:marchal.domain.name.hidden?Subject=Re%3A%20The%20Meaning%20of%20Life">marchal.domain.name.hidden</a>&gt;
</span><br />
<span id="date"><dfn>Date</dfn>: Wed, 10 Jan 2007 14:20:36 +0100</span><br />
</address>
<br />
Stathis,

<br />
I will ask you to be patient until next wednesday because I am busy  
<br />
right now. I think we agree on many things, and this is an opportunity  
<br />
to search where exactly we diverge, if we diverge.
<br />
For example I disagree with the expression &quot;brain are conscious&quot;, but I  
<br />
am read you more carefully to see if this is just a way of talk, or if  
<br />
this hides some deeper divergence.

<br />
Also, when you say:

<br />
<em class="quotelev1">&gt; If consciousness is decoupled from physical activity, this means that  
</em><br />
<em class="quotelev1">&gt; our conscious experience would be unchanged if the apparent physical  
</em><br />
<em class="quotelev1">&gt; activity of our brain were not really there. This would make all the  
</em><br />
<em class="quotelev1">&gt; evidence of our senses on which we base the existence of a physical  
</em><br />
<em class="quotelev1">&gt; world illusory: we would have these same experiences if the physical  
</em><br />
<em class="quotelev1">&gt; world suddenly disappeared or never existed in the first place. We can  
</em><br />
<em class="quotelev1">&gt; keep (1) and (2) because I qualified them with the word &quot;appears&quot;, but  
</em><br />
<em class="quotelev1">&gt; the necessity of a separate physical reality accounting for this  
</em><br />
<em class="quotelev1">&gt; appearance goes.

</em><br />
I really don't understand. It is not because we lost the primitive  
<br />
physical reality that we lost a reality. Remember that my point is that  
<br />
IF we keep comp, then indeed we must explain the appearance of physical  
<br />
reality, without conjecturing such a reality as a primitive one, and  
<br />
then I show how to do that (including some results encouraging for the  
<br />
comp hyp.).

<br />
It is a way to sum up my point. If we are machine, then the assumption  
<br />
of a physical reality cannot explain why we believe in a physical  
<br />
reality. Indeed the believe in a physical reality is something  
<br />
emergent. With comp the mind-body problem is two times more difficult  
<br />
than without, because we have to explain &quot;matter appearance&quot; without  
<br />
assuming it.

<br />
Why do you think all the evidence of our senses would be illusory?  
<br />
Nobody has seen &quot;primitive matter&quot;. I thought even just Kant made this  
<br />
clear.
<br />
Paper by physicists never assume &quot;primitive matter&quot;, except in some  
<br />
implicit background which play no role in the ideas. All papers by  
<br />
physicists just propose relation between numbers, and eventually &quot;I&quot; do  
<br />
relate those numbers with some of my qualia (like the qualia of being  
<br />
convinced having seen a needle in such or such places. Primitive matter  
<br />
is a metaphysical concept, today, and with comp it is has been made  
<br />
scientific: indeed it is made quasi-contradictory.

<br />
I believe in the moon, but I don't believe the moon is made of atoms  
<br />
(this is just a good local description). I believe in atoms, but I  
<br />
don't believe atoms are made of elementary particles (this is just a  
<br />
good local description). I believe in elementary particles, but I don't  
<br />
believe elementary particles are made of strings (this could be a good  
<br />
local description), etc. (and with comp probably the &quot;etc&quot; is  
<br />
obligatory).
<br />
I believe in strings, but I don't believe strings are made of something  
<br />
else, except in some first approximation.
<br />
But with comp I do believe all that *must* emerge from the &quot;collective  
<br />
behavior&quot; of numbers.

<br />
Regards,

<br />
Bruno



<br />
Le 08-janv.-07, à 23:36, Stathis Papaioannou a écrit :

<br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Bruno Marchal writes:
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev4">&gt;&gt; &gt;&gt; Le 07-janv.-07, à 19:21, Brent Meeker a écrit :
</em><br />
<em class="quotelev1">&gt;&gt; &gt;&gt; &gt; And does it even have to be very good?  Suppose it made a sloppy  
</em><br />
<em class="quotelev4">&gt;&gt; &gt;&gt; copy &gt; of me that left out 90% of my memories - would it still be  
</em><br />
<em class="quotelev4">&gt;&gt; &gt;&gt; &quot;me&quot;?  How &gt; much fidelity is required for Bruno's argument?  I  
</em><br />
<em class="quotelev2">&gt;&gt; think &gt;&gt; not much.
</em><br />
<em class="quotelev4">&gt;&gt; &gt;&gt; The argument does not depend at all of the level of fidelity.  
</em><br />
<em class="quotelev2">&gt;&gt; Indeed &gt;&gt; I make clear (as much as possible) that comp is equivalent  
</em><br />
<em class="quotelev2">&gt;&gt; to the &gt;&gt; belief there is a level of substitution of myself  
</em><br />
<em class="quotelev2">&gt;&gt; (3-person) such &gt;&gt; that I (1-person) survive a functional  
</em><br />
<em class="quotelev2">&gt;&gt; substitution done at that &gt;&gt; level. Then I show no machine can know  
</em><br />
<em class="quotelev2">&gt;&gt; what is her level of &gt;&gt; substitution (and thus has to bet or guess  
</em><br />
<em class="quotelev2">&gt;&gt; about it).
</em><br />
<em class="quotelev4">&gt;&gt; &gt;&gt; This is also the reason why comp is not jeopardized by the idea  
</em><br />
<em class="quotelev2">&gt;&gt; that &gt;&gt; the environment is needed: just put the environment in the  
</em><br />
<em class="quotelev2">&gt;&gt; definition &gt;&gt; of my &quot;generalized brain&quot;.
</em><br />
<em class="quotelev4">&gt;&gt; &gt;&gt; Imagine someone who say that his brain is the entire galaxy, &gt;&gt;  
</em><br />
<em class="quotelev2">&gt;&gt; described at the level of all interacting quantum strings. This can  
</em><br />
<em class="quotelev4">&gt;&gt; &gt;&gt; be captured by giant (to say the least) but finite, rational  
</em><br />
<em class="quotelev2">&gt;&gt; complex &gt;&gt; matrices. Of course the thought experiment with the &quot;yes  
</em><br />
<em class="quotelev2">&gt;&gt; doctor&quot; will &gt;&gt; look very non-realist, but *in fine*, all what is  
</em><br />
<em class="quotelev2">&gt;&gt; needed (for the &gt;&gt; reversal) is that the Universal Dovetailer get  
</em><br />
<em class="quotelev2">&gt;&gt; through the state of &gt;&gt; my generalized brain, and the UD will get it  
</em><br />
<em class="quotelev2">&gt;&gt; even if my &quot;state&quot; is &gt;&gt; the state of the whole galaxy, or more.
</em><br />
<em class="quotelev4">&gt;&gt; &gt;&gt; If it happens that my state is the galaxy state AND that the  
</em><br />
<em class="quotelev2">&gt;&gt; galaxy &gt;&gt; state cannot be captured in a finite ('even giant) way(*),  
</em><br />
<em class="quotelev2">&gt;&gt; then we &gt;&gt; are just out of the scope of the comp- reasoning. This is  
</em><br />
<em class="quotelev2">&gt;&gt; possible &gt;&gt; because comp may be wrong.
</em><br />
<em class="quotelev3">&gt;&gt; &gt;
</em><br />
<em class="quotelev3">&gt;&gt; &gt; This is right, and it is perhaps a consequence of comp that &gt;  
</em><br />
<em class="quotelev2">&gt;&gt; computationalists did not brgain on. If the functional equivalent of  
</em><br />
<em class="quotelev3">&gt;&gt; &gt; my brain has to interact with the environment in the same way that  
</em><br />
<em class="quotelev2">&gt;&gt; I &gt; do then that puts a constraint what sort of machine it can be, as  
</em><br />
<em class="quotelev2">&gt;&gt; well &gt; as necessitating of course that it be an actual physical  
</em><br />
<em class="quotelev2">&gt;&gt; machine. For &gt; example, if as part of asserting my status as a  
</em><br />
<em class="quotelev2">&gt;&gt; conscious being I &gt; decide to lift my hand in the air when I see a  
</em><br />
<em class="quotelev2">&gt;&gt; red ball, then my &gt; functional replacement must (at least) have  
</em><br />
<em class="quotelev2">&gt;&gt; photoreceptors which send &gt; a signal to a central processor which  
</em><br />
<em class="quotelev2">&gt;&gt; then sends a motor signal to its &gt; hand. If it fails the red ball  
</em><br />
<em class="quotelev2">&gt;&gt; test, then it isn't functionally &gt; equivalent to me.
</em><br />
<em class="quotelev3">&gt;&gt; &gt; However, what if you put the red ball, the hand and the whole &gt;  
</em><br />
<em class="quotelev2">&gt;&gt; environment inside the central processor? You program in data which &gt;  
</em><br />
<em class="quotelev2">&gt;&gt; tells it is seeing a red ball, it sends a signal to what it thinks is  
</em><br />
<em class="quotelev3">&gt;&gt; &gt; its hand, and it receives visual and proprioceptive data telling it  
</em><br />
<em class="quotelev2">&gt;&gt; it &gt; has successfully raised the hand. Given that this self-contained  
</em><br />
<em class="quotelev3">&gt;&gt; &gt; machine was derived from a known computer architecture with known &gt;  
</em><br />
<em class="quotelev2">&gt;&gt; sensors and effectors, we would know what it was thinking by &gt;  
</em><br />
<em class="quotelev2">&gt;&gt; eavesdropping on its internal processes. But if we didn't have this &gt;  
</em><br />
<em class="quotelev2">&gt;&gt; knowledge, is there any way, even in theory, that we could figure it  
</em><br />
<em class="quotelev3">&gt;&gt; &gt; out? The answer in general is &quot;no&quot;: without benefit of  
</em><br />
<em class="quotelev2">&gt;&gt; environmental &gt; interaction, or an instruction manual, there is no  
</em><br />
<em class="quotelev2">&gt;&gt; way to assign &gt; meaning to the workings of a machine and there is no  
</em><br />
<em class="quotelev2">&gt;&gt; way to know &gt; anything about its consciousness.
</em><br />
<em class="quotelev2">&gt;&gt; Up to here I do agree.
</em><br />
<em class="quotelev3">&gt;&gt; &gt; The corollary of this is that under the right interpretation a  
</em><br />
<em class="quotelev2">&gt;&gt; machine &gt; could have any meaning or any consciousness.
</em><br />
<em class="quotelev2">&gt;&gt; I don't think that this corollary follows. Unless you are postulating  
</em><br />
<em class="quotelev2">&gt;&gt; a &quot;physical world&quot; having some special property (and then the  
</em><br />
<em class="quotelev2">&gt;&gt; question is: what are your axiom for that physical realm, and where  
</em><br />
<em class="quotelev2">&gt;&gt; does those axioms come from). Even in classical physics, where a  
</em><br />
<em class="quotelev2">&gt;&gt; point can move along &quot;all real numbers&quot;, I don't see any reason such  
</em><br />
<em class="quotelev2">&gt;&gt; move can represent any computation. Of course I consider a  
</em><br />
<em class="quotelev2">&gt;&gt; computation as being something non physical, and essentially  
</em><br />
<em class="quotelev2">&gt;&gt; discrete, at the start. The physical and continuous aspect comes from  
</em><br />
<em class="quotelev2">&gt;&gt; the fact that any computation is &quot;embedded&quot; into an infinity of  
</em><br />
<em class="quotelev2">&gt;&gt; &quot;parallel&quot; computations (those generated by the UD).
</em><br />
<em class="quotelev2">&gt;&gt; Brent says that the &quot;evil problem&quot; is a problem only for those who  
</em><br />
<em class="quotelev2">&gt;&gt; postulate an omniscient and omnipotent good god. I believe that  
</em><br />
<em class="quotelev2">&gt;&gt; somehow a large part of the &quot;mind/body&quot; problem comes from our  
</em><br />
<em class="quotelev2">&gt;&gt; (instinctive) assumption of a basic (primitive) physical reality.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Yes, but you're assuming here that which I (you?) set out to prove. In  
</em><br />
<em class="quotelev1">&gt; your UDA you do not explicitly start out with &quot;there is no physical  
</em><br />
<em class="quotelev1">&gt; world&quot; but arrive at this as a conclusion. Consider the following  
</em><br />
<em class="quotelev1">&gt; steps:
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; 1. There appears to be a physical world
</em><br />
<em class="quotelev1">&gt; 2. Some of the substructures in this world appear to be conscious,  
</em><br />
<em class="quotelev1">&gt; namely brains
</em><br />
<em class="quotelev1">&gt; 3. The third person behaviour of these brains can be copied by an  
</em><br />
<em class="quotelev1">&gt; appropriate digital computer
</em><br />
<em class="quotelev1">&gt; 4. The first person experience of these brains would also thereby be  
</em><br />
<em class="quotelev1">&gt; copied by such a computer
</em><br />
<em class="quotelev1">&gt; 5. The first person experience of the computer would remain unchanged  
</em><br />
<em class="quotelev1">&gt; if the third person behaviour were made part of the program
</em><br />
<em class="quotelev1">&gt; 6. But this would mean there is no way to attach meaning or  
</em><br />
<em class="quotelev1">&gt; consciousness to the self-contained computer - it could be thinking of  
</em><br />
<em class="quotelev1">&gt; a red ball, blue ball, or no ball
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; This is an unexpected result, which can be resolved several ways:
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; 7. (3) is incorrect, and we need not worry about the rest
</em><br />
<em class="quotelev1">&gt; 8. (4) is incorrect, and we need not worry about the rest
</em><br />
<em class="quotelev1">&gt; 9. (5) is incorrect, and we need not worry about the rest
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; [(7) or (8) would mean that there is something non-computational about  
</em><br />
<em class="quotelev1">&gt; the brain; (9) would mean there is something non-computational about a  
</em><br />
<em class="quotelev1">&gt; computer interacting with its environment, which seems to me even less  
</em><br />
<em class="quotelev1">&gt; plausible.]
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; But if (3) to (5) are all correct, that leaves (6) as correct, which  
</em><br />
<em class="quotelev1">&gt; implies that consciousness is decoupled from physical activity. This  
</em><br />
<em class="quotelev1">&gt; would mean that in those cases where we do associate consciousness  
</em><br />
<em class="quotelev1">&gt; with particular physical activity, such as in brains or computers, it  
</em><br />
<em class="quotelev1">&gt; is not really the physical activity which is &quot;causing&quot; the  
</em><br />
<em class="quotelev1">&gt; consciousness. I think of it as analogous to a computer doing  
</em><br />
<em class="quotelev1">&gt; arithmetic: it is not &quot;causing&quot; the arithmetic, but is harnessing a  
</em><br />
<em class="quotelev1">&gt; mathematical truth to some physical task.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; If consciousness is decoupled from physical activity, this means that  
</em><br />
<em class="quotelev1">&gt; our conscious experience would be unchanged if the apparent physical  
</em><br />
<em class="quotelev1">&gt; activity of our brain were not really there. This would make all the  
</em><br />
<em class="quotelev1">&gt; evidence of our senses on which we base the existence of a physical  
</em><br />
<em class="quotelev1">&gt; world illusory: we would have these same experiences if the physical  
</em><br />
<em class="quotelev1">&gt; world suddenly disappeared or never existed in the first place. We can  
</em><br />
<em class="quotelev1">&gt; keep (1) and (2) because I qualified them with the word &quot;appears&quot;, but  
</em><br />
<em class="quotelev1">&gt; the necessity of a separate physical reality accounting for this  
</em><br />
<em class="quotelev1">&gt; appearance goes.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev3">&gt;&gt; &gt; You can't avoid the above problem without making changes to  
</em><br />
<em class="quotelev2">&gt;&gt; (standard) &gt; computationalism. You can drop computationalism  
</em><br />
<em class="quotelev2">&gt;&gt; altogether and say &gt; that the brain + environment is not Turing  
</em><br />
<em class="quotelev2">&gt;&gt; emulable. Or, as Bruno has &gt; suggested, you can keep computationalism  
</em><br />
<em class="quotelev2">&gt;&gt; and drop the physical &gt; supervenience criterion.
</em><br />
<em class="quotelev2">&gt;&gt; I am OK with this ('course).
</em><br />
<em class="quotelev2">&gt;&gt; Bruno
</em><br />
<em class="quotelev2">&gt;&gt; <a href="http://iridia.ulb.ac.be/~marchal/">http://iridia.ulb.ac.be/~marchal/</a>
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Stathis Papaioannou
</em><br />
<em class="quotelev1">&gt; _________________________________________________________________
</em><br />
<em class="quotelev1">&gt; Be one of the first to try Windows Live Mail.
</em><br />
<em class="quotelev1">&gt; <a href="http://ideas.live.com/programpage.aspx?versionId=5d21c51a-b161-4314">http://ideas.live.com/programpage.aspx?versionId=5d21c51a-b161-4314</a> 
</em><br />
<em class="quotelev1">&gt; -9b0e-4911fb2b2e6d
</em><br />
<em class="quotelev2">&gt; &gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<a href="http://iridia.ulb.ac.be/~marchal/">http://iridia.ulb.ac.be/~marchal/</a>


<br />
--~--~---------~--~----~------------~-------~--~----~
<br />
&nbsp;You received this message because you are subscribed to the Google Groups &quot;Everything List&quot; group.
<br />
To post to this group, send email to everything-list.domain.name.hidden
<br />
To unsubscribe from this group, send email to everything-list-unsubscribe.domain.name.hidden
<br />
For more options, visit this group at <a href="http://groups.google.com/group/everything-list?hl=en">http://groups.google.com/group/everything-list?hl=en</a>
<br />
-~----------~----~----~----~------~----~------~--~---

<br />
<span id="received"><dfn>Received on</dfn> Wed Jan 10 2007 - 08:20:57 PST</span>
</div>
<!-- body="end" -->
<div class="foot">
<map id="navbarfoot" name="navbarfoot" title="Related messages">
<ul class="links">
<li><dfn>This message</dfn>: [ <a href="#start12507">Message body</a> ]</li>
<!-- lnext="start" -->
<li><dfn>Next message</dfn>: <a href="12508.html" title="Next message in the list">John M: "Re: Evil ? (was: Hypostases"</a></li>
<li><dfn>Previous message</dfn>: <a href="12506.html" title="Previous message in the list">Bruno Marchal: "Re: The Meaning of [your] Life"</a></li>
<li><dfn>In reply to</dfn>: <a href="12500.html" title="Message to which this message replies">Stathis Papaioannou: "RE: The Meaning of Life"</a></li>
<!-- lnextthread="start" -->
<li><dfn>Next in thread</dfn>: <a href="12513.html" title="Next message in this discussion thread">Stathis Papaioannou: "RE: The Meaning of Life"</a></li>
<!-- lreply="end" -->
</ul>
<ul class="links">
<li><a name="options3" id="options3"></a><dfn>Contemporary messages sorted</dfn>: [ <a href="date.html#msg12507" title="Contemporary messages by date">by date</a> ] [ <a href="index.html#msg12507" title="Contemporary discussion threads">by thread</a> ] [ <a href="subject.html#msg12507" title="Contemporary messages by subject">by subject</a> ] [ <a href="author.html#msg12507" title="Contemporary messages by author">by author</a> ] [ <a href="attachment.html" title="Contemporary messages by attachment">by messages with attachments</a> ]</li>
</ul>
</map>
</div>
<!-- trailer="footer" -->
<p><small><em>
This archive was generated by <a href="http://www.hypermail-project.org/">hypermail 2.3.0</a>
: Fri Feb 16 2018 - 13:20:13 PST
</em></small></p>
</body>
</html>
