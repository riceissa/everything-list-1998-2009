<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1" />
<meta name="generator" content="hypermail 2.3.0, see http://www.hypermail-project.org/" />
<title>Re: Dreaming On from Flammarion on 2009-09-03 (everything)</title>
<meta name="Author" content="Flammarion (peterdjones.domain.name.hidden)" />
<meta name="Subject" content="Re: Dreaming On" />
<meta name="Date" content="2009-09-03" />
<style type="text/css">
/*<![CDATA[*/
/* To be incorporated in the main stylesheet, don't code it in hypermail! */
body {color: black; background: #ffffff}
dfn {font-weight: bold;}
pre { background-color:inherit;}
.head { border-bottom:1px solid black;}
.foot { border-top:1px solid black;}
th {font-style:italic;}
table { margin-left:2em;}map ul {list-style:none;}
#mid { font-size:0.9em;}
#received { float:right;}
address { font-style:inherit ;}
/*]]>*/
.quotelev1 {color : #990099}
.quotelev2 {color : #ff7700}
.quotelev3 {color : #007799}
.quotelev4 {color : #95c500}
.period {font-weight: bold}
</style>
</head>
<body>
<div class="head">
<h1>Re: Dreaming On</h1>
<!-- received="Thu Sep 03 01:32:16 2009" -->
<!-- isoreceived="20090903083216" -->
<!-- sent="Thu, 3 Sep 2009 01:32:16 -0700 (PDT)" -->
<!-- isosent="20090903083216" -->
<!-- name="Flammarion" -->
<!-- email="peterdjones.domain.name.hidden" -->
<!-- subject="Re: Dreaming On" -->
<!-- id="36abde5a-654d-4e63-8b16-f3fa86d63a39.domain.name.hidden" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="b0b263660909021726j4085825dm977f1a77bbb28c29.domain.name.hidden" -->
<!-- expires="-1" -->
<map id="navbar" name="navbar">
<ul class="links">
<li>
<dfn>This message</dfn>:
[ <a href="#start17734" name="options1" id="options1" tabindex="1">Message body</a> ]
 [ More options (<a href="#options2">top</a>, <a href="#options3">bottom</a>) ]
</li>
<li>
<dfn>Related messages</dfn>:
<!-- unext="start" -->
[ <a href="17735.html" accesskey="d" title="Quentin Anciaux: &quot;Re: Dreaming On&quot;">Next message</a> ]
[ <a href="17733.html" title="David Nyman: &quot;Re: Dreaming On&quot;">Previous message</a> ]
[ <a href="17733.html" title="David Nyman: &quot;Re: Dreaming On&quot;">In reply to</a> ]
<!-- unextthread="start" -->
[ <a href="17735.html" accesskey="t" title="Quentin Anciaux: &quot;Re: Dreaming On&quot;">Next in thread</a> ]
 [ <a href="#replies">Replies</a> ]
<!-- ureply="end" -->
</li>
</ul>
</map>
<ul class="links">
<li><a name="options2" id="options2"></a><dfn>Contemporary messages sorted</dfn>: [ <a href="date.html#msg17734" title="Contemporary messages by date">by date</a> ] [ <a href="index.html#msg17734" title="Contemporary discussion threads">by thread</a> ] [ <a href="subject.html#msg17734" title="Contemporary messages by subject">by subject</a> ] [ <a href="author.html#msg17734" title="Contemporary messages by author">by author</a> ] [ <a href="attachment.html" title="Contemporary messages by attachment">by messages with attachments</a> ]</li>
</ul>
</div>
<!-- body="start" -->
<div class="mail">
<address class="headers">
<span id="from">
<dfn>From</dfn>: Flammarion &lt;<a href="mailto:peterdjones.domain.name.hidden?Subject=Re%3A%20Dreaming%20On">peterdjones.domain.name.hidden</a>&gt;
</span><br />
<span id="date"><dfn>Date</dfn>: Thu, 3 Sep 2009 01:32:16 -0700 (PDT)</span><br />
</address>
<br />
On 3 Sep, 01:26, David Nyman &lt;david.ny....domain.name.hidden&gt; wrote:
<br />
<em class="quotelev1">&gt; 2009/9/2 Flammarion &lt;peterdjo....domain.name.hidden&gt;:
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev3">&gt; &gt;&gt; and is thus not any particular physical
</em><br />
<em class="quotelev3">&gt; &gt;&gt; object.  A specific physical implementation is a token of that
</em><br />
<em class="quotelev3">&gt; &gt;&gt; computational type, and is indeed a physical object, albeit one whose
</em><br />
<em class="quotelev3">&gt; &gt;&gt; physical details can be of any variety so long as they continue to
</em><br />
<em class="quotelev3">&gt; &gt;&gt; instantiate the relevant computational invariance.  Hence it is hard
</em><br />
<em class="quotelev3">&gt; &gt;&gt; to see how a specific (invariant) example of an experiential state
</em><br />
<em class="quotelev3">&gt; &gt;&gt; could be justified as being token-identical with all the different
</em><br />
<em class="quotelev3">&gt; &gt;&gt; physical implementations of a computation.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev2">&gt; &gt; I was right.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev2">&gt; &gt; A mental type can be associated with a computational
</em><br />
<em class="quotelev2">&gt; &gt; type.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev2">&gt; &gt; Any token of a mental type can be associated with a token
</em><br />
<em class="quotelev2">&gt; &gt; of the corresponding computational type.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; But what difference is that supposed to make?  The type association is
</em><br />
<em class="quotelev1">&gt; implicit in what I was saying.  All you've said above is that it makes
</em><br />
<em class="quotelev1">&gt; no difference whether one talks in terms of the mental type or the
</em><br />
<em class="quotelev1">&gt; associated computational type because their equivalence is a posit of
</em><br />
<em class="quotelev1">&gt; CTM.  And whether it is plausible that the physical tokens so picked
</em><br />
<em class="quotelev1">&gt; out possess the causal efficacy presupposed by CTM is precisely what I
</em><br />
<em class="quotelev1">&gt; was questioning.
</em><br />
<br /><br />question it then. what's the problem?
<br />
<br /><em class="quotelev3">&gt; &gt;&gt; But even on this basis it still doesn't seem possible to establish any
</em><br />
<em class="quotelev3">&gt; &gt;&gt; consistent identity between the physical variety of the tokens thus
</em><br />
<em class="quotelev3">&gt; &gt;&gt; distinguished and a putatively unique experiential state.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev2">&gt; &gt; The variety of the physical implementations is reduced by grouping
</em><br />
<em class="quotelev2">&gt; &gt; them
</em><br />
<em class="quotelev2">&gt; &gt; as  equivalent computational types. Computation is abstract.
</em><br />
<em class="quotelev2">&gt; &gt; Abstraction is
</em><br />
<em class="quotelev2">&gt; &gt; ignoring irrelevant details. Ignoring irrelevant details establishes a
</em><br />
<em class="quotelev2">&gt; &gt; many-to-one relationship : many possible implementations of one mental
</em><br />
<em class="quotelev2">&gt; &gt; state.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Again, that's not an argument - you're just reciting the *assumptions*
</em><br />
<em class="quotelev1">&gt; of CTM, not arguing for their plausibility.
</em><br />
<br />you're not arguing against its plausibility
<br />
<br /><em class="quotelev1">&gt; The justification of the
</em><br />
<em class="quotelev1">&gt; supposed irrelevance of particular physical details is that they are
</em><br />
<em class="quotelev1">&gt; required to be ignored for the supposed efficacy of the type-token
</em><br />
<em class="quotelev1">&gt; relation to be plausible.  That doesn't make it so.
</em><br />
<br />why not? we already know they can be ignored to establish
<br />
computational
<br />
equivalence.
<br />
<br /><em class="quotelev3">&gt; &gt;&gt;  On the
</em><br />
<em class="quotelev3">&gt; &gt;&gt; contrary, any unbiased a priori prediction would be of experiential
</em><br />
<em class="quotelev3">&gt; &gt;&gt; variance on the basis of physical variance.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev2">&gt; &gt; Yes. The substance of the CTM claim is that physical
</em><br />
<em class="quotelev2">&gt; &gt; differences do not make  a mental difference unless they
</em><br />
<em class="quotelev2">&gt; &gt; make a computational difference. That is to say, switching from
</em><br />
<em class="quotelev2">&gt; &gt; one token of a type of computation to another cannot make
</em><br />
<em class="quotelev2">&gt; &gt; a difference in mentation. That is not to be expected on an
</em><br />
<em class="quotelev2">&gt; &gt; &quot;unbiased&quot; basis, just because it is a substantive claim.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Yes it's precisely the claim whose plausibility I've been questioning.
</em><br />
<br />You haven't said anything specific about what is wrong with it at all.
<br />
<br /><em class="quotelev2">&gt; &gt; The variety of the physical implementations is reduced by grouping
</em><br />
<em class="quotelev2">&gt; &gt; them
</em><br />
<em class="quotelev2">&gt; &gt; as  equivalent computational types. Computation is abstract.
</em><br />
<em class="quotelev2">&gt; &gt; Abstraction is
</em><br />
<em class="quotelev2">&gt; &gt; ignoring irrelevant details. Ignoring irrelevant details establishes a
</em><br />
<em class="quotelev2">&gt; &gt; many-to-one relationship : many possible implementations of one mental
</em><br />
<em class="quotelev2">&gt; &gt; state.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Yes thanks, this is indeed the hypothesis.  But simply recapitulating
</em><br />
<em class="quotelev1">&gt; the assumptions isn't exactly an uncommitted assessment of their
</em><br />
<em class="quotelev1">&gt; plausibility is it?  
</em><br />
<br /><br />Saying it is not necessarily correct is not a critique
<br />
<br /><em class="quotelev1">&gt;That can only immunise it from criticism.  There
</em><br />
<em class="quotelev1">&gt; is no whiff in CTM of why it should be considered plausible on
</em><br />
<em class="quotelev1">&gt; physical grounds alone.
</em><br />
<br /><br /><em class="quotelev1">&gt; Hence counter arguments can legitimately
</em><br />
<em class="quotelev1">&gt; question the consistency of its claims as a physical theory in the
</em><br />
<em class="quotelev1">&gt; absence of its type-token presuppositions.
</em><br />
<br />&nbsp;If you mean you can criticise the CTM as offering nothing specific
<br />
to resolve the HP, you are correct. But I *thought* we were
<br />
discussing the MG/Olympia style of argument, which purportedly
<br />
still applies even if you restrict yourself to cognition and forget
<br />
about experience/qualia.
<br />
Are we?
<br />
<br /><em class="quotelev1">&gt; Look, let me turn this round.  You've said before that you're not a
</em><br />
<em class="quotelev1">&gt; diehard partisan of CTM.  What in your view would be persuasive
</em><br />
<em class="quotelev1">&gt; grounds for doubting it?
</em><br />
<br />I'll explain below. But the claim I am interested in is that CTM
<br />
somehow disproves materalism (Maudlin, BTW takes it the other way
<br />
around--
<br />
materialism disproves CTM). I have heard not a word in support of
<br />
*that* claim.
<br />
<br /><br />ust an Artificial Intellence be a Computer ?
<br />
<br />An AI is not necessarily a computer. Not everything is a computer or
<br />
computer-emulable. It just needs to be artificial and intelligent! The
<br />
extra ingredient a conscious system has need not be anything other
<br />
than the physics (chemistry, biology) of its hardware -- there is no
<br />
forced choice between ghosts and machines.
<br />
<br />A physical system can never be exactly emulated with different
<br />
hardware -- the difference has to show up somewhere. It can be hidden
<br />
by only dealing with a subset of a systems abilities relevant to the
<br />
job in hand; a brass key can open a door as well as an iron key, but
<br />
brass cannot be substituted for iron where magnetism is relevant.
<br />
Physical differences can also be evaded by taking an abstract view of
<br />
their functioning; two digital circuits might be considered equivalent
<br />
at the &quot;ones and zeros&quot; level of description even though they
<br />
physically work at different voltages.
<br />
<br />Thus computer-emulability is not a property of physical systems as
<br />
such. Even if all physical laws are computable, that does not mean
<br />
that any physical systems can be fully simulated. The reason is that
<br />
the level of simulation matters. A simulated plane does not actually
<br />
fly; a simulated game of chess really is chess. There seems to be a
<br />
distinction between things like chess, which can survive being
<br />
simulated at a higher level of abstraction, and planes, which can't.
<br />
Moreover, it seems that chess-like things are in minority, and that
<br />
they can be turned into an abstract programme and adequately simulated
<br />
because they are already abstract.
<br />
<br />Consciousness. might depend on specific properties of hardware, of
<br />
matter. This does not imply parochialism, the attitude that denies
<br />
consciousness to poor Mr Data just because he is made out of silicon,
<br />
not protoplasm. We know our own brains are conscious; most of us
<br />
intuit that rocks and dumb Chinese Rooms are not; all other cases are
<br />
debatable.
<br />
<br />Of course all current research in AI is based on computation in one
<br />
way or another. If the Searlian idea that consciousness is rooted in
<br />
physics, strongly emergent, and non-computable is correct, then
<br />
current AI can only achieve consciousness accidentally. A Searlian
<br />
research project would understand how brains generate consciousness in
<br />
the first place -- the aptly-named Hard Problem -- before moving onto
<br />
possible artificial reproductions, which would have to have the right
<br />
kind of physics and internal causal activity -- although not
<br />
necessarily the same kind as humans.
<br />
<br />&nbsp;&nbsp;&nbsp;&nbsp;&quot;When I say that the brain is a biological organ and consciousness
<br />
a biological process, I do not, of course, say or imply that it would
<br />
be impossible to produce an artificial brain out of nonbiological
<br />
materials that could also cause and sustain consciousness...There is
<br />
no reason, in principle, why we could not similarly make an artificial
<br />
brain that causes consciousness. The point that needs to be empnasized
<br />
is that any such artificial brain would have to duplicate the actual
<br />
causes of human and animal brains to produce inner, qualitative,
<br />
subjective states of consciousness. Just producing similar output
<br />
behavior would not by itself be enough.&quot;
<br />
<br />[Searle, MLS, p. 53]
<br />
<br />&quot;Is the Brain A Machine?&quot;
<br />
John Searle thinks so .
<br />
<br />&nbsp;&nbsp;&nbsp;&nbsp;The brain is indeed a machine, an organic machine; and its
<br />
processes, such as neuron firings, are organic machine processes.
<br />
<br />The Mystery of Consciousness. page 17: Is he right ? To give a
<br />
typically philosophical answer, that depends on what you mean by
<br />
'machine'. If 'machine' means an artificial construct, then the answer
<br />
is obviously 'no'. However. Searle also thinks the the body is a
<br />
machine, by which he seems to mean that it has been understand in
<br />
scientific terms, we can explain biology by in terms of to chemistry
<br />
and chemistry in terms of physics. Is the brain a machine by this
<br />
definition ? It is being granted that the job of he brain is to
<br />
implement a conscious mind, just as the job of the stomach is to
<br />
digest, the problem then is that although our 'mechanical'
<br />
understanding of the stomach does allow us to understand digestion we
<br />
do not, according to Searle himself, understand how the brain produces
<br />
consciousness. He does think that the problem of consciousness is
<br />
scientifically explicable, so yet another definition of 'machine' is
<br />
needed, namely 'scientifically explained or scientifically explicable'
<br />
-- with the brain being explicable rather than explained. The problem
<br />
with this stretch-to-fit approach to the meaning of the word 'machine'
<br />
is that every time the definition of brain is broadened, the claim is
<br />
weakened, made less impactful.
<br />
<br />PDJ 03/02/03
<br />
The Chinese Room
<br />
The Chinese Room and Consciousness
<br />
According to the proponents of Artificial Intelligence, a system is
<br />
intelligent if it can convince a human interlocutor that it is. This
<br />
is the famous Turing Test. It focuses on external behaviour and is
<br />
mute about how that behaviour is produced. A rival idea is that of the
<br />
Chinese Room, due to John Searle. Searle places himself in the room,
<br />
manually executing a computer algorithm that implements intelligent-
<br />
seeming behaviour, in this case getting questions written in Chinese
<br />
and mechanically producing answers, without himself understanding
<br />
Chinese. He thereby focuses attention on how the supposedly
<br />
intelligent behaviour is produced. Although Searle's original idea was
<br />
aimed at semantics, my variation is going to focus on consciousness.
<br />
Likewise, although Searle's original specification has him
<br />
implementing complex rules, I am going to take it that the Chinese
<br />
Room is implemented as a conceptually simple system --for instance, a
<br />
Giant Look-Up Table -- in line with the theorem of Computer Science
<br />
which has it that any computer can be emulated by a Turing Machine.
<br />
<br />If you think a Chinese Room implemented with a simplistic, &quot;dumb&quot;
<br />
algorithm can still be conscious, you are probably a behaviourist; you
<br />
only care about that external stimuli get translated into the
<br />
appropriate responses, not how this happens, let alone what it feels
<br />
like to the system in question.
<br />
<br />If you think this dumb Chinese Room is not conscious, but a smart one
<br />
would be, you need to explain why. There are two explanatory routes:
<br />
one that says consciousness is inessential, and another that says that
<br />
hardware counts as well as software.
<br />
<br />Any smart AI can be implemented as a dumb TM, so the more complex
<br />
inner workings which supposedly implement consciousness , could be
<br />
added or subtracted without making any detectable difference. Given
<br />
the assumption that the computational differences are what matter,
<br />
this would to add up to epiphenomenalism, the view that consciousness
<br />
exists but is a bystander that doesn't cause anything, since there is
<br />
not any computational difference between the simple implementation and
<br />
the complex one.
<br />
<br />On the other hand, if it is assumed that epiphenomenalism if false,
<br />
then it follows that implementational differences must matter, since
<br />
the difference between the complex and the dumb systems is not in
<br />
their computational properties. That in turn means computationalism is
<br />
false. The Chinese Room argument then succeeds, but only as
<br />
interpreted fairly strictly as an argument about the ability of
<br />
algorithms to implement consciousness. Any actual computational
<br />
systems, or artificial intelligence construct, will be more than just
<br />
an algorithm; it will be the concrete implementation of an algorithm.
<br />
Since it is the implementation that makes the difference between a
<br />
fully successful AI and a &quot;zombie&quot; (functional enough to pass a Turing
<br />
test, but lacking real consciousness), and since every AI would have
<br />
some sort of implementation, the possibility of an actual systems
<br />
being conscious is far from ruled out. The CR argument only shows that
<br />
it is not conscious purely by virtue of implementing an algorithm. It
<br />
is a succesful argument up to that point, the point that why AI may be
<br />
possible, it will not be pruely due to running the right algorithm.
<br />
While the success of an AI programme is not ruled out, it is not
<br />
guaranteed either. It is not clear which implementations are the right
<br />
ones. A system running the right algorithm on the wrong hardware may
<br />
well be able to pass a Turing Test, but if the hardware is relevant to
<br />
consciousness as well, a system with the wrong hardware will be an
<br />
artificial zombie. It will be cognitively competent, but lacking in
<br />
genuine phenomenal consciousness. (This is in line with the way robots
<br />
and the like are often portrayed in science fiction. A further wrinkle
<br />
is that an exact computational emulation of a real person -- a real
<br />
person who believes in qualia anyway -- would assert its possession of
<br />
qualia while quite possibly not possessing any qualia to boast about).
<br />
<br />Thus the success of the CR argument against a software-only approach
<br />
to AI has the implication that the TT is not adequate to detect the
<br />
success of a strong AI (artificial consciousness) project. (Of course,
<br />
all this rests on beahviourism being false; if behaviourism is true
<br />
there is no problem with a TT, since it is a test of behaviour). We
<br />
need to peek inside the box; in order to know whether an AI device has
<br />
full phenomenal, consciousness, we would need a successful theory
<br />
linking consciousness to physics. Such a theory would be nothing less
<br />
than an answer to the Hard Problem. So a further implication of the
<br />
partial success of Searlian arguments is that we cannot bypass the
<br />
problem of explaining consciousness by some research programme of
<br />
building AIs. The HP is logically prior. Except for beahviourists.
<br />
<br />Peter D Jones 8/6/05
<br />
Syntax and Semantics. The Circularity Argument as an Alternative
<br />
Chinese Room
<br />
<br />The CR concludes that syntax, an abstract set of rules is insufficient
<br />
for semantics. This conclusions is also needed as a premise for
<br />
Searle's syllogistic argument
<br />
<br />&nbsp;&nbsp;&nbsp;1. Syntax is not sufficient for semantics.
<br />
&nbsp;&nbsp;&nbsp;2. Computer programs are entirely defined by their formal, or
<br />
syntactical, structure.
<br />
&nbsp;&nbsp;&nbsp;3. Minds have mental contents; specifically, they have semantic
<br />
contents.
<br />
&nbsp;&nbsp;&nbsp;4. Therefore, No computer program by itself is sufficient to give a
<br />
system a mind. Programs, in short, are not minds, and they are not by
<br />
themselves sufficient for having minds.
<br />
<br />Premise 01 is the most contentious of the four. The Chinese Room
<br />
Argument, which Searle puts forward to support itm is highly
<br />
contentious. We will put forward a different argument to support it.
<br />
<br />An objection to the CR argument goes: &quot;But there must be some kind of
<br />
information processing structure that implements meaning in our heads.
<br />
Surely that could be turned into rules for the operator of the Chinese
<br />
Room&quot;.
<br />
<br />A response, the Circularity Argument goes: a system of syntactic
<br />
process can only transform one symbol-string into another; it does not
<br />
have the power to relate the symbols to anything outside the system.
<br />
It is a circualr, closed system. However, to be meaningful a symbol
<br />
must stand for something other than itself. (The Symbol must be
<br />
Grounded). Therefore it must fail to have any real semantics.
<br />
<br />It is plausible that any given term can be given an abstract
<br />
definition that doesn't depend on direct experience. A dictionary is
<br />
collection of such definitions. It is much less plausible that every
<br />
term can be defined that way. Such a system would be circular in the
<br />
same way as:
<br />
<br />&quot;present: gift&quot;
<br />
<br />&quot;gift: present&quot;
<br />
<br />...but on a larger scale.
<br />
<br />A dictionary relates words to each other on a static way. It does not
<br />
directly have the power to relate words to anything outside itseld. We
<br />
can understand dictionary definitons because we have already grapsed
<br />
the meanings of some words. A better analogy for the Symbol Grounding
<br />
problem is that of trying to learn an entirely unknown langauge for a
<br />
dictionary. (I have switched from talking about syntacital
<br />
manipluation processes to static dicitonaries; Searles arguments that
<br />
syntax cannot lead to semantics have been critices for dealing with
<br />
&quot;syntax&quot; considered as abstract rules, whereas the computational
<br />
processes they are aimend are concrete, physcial and dynamic. The
<br />
Circularity argument does not have that problem. Both abstract syntax
<br />
and symbol-manipulation processed can be considered as circular).
<br />
<br />If the Circularity Argument, is correct, the practice of giving
<br />
abstract definitions, like &quot;equine quadruped&quot; only works because
<br />
somewhere in the chain of definitions are words that have been defined
<br />
directly; direct reference has been merely deferred, not avoided
<br />
altogether.
<br />
<br />The objection continues: &quot;But the information processing structure in
<br />
our heads has a concrete connection to the real world: so do AI's
<br />
(although those of the Chinese Room are minimal). Call this is the
<br />
Portability Assumption.
<br />
<br />But they are not the same concrete connections. The portability of
<br />
abstract rules is guaranteed by the fact that they are abstract. But
<br />
concrete causal connections are not-abstract. They are unlikely to be
<br />
portable -- how can you explain colour to an alien whose senses do not
<br />
include anything like vision?
<br />
<br />Copying the syntactic rules from one hardware platform to another will
<br />
not copy the semantics. Therefore,semantics is more than syntax.
<br />
<br />If the Portability Assumption is correct, an AI (particularly a
<br />
robotic one) could be expected to have some semantics, but there is no
<br />
reason it should have human semantics. As Wittgenstein said: &quot;if a
<br />
lion could talk, we could not understand it&quot;.
<br />
<br />Peter D Jones 13/11/05
<br />
The Chinese Room and Computability
<br />
I casually remarked that mental behaviour 'may not be computable'.
<br />
This will shock some AI proponents, for whom the Church-Turing thesis
<br />
proves that everything is computable. More precisely, everything that
<br />
is mathematically computable is computable by a relatively dumb
<br />
computer, a Turing that something can be simulated doesn't mean the
<br />
simulation has all the relevant properties of the original: flight
<br />
simulators don't take off. Thirdly the mathematical sense of
<br />
'computable' doesn't fit well with the idea of computer-simulating
<br />
fundamental physics. A real number is said to be mathematically
<br />
computable if the algorithm that churns it out keeps on churning out
<br />
extra digits of accuracy..indefinitely. Since such a algorithm will
<br />
never finish churning out a single real number physical value, it is
<br />
difficult to see how it could simulate an entire universe. Yes, I am
<br />
assuming the universe is fundamentally made of real numbers. If it is,
<br />
for instance finite, fundamental physics might be more readily
<br />
computable, but the computability of physics still depends very much
<br />
on physics and not just on computer science).
<br />
The Systems Response and Emergence
<br />
By far the most common response to the CR argument is that, while the
<br />
room's operator, Searle himself, does not understand Chinese, the room
<br />
as a whole does. According to one form of the objection, individual
<br />
neurons do not understand Chinese either; but this is not a fair
<br />
comparison. If you were to take a very simple brain and gradually add
<br />
more neurons to it, the increase in information-processing capacity
<br />
would keep in line with an increase in causal activity. However, the
<br />
equivalent procedure of gradually beefing up a CR would bascially
<br />
consist of adding more and more rules to the rule book while the
<br />
single &quot;neuron&quot;, the single causally active constituent, the operator
<br />
of the room did all the work. It is hard to attribute understanding to
<br />
a passive rulebook, and hard to attribute it to an operator performing
<br />
simple rote actions. It is also hard to see how the whole can be more
<br />
than the sum of the parts. It is very much a characteristic of a
<br />
computer, or other mechanism, that there is no mysterious emegence
<br />
going on; the behavour of the whole is always explicable in term sof
<br />
the behaviour of the parts. There is no mystery, by contrast, in more
<br />
neurons being able to do more work. Searle doesn't think you can put
<br />
two dumbs together and get a smart. That is no barrier to putting 100
<br />
billion dumbs together to get a smart. Or to putting two almost-smarts
<br />
together to get a smart.
<br />
The Chinese Room and Speed
<br />
Of course, if we burden the room's operator with more and more rules,
<br />
he will go slower and slower. Dennett thinks a slow chinese room would
<br />
not count as conscious at all. Nature, he notes, requires conscious
<br />
beings to react within a certain timescale in order to survive. That
<br />
is true, but it does not suggest any absolute speed requirement.
<br />
Nature accomodates the tortoise and the mayfly alike. The idea that a
<br />
uselessly slow consciousness would not be actually be a concsiousness
<br />
at all is also rather idiosyncratic. We generally credit a useless
<br />
vestigal limb with being a loimb, at least.
<br />
<br />Anyway, Dennett's speed objection is designed to lead into one of his
<br />
favourite ideas: the need for massive parallelism. One Searle might
<br />
lack conscious semantics, but a million might do the trick. Or so he
<br />
says. But what would parallelism bring us except speed?
<br />
The Chinese Room and complexity.
<br />
The Dennettians make two claims; that zombies are impossible, and that
<br />
the problem with the Chinese room is that it is too simple. We will
<br />
show that both claims cannot be true.
<br />
<br />What kind of complexity does the Chinese Room lack? By hypothesis it
<br />
can pass a Turing test: it has that much complexity in the sense of
<br />
outward performance. There is another way of thinking about
<br />
complexity: complexity of implementation. . So would the Chinese Room
<br />
be more convincing if it had a more complex algorithm? The problem
<br />
here is that there is a well-founded principle of computer science
<br />
according to which a computer programme of any complexity can emulated
<br />
by a particular type of essentially simple machine called a Turing
<br />
Machine. As it happens, the Chinese Room scenario matches a Turing
<br />
Machine pretty well. A Turing Machine has a simple active element, the
<br />
read-write head and a complex instruction table. In the Chinese Room
<br />
the sole active element is the operator, performing instruction by
<br />
rote; any further complexity is in the rulebooks. Since there is no
<br />
stated limit to the &quot;hardware&quot; of the Chinese Room -- the size of the
<br />
rulebook, the speed of the operator -- the CR could be modified to
<br />
implement more complex algorithms without changing any of the
<br />
essential features.
<br />
<br />Of course differences in implementation could make all sorts of non-
<br />
computational differences. Dennett might think no amount of
<br />
computation will make a flight simulator fly. He might think that the
<br />
Chinese Room lack sensor and effectuators to interact with its
<br />
environment, and that such interactions are needed to solve the symbol-
<br />
grounding problem. He might think that implementational complexity,
<br />
hardware over software is what makes the difference between real
<br />
consciousness and zombiehood. And Searle might well agree with him on
<br />
all those points: he may not be a computationalist, but he is a
<br />
naturalist. The dichotomy is this: Denett's appeal to complexity is
<br />
either based on software, in which case it is implausible, being
<br />
undermined by Turing equivalence; or it is based in hardware, in which
<br />
case it is no disproof of Searle. Rather, Searle's argument can be
<br />
seen as a successful disproof of computationalism(ie the only-software-
<br />
matters approach) and Dennett's theory of consciousness is a proposal
<br />
for a non-computationalistic, hardware-based, robotic approach of the
<br />
kind Searle favours.
<br />
<br />Some Denettians think a particular kind of hardware issue matters:
<br />
parallelism. The Chinese room is &quot;too simple&quot; in that it is a serial
<br />
processor. Parallel processors cannot in fact computer anything --
<br />
cannot solve any problem -- that single processors can't. So parallel
<br />
processing is a difference in implementation, not computation. What
<br />
parallel-processing hardware can do that serial hardware cannot is
<br />
perform opertations simultaneously. Whatever &quot;extra factor&quot; is added
<br />
by genuine simultaneity is not computational. Presumably that means it
<br />
would not show up in a Turing test -- it would be indetectable from
<br />
the outside. So the extra factor added by simultaneity is something
<br />
that works just like phenomenality. It is indescernable from the
<br />
outside, and it is capable of going missing while external
<br />
functionality is preserved. (We could switch a parallel processor off
<br />
during a TT and replace it with a computationally equivalent serial
<br />
one. According to the parallel processing claim, any genuine
<br />
cosnciousness would vanish, although the external examiner preforming
<br />
the TT would be none the wiser). In short, simulateneity implies
<br />
zombies.
<br />
The Chinese Room and Abstraction
<br />
Consider the argument that computer programmes are too abstract to
<br />
cause consciousness. Consider the counter-argument that a running
<br />
computer programme is a physical process and therefore not abstract at
<br />
all.
<br />
<br />&nbsp;&nbsp;&nbsp;1. Computationalism in general associates that consciousness with a
<br />
specific comptuer programme, programme C let's say.
<br />
&nbsp;&nbsp;&nbsp;2. Let us combine that with the further claim that programme C
<br />
causes cosnciousness, somehow leveraging the physical causality of the
<br />
hardware it is running on.
<br />
&nbsp;&nbsp;&nbsp;3. A corrolary of that is that running programme C will always
<br />
cause the same effect.
<br />
&nbsp;&nbsp;&nbsp;4. Running a programme on hardware is a physical process with
<br />
physical effects.
<br />
&nbsp;&nbsp;&nbsp;5. It is in the nature of causality that the same kind of cause
<br />
produces the same kind of effects-- that is, causaliy attaches to
<br />
types not tokens.
<br />
&nbsp;&nbsp;&nbsp;6. Running a programme on hardware will cause physical effects, and
<br />
these will be determined by the kind of physical hardware. (Valve
<br />
computers will generate heat, cogwheel computers will generate noise,
<br />
etc).
<br />
&nbsp;&nbsp;&nbsp;7. Therefore, running programme C on different kinds of hardware
<br />
will not produce a uniform effect as required by 1.
<br />
&nbsp;&nbsp;&nbsp;8. Programmes do not have a physical typology: they are not natural
<br />
kinds. In that sense they are abstract. (Arguably, that is not as
<br />
abstract as the square root of two, since they still have physical
<br />
tokens. There may be more than one kind or level of abstraction).
<br />
&nbsp;&nbsp;&nbsp;9. Conclusion: even running programmes are not apt to cause
<br />
consciousness. They are still too abstract.
<br />
<br />Computational Zombies
<br />
This argument explores the consequenes of two assumptions:
<br />
<br />&nbsp;&nbsp;&nbsp;1. We agree that Searle is right in his claim that software alone
<br />
is not able to bring about genuine intelligence,
<br />
&nbsp;&nbsp;&nbsp;2. But continue to insist that AI research should nonetheless be
<br />
pursued with computers.
<br />
<br />In other words, we expect the success or failure of our AI to be
<br />
dependent on the choice of software in combination with the choice of
<br />
hardware.
<br />
<br />The external behaviour of a computational system -- software and
<br />
hardware taken together -- is basically detemined by the software it
<br />
is running; that is to say, while running a programme on different
<br />
hardware will make some kind of external differences, they tend to be
<br />
irrelevant and uninteresting differences such as the amount of heat
<br />
and noise generated. Behaviourstic tests like the Turing Test are
<br />
specifically designed to filter out such differences (so that the
<br />
examiner's prejudices about what kind of system could be conscious are
<br />
excluded). The questions and responses in a TT are just the inputs and
<br />
outputs of the software.
<br />
<br />Abandoning the software-only approach for a combined software-and-
<br />
hardware approach has a peculiar consequence: that it is entirely
<br />
possible that out of two identically programmed systems running on
<br />
different hardware, one will be genuinely intelligent (or have genuine
<br />
consciousness, or genuine semantic comprehension, etc) and the other
<br />
will not. Yet, as we have seen above, these differences will be --
<br />
must be -- indiscernable in a Turing Test. Thus, if hardware is
<br />
involved in the implementation of AI in computers, the Turing Test
<br />
must be unreliable. There is a high probability that it will give
<br />
&quot;false positives&quot;, telling us that unconscious AIs are actually
<br />
conscious -- a probability that rises with the number of different
<br />
systems tested.
<br />
<br />To expand on the last point: suppose you get a positive TT result for
<br />
one system, A. Then suppose you duplicate the software onto a whole
<br />
bunch of different hardware platforms, B, C, D....
<br />
<br />(Obviously, they are all assumed to be capable of running the software
<br />
in the first place). They must give the same results to the TT for A,
<br />
since they all run the same software, and since the software
<br />
determines the responses to a TT, as we established above, they must
<br />
give positive results. But eventually you will hit the wrong hardware
<br />
-- it would be too unlikely to always hit on the right hardware by
<br />
sheer chance, like throwing an endless series of heads. When you do
<br />
hit the wrong hardware, you get a false positive. (Actually you don't
<br />
know you got a true positive with A in the first place...)
<br />
<br />Thus, some AIs would be &quot;zombies&quot; in a restricted sense of &quot;zombie&quot;.
<br />
Whereas a zombie is normally thought of a physical duplicate lacking
<br />
consciousness, these are software duplicates lacking appropriate
<br />
hardware.
<br />
<br />This peculiar sitation comes about because of the separability of
<br />
software and hardware in a computational approach, and the further
<br />
separation of relevant and irrelvant behaviour in the Turing Test.
<br />
(The separability of software simply means the ability to run the same
<br />
software on differenet hardware). Physical systems in general -- non
<br />
computers, not susceptible to separate descriptions of hardware and
<br />
software -- do not have that separability. Their total behaviour is
<br />
determined by their total physical makeup. A kind of Articial
<br />
Intelligence that was basically non-computational would not be subject
<br />
to the Compuational Zombie problem. Searle is therefore correct to
<br />
maintain, as he does, that AI is broadly possible.
<br />
Neuron-silicon replacement scenarios
<br />
Chalmers claims that replacing neurons with silicon will preserve
<br />
qualia so long as it preserves function -- by which he means not just
<br />
outward, behavioural function but also the internal organisation that
<br />
produces it. Obviously, he has to make that stipulation because it is
<br />
possible to think of cases, such as Searle's Chinese Room, where
<br />
outward behaviour is generated by a very simplistic mechanism, such as
<br />
a lookup table. In fact, if one takes the idea that consciousness
<br />
supervenes on the functional to the extreme, it becomes practically
<br />
tautologous. The most fine-grained possible functional description
<br />
just is a physical description (assuming physics does not deliver
<br />
intrinsic properties, only structural/behavioural ones) , and the
<br />
mental supervenes in some sense on the physical, so consciousness can
<br />
hardly fail to supervene on an ultimately fine-grained functional
<br />
simulation. So the interesting question is what happens between these
<br />
two extremes at, say, the neuronal level.
<br />
<br />One could imagine a variation of the thought-experiment where one's
<br />
brain is first replaced at the fine-grained level, and the replaced
<br />
again with a coarser-grained version, and so, on, finishing in a Giant
<br />
Look Up Table. Since hardly anyone thinks a GLUT would have phenomenal
<br />
properties, phenomenality would presumably fade out. So there is no
<br />
rigid rule that phenomenality is preserved where functionality is
<br />
preserved.
<br />
<br />It is natural suppose that one's functional dispositions are in line
<br />
with one's qualia. One claims to see red because one is actually
<br />
seeing red. But an intuition that is founded on naturalness cannot be
<br />
readily carried across to the very unnaturual situation of having
<br />
one's brain gradually replaced.
<br />
<br />What is it like to have one's qualia fade away ? If one had ever been
<br />
a qualiaphile, one would continue to claim to have qualia, without
<br />
actually doing so. That is, one would be under an increasing series of
<br />
delusions. It is not difficult to imagine thought-experiments where
<br />
the victim's true beliefs are changed into false ones. For instance,
<br />
the Mad Scientist could transport the victim from their bedroom to a
<br />
&quot;Truman Show&quot; replica while they slept. Thus the victim's belief that
<br />
they were still in their own bedroom would be falsified. Since beliefs
<br />
refer to states-of-afairs outside the head, you don't even need to
<br />
change anything about someone's psychology to change the truth of
<br />
their beliefs. So there is no great problem with the idea that
<br />
rummaging in someone's head does change their beliefs -- any such
<br />
process must change beliefs relating to what is physically inside the
<br />
victims head. Since the victim is funtionally identical, they must
<br />
carry on believing they have neural tissue in their head, even after
<br />
it has all been replaced. It doesn't follow from this that replacing a
<br />
brain with silicon must destroy qualia, but there is definitely a
<br />
precedent for having false beliefs about one's own qualia after one's
<br />
brain has been tampered with.
<br />
<br />A GLUT of Turings
<br />
An old programmer's trick is to store &quot;potted&quot; results rather than
<br />
calculating them afresh each time. This saves tiem at the expense of
<br />
using up memory. Earlier, we used the idea of a &quot;Giant Look-Up Table&quot;
<br />
to implement, in an essentially dumb way, the whole of an extremely
<br />
coplicated system, such as a human brain.
<br />
<br />Can a (Giant) Look-Up Table emulate any Turing Machine (and therefore,
<br />
any computer, and therefore, if computationalism is true, any brain).
<br />
<br />The usual objection to LUT's is that they are stateless. But that is
<br />
easy get round. Add a timestamp as an additional input.
<br />
<br />Or includde with the fresh input each time a record of all previous
<br />
conversations it has had, with the total table size limiting the
<br />
&quot;lifespan&quot; of the machine
<br />
<br />The feedback of the old conversation gives the machine a state memory,
<br />
very straightforwardly is voluminously encoded
<br />
<br />What is the LUT for a sorting algorithm?
<br />
<br />It is a table which matches lists of unsorted numbers against sorted
<br />
numbers. it doesn't even need to be stateful. And, yes, if it is
<br />
finite it will only sort lists of up to some size limit. But then any
<br />
algorithm has to run for a finite length of time, and will not be able
<br />
to sort some lists in the time allowed. So time limits are just being
<br />
traded for space limits.
<br />
<br />If you want to pass a Turing test with a glut, you only need a coarse-
<br />
grained (but still huge) GLUT, that matches verbal resonses to verbal
<br />
inputs. (A GLUT that always produced the same response to the same
<br />
query would be quickly detected as a machine, so it would need the
<br />
statefullness trick, making it even larger...). However, it is
<br />
counterinutitive that such a GLUT would simulate thought since nothing
<br />
goes on between stimulus and response. Well, it is counterintuitive
<br />
that any GLUT would think or feel anything. Daryl McCullough and Dave
<br />
Chalmers chew the issue over in this extract from a Newsgroup
<br />
discussion.
<br />
Computationalism
<br />
Computationalism is the claim that the human mind is essentially a
<br />
computer. It can be picturesquely expressed in the &quot;yes, doctor&quot;
<br />
hypothesis -- the idea that, faced with a terminal disease, you would
<br />
consent to having your consciousness downloaded to a computer.
<br />
<br />There are two ambiguities in &quot;computationalism&quot; -- consciousness vs.
<br />
cognition, process vs programme -- leading to a total of four possible
<br />
meanings.
<br />
<br />Most people would not say &quot;yes doctor&quot; to a process that recorded
<br />
their brain on a tape a left it in a filing cabinet. Yet, that is all
<br />
you can get out of the timeless world of Plato's heaven (programme vs
<br />
process).
<br />
<br />That intuition is, I think, rather stronger than the intuition that
<br />
Maudlin's argument relies on: that consciousness supervenes only on
<br />
brain activity, not on counterfactuals.
<br />
<br />But the other ambiguity in computationalism offers another way out. If
<br />
only cognition supervenes on computational (and hence counterfactual)
<br />
activity, then consciousness could supervene on non-counterfactual
<br />
activity -- i.e they could both supervene on physical processes, but
<br />
in different ways.
<br />
Aritifical intelligence and emotion
<br />
AI enthusiasts are much taken with the analogy between the brain's
<br />
(electro) chemical activity and the electrical nature of most current
<br />
computers. But brains are not entirely electrical. Neurons sit in a
<br />
bath of chemicals which effects their behaviour, too. Adrenaline, sex
<br />
hormones, recreational drugs all affect the brain. Why are AI
<br />
proponents so unconcerned about brain chemistry? Is it because they
<br />
are so enamoured with the electrical analogy? Or because they just
<br />
aren't that interested in emotion?
<br />
Platonic computationalism -- are computers numbers?
<br />
Any computer programme (in a particular computer) is a long sequence
<br />
of 1's and 0's, and therefore, a long number. According to Platonism,
<br />
numbers exist immaterially in &quot;Plato's Heaven&quot;. If programmes are
<br />
numbers, does that mean Plato's heaven is populated with computer
<br />
programmes?
<br />
<br />The problem, as we shall see is the &quot;in a a particular computer&quot;
<br />
clause.
<br />
<br />As Bruno Marchal states the claim in a more formal language:
<br />
<br />&quot;Of course I can [identify programmes with numbers ]. This is a key
<br />
point, and it is not obvious. But I can, and the main reason is Church
<br />
Thesis (CT). Fix any universal machine, then, by CT, all partial
<br />
computable function can be arranged in a recursively enumerable list
<br />
F1, F2, F3, F4, F5, etc. &quot;
<br />
<br />Of course you can count or enumerate machines or algorithms, i.e.
<br />
attach unique numerical labels to them. The problem is in your &quot;Fix
<br />
any universal machine&quot;. Given a string of 1's and 0s wihouta universal
<br />
machine, and you have no idea of which algorithm (non-universal
<br />
machine) it is. Two things are only identical if they have all*their
<br />
properties in common (Leibniz's law). But none of the propeties of the
<br />
&quot;machine&quot; are detectable in the number itself.
<br />
<br />(You can also count the even numbers off against the odd numbers , but
<br />
that hardly means that even numbers are identical to odd numbers!)
<br />
<br />&quot;In computer science, a fixed universal machine plays the role of a
<br />
coordinate system in geometry. That's all. With Church Thesis, we
<br />
don't even have to name the particular universal machine, it could be
<br />
a universal cellular automaton (like the game of life), or Python,
<br />
Robinson Aritmetic, Matiyasevich Diophantine universal polynomial,
<br />
Java, ... rational complex unitary matrices, universal recursive group
<br />
or ring, billiard ball, whatever.&quot;
<br />
<br />Ye-e-es. But if all this is taking place in Platonia, the only thing
<br />
it can be is a number. But that number can't be associated with a
<br />
computaiton by another machine, or you get infinite regress.
<br />
Is the computationalist claim trivial -- are all systems computers?
<br />
It can be argued that any physical theory involving real numbers poses
<br />
problems (and all major theories do, at the time of writing). Known
<br />
physics is held to be computable, but that statement needs to be
<br />
qualified in various ways. A number thinking particularly of a real
<br />
number, one with an infinite number of digits -- is said to be
<br />
computable if a Turing machine will continue to spit out digits
<br />
endlessly. In other words, there is no question of getting to the
<br />
&quot;last digit&quot;. But this sits uncomfortably with the idea of simulating
<br />
physics in real time (or any plausible kind of time). Known physical
<br />
laws (including those of quantum mechanics) are very much infused with
<br />
real numbers and continua.
<br />
<br />&nbsp;&nbsp;&nbsp;&nbsp;&quot;So ordinary computational descriptions do not have a cardinality
<br />
of states and state space trajectories hat is sufficient for them to
<br />
map onto ordinary mathematical descriptions of natural systems. Thus,
<br />
from the point of view of strict mathematical description, the thesis
<br />
that everything is a computing system in this second sense cannot be
<br />
supported&quot;
<br />
<br />Moreover, the universe seems to be able decide on their values on a
<br />
moment-by-moment basis. As Richard Feynman put it: &quot;It always bothers
<br />
me that, according to the laws as we understand them today, it takes a
<br />
computing machine an infinite number of logical operations to figure
<br />
out what goes on in no matter how tiny a region of space, and no
<br />
matter how tiny a region of time. How can all that be going on in that
<br />
tiny space? Why should it take an infinite amount of logic to figure
<br />
out what one tiny piece of space/time is going to do?
<br />
<br />However, he went on to say:
<br />
<br />So I have often made the hypotheses that ultimately physics will not
<br />
require a mathematical statement, that in the end the machinery will
<br />
be revealed, and the laws will turn out to be simple, like the chequer
<br />
board with all its apparent complexities. But this speculation is of
<br />
the same nature as those other people make I like it, I dont like it,
<br />
and it is not good to be prejudiced about these things&quot;.
<br />
Is no physical system a computer, except in the eye of the beholder
<br />
Comsider the claim that &quot;computation&quot; may not correctly be ascribed to
<br />
the physics per se. Maybe it can be ascribed as an heuristic device as
<br />
physical explanation has an algorithmic component as Wolfram suggests.
<br />
<br />Whether everything physical is computational or whether specific
<br />
physical systems are computational are two quite different questions.
<br />
As far as I can see, a NAND's gate being a NAND gate is just as
<br />
objective as a square thing's being square.
<br />
<br />Are the computations themselves part of the purely physical story of
<br />
what is going on inside a compter?
<br />
<br />Seen mathematically, they have to be part of the physical story. They
<br />
are not some non-physical aura hanging over it. A computer doing
<br />
something semantic like word-processing needs external interpretation
<br />
in the way anything semantic does: there is nothing intrinsic and
<br />
objective about a mark that makes it a sign standing for something.
<br />
But that is down to semantics, not computation. Whilst we don't expect
<br />
the sign &quot;dog&quot; to be understood universally, we regard mathematics as
<br />
a universal language, so we put things like
<br />
<br />| || ||| ||||
<br />
<br /><br />on space probes, expecting them to be understood. But an entity that
<br />
can understand a basic numeric sequence could understand a basic
<br />
mathematical function. So taking our best guesses about
<br />
intersubjective comprehensibility to stand for objectivity,
<br />
mathematical computation is objective.
<br />
Is hypercomoutation a testable hypothesis? We can decide between non-
<br />
computable physics (CM) and computable physics (QM). What the question
<br />
hinges on is the different kinds and levels of proof used in emprical
<br />
science and maths/logic.
<br />
Is Reality real ? Nick Bostrom's Simulation Argument
<br />
The Simulation Argument seeks to show that it is not just possible
<br />
that we are living inside a simulation, but likely.
<br />
<br />1 You cannot simulate a world of X complexity inside a world of X
<br />
complexity.(quart-into-a-pint-pot-problem).
<br />
<br />2 Therefore, if we are in a simulation the 'real' world outside the
<br />
simulation is much more complex and quite possibly completely
<br />
different to the simulated world.
<br />
<br />3 In which case, we cannot make sound inferences from the world we are
<br />
appear to be in to alleged real world in which the simulation is
<br />
running
<br />
<br />4 Therefore we cannot appeal to an argumentative apparatus of advanced
<br />
races, simulations etc, since all those concepts are derived from the
<br />
world as we see it -- which, by hypothesis is a mere simulation.
<br />
<br />5 Therefore, the simulation argument pulls the metaphysical rug from
<br />
under its epistemological feet.
<br />
<br />The counterargument does not show that we are not living in a
<br />
simulation, but if we are , we have no way of knowing whether it is
<br />
likely or not. Even if it seems likely that we will go on to create
<br />
(sub) simulations, that does not mean we are living in a simulation
<br />
that is likely for the same reasons, since our simulation might be
<br />
rare and peculiar. In particular, it might have the peculiarity that
<br />
sub-simulations are easy to create in it. For all we know our
<br />
simulators had extreme difficulty in creating our universe. In this
<br />
case, the fact that it is easy to create sub simulations within our
<br />
(supposed) simulation, does not mean it is easy to creae simulations
<br />
per se.
<br />
Computational counterfactuals, and the Computational-Platonic Argument
<br />
for Immaterial Minds
<br />
For one, there is the argument that: A computer programme is just a
<br />
long number, a string of 1's and 0's.
<br />
(All) numbers exist Platonically (according to Platonism)
<br />
Therefore, all programmes exist Platonically.
<br />
<br />A mind is special kind of programme (According to computaionalism)
<br />
All programmes exist Platonically (previous argument)
<br />
Therefore, all possible minds exist Platonically
<br />
Therefore, a physical universe is unnecessary -- our minds exist
<br />
already in the Platonic realm
<br />
<br />The argument has a number of problems even allowing the assumptions of
<br />
Platonism, and computationalism.
<br />
<br />A programme is not the same thing as a process.
<br />
<br />Computationalism refers to real, physical processes running on
<br />
material computers. Proponents of the argument need to show that the
<br />
causality and dynamism are inessential (that there is no relevant
<br />
difference between process and programme) before you can have
<br />
consciousness implemented Platonically.
<br />
<br />To exist Platonically is to exist eternally and necessarily. There is
<br />
no time or change in Plato's heave. Therefore, to &quot;gain entry&quot;, a
<br />
computational mind will have to be translated from a running process
<br />
into something static and acausal.
<br />
<br />One route is to replace the process with a programme. let's call this
<br />
the Programme approach.. After all, the programme does specify all the
<br />
possible counterfactual behaviour, and it is basically a string of 1's
<br />
and 0's, and therefore a suitable occupant of Plato's heaven. But a
<br />
specification of counterfactual behaviour is not actual counterfactual
<br />
behaviour. The information is the same, but they are not the same
<br />
thing.
<br />
<br />No-one would believe that a brain-scan, however detailed, is
<br />
conscious, so not computationalist, however ardent, is required to
<br />
believe that a progamme on a disk, gathering dust on a shelf, is
<br />
sentient, however good a piece of AI code it may be!
<br />
<br />Another route is &quot;record&quot; the actual behaviour, under some
<br />
circumstances of a process, into a stream of data (ultimately, a
<br />
string of numbers, and therefore something already in Plato's heaven).
<br />
Let's call this the Movie approach. This route loses the conditional
<br />
structure, the counterfactuals that are vital to computer programmes
<br />
and therefore to computationalism.
<br />
<br />Computer programmes contain conditional (if-then) statements. A given
<br />
run of the programme will in general not explore every branch. yet the
<br />
unexplored branches are part of the programme. A branch of an if-then
<br />
statement that is not executed on a particular run of a programme will
<br />
constitute a counterfactual, a situation that could have happened but
<br />
didn't. Without counterfactuals you cannot tell which programme
<br />
(algorithm) a process is implementing because two algorithms could
<br />
have the same execution path but different unexecuted branches.
<br />
<br />Since a &quot;recording&quot; is not computation as such, the computationalist
<br />
need not attribute mentality to it -- it need not have a mind of its
<br />
own, any more than the characters in a movie.
<br />
<br />(Another way of looking at this is via the Turing Test; a mere
<br />
recording would never pass a TT since it has no condiitonal/
<br />
counterfactual behaviour and therfore cannot answer unexpected
<br />
questions).
<br />
<br />A third approach is make a movie of all possible computational
<br />
histories, and not just one. Let's call thsi the Many-Movie approach.
<br />
<br />In this case a computation would have to be associated with all
<br />
related branches in order to bring all the counterfactuals (or rather
<br />
conditionals) into a single computation.
<br />
<br />(IOW treating branches individually would fall back into the problems
<br />
of the Movie approach)
<br />
<br />If a computation is associated with all branches, consciousness will
<br />
also be according to computationalism. That will bring on a White
<br />
Rabbit problem with a vengeance.
<br />
<br />However, it is not that computation cannot be associated with
<br />
counterfactuals in single-universe theories -- in the form of
<br />
unrealised possibilities, dispositions and so on. If consciousness
<br />
supervenes on computation , then it supervenes on such counterfactuals
<br />
too; this amounts to the response to Maudlin's argument in wihch the
<br />
physicalist abandons the claim that consciousness supervenes on
<br />
activity.
<br />
<br />Of ocurse, unactualised possibilities in a single universe are never
<br />
going to lead to any White Rabbits!
<br />
Turing and Other Machines
<br />
Turing machines are the classical model of computation, but it is
<br />
doubtful whether they are the best model for human (or other organic)
<br />
intelligence. Turing machines take a fixed input, take as much time as
<br />
necessary to calculate a result, and produce a perfect result (in some
<br />
cases, they will carry on refining a result forever). Biological
<br />
survival is all about coming up with good-enough answers to a tight
<br />
timescale. Mistaking a shadow for a sabre-tooth tiger is a msitake,
<br />
but it is more accpetable than standing stock still calculating the
<br />
perfect interpretation of your visual information, only to ge eaten.
<br />
This doesn't put natural cognition beyone the bounds of computation,
<br />
but it does mean that the Turing Machine is not the ideal model.
<br />
Biological systems are more like real time systems, which have to
<br />
&quot;keep up&quot; with external events, at the expense of doing some things
<br />
imprefectly.
<br />
Quantum and Classical Computers
<br />
(Regarding David Deutsch's FoR)
<br />
To simulate a general quantum system with a classical computer you
<br />
need a number of bits that scales exponentially with the number of
<br />
qubits in the system. For a universal quantum computer the number of
<br />
qubits needed to simulate a system scales linearly with the number of
<br />
qubits in the system. So simulating quantum systems classically is
<br />
intractable, simulating quantum systems with a universal quantum
<br />
computer is tractable.
<br />
Time and Causality in Physics and Computation
<br />
The sum total of all the positions of particles of matter specififies
<br />
a (classical) physical state, but not how the state evolves. Thus it
<br />
seems that the universe cannot be built out of 0-width (in temporal
<br />
terms) slices alone. Physics needs to appeal to something else.
<br />
<br />There is one dualistic and two monistic solutions to this.
<br />
<br />The dualistic solution is that the universe consists (separately) of
<br />
states+the laws of universe. It is like a computer, where the data
<br />
(state) evolves according to the programme (laws).
<br />
<br />One of the monistic solutions is to put more information into states.
<br />
Physics has an age old &quot;cheat&quot; of &quot;instantaneous velocities&quot;. This
<br />
gives more information about how the state will evolve. But the state
<br />
is no longer 0-width, it is infinitessimal.
<br />
<br />Another example of states-without-laws is Julian Barbour's Platonia.
<br />
Full Newtonian mechanics cannot be recovered from his &quot;Machian&quot;
<br />
approach, but he thinks that what is lost (universes with overall
<br />
rotation and movement) is no loss.
<br />
<br />The other dualistic solution is the opposite of the second: laws-
<br />
without-states. For instance, Stephen Hawking's No Boundary Conditions
<br />
proposal
<br />
<br />Maudlin's Argument and Counterfactuals
<br />
We have already mentioned a parallel with computation. There is also
<br />
relevance to Tim Maudlin's claim that computationalism is incompatible
<br />
with physicalism. His argument hinges on serparating the activity of a
<br />
comptuaitonal system from its causal dispositions. Consciousness, says
<br />
Maudlin supervened on activity alone. Parts of an AI mechansim that
<br />
are not triggered into activity can be disabled without changing
<br />
consciousness. However, such disabling changes the computation being
<br />
performed, because programmes contain if-then statements only one
<br />
branch of which can be executed at a time. The other branch is a
<br />
&quot;counterfactual&quot;, as situation that could have happened but didn't.
<br />
Nonetheless, these counterfactuals are part of the algorithm. If
<br />
changing the algorithm doesn't change the conscious state (because it
<br />
only supervenes on the active parts of the process, not the unrealised
<br />
counterfactuals), consciousness does not supervene on computation.
<br />
<br />However, If causal dispositions are inextricably part of a physical
<br />
state, you can't separate activity from counterfactuals. Maudlin's
<br />
argument would then have to rely on disabling counterfactuals of a
<br />
specifically computational sort.
<br />
<br />We earlier stated that the dualistic solution is like the separation
<br />
between programme and data in a (conventional) computer programme.
<br />
However, AI-type programmes are typified by the fact that there is not
<br />
a barrier between code and programme -- AI software is self-modifying,
<br />
so it is its own data. Just as it is not physically necessary that
<br />
there is a clear distinction between states and laws (and thus a
<br />
separability of physical counterfactuals), so it isn't necessarily the
<br />
case that there is a clear distinciton between programme and data, and
<br />
thus a separability of computational counterfactuals. PDJ 19/8/06
<br />
Chalmers on GLUTS
<br />
Daryl McCullough writes:
<br />
<br />&nbsp;&nbsp;&nbsp;&nbsp;I made the split to satisfy *you*, Dave. In our discussion about
<br />
the table lookup program, your main argument against the table lookup
<br />
being conscious was the &quot;lack of richness&quot; of its thinking process.
<br />
And this lack of richness was revealed by the fact that it took zero
<br />
time to &quot;think&quot; about its inputs before it made its outputs. So I have
<br />
patched up this discrepancy by allowing &quot;silent&quot; transitions where
<br />
there is thinking, but no inputs. However, as I thought my example
<br />
showed, this silent, internal thinking can be perfectly trivial; as
<br />
simple as counting. It is therefore not clear to me in what sense
<br />
there can be more &quot;richness&quot; in some FSA's than there is in a table
<br />
lookup.
<br />
<br />Dave Chalmers writes:
<br />
<br />&nbsp;&nbsp;&nbsp;&nbsp;I made it abundantly clear that the problem with the lookup table
<br />
is not the mere lack of silent transitions -- see my response to your
<br />
message about the brain that beeps upon every step. Rather, the
<br />
objection is that (a) a lot of conscious experience goes on between
<br />
any two statements I make in a conversation; and (b) it's very
<br />
implausible that a single state-transition could be responsible for
<br />
all that conscious experience.
<br />
<br />&nbsp;&nbsp;&nbsp;&nbsp;Like the beeping brain, ordinary FSAs with null inputs and outputs
<br />
aren't vulnerable to this argument, as in those cases the richness of
<br />
such conscious experience need not result from a single state-
<br />
transition, but from a combination of many.
<br />
<br />DM:
<br />
<br />&nbsp;&nbsp;&nbsp;&nbsp;If you allow a &quot;null input&quot; to be a possible input, then the
<br />
humongous table lookup program becomes functionally equivalent to a
<br />
human brain. To see this, note that the states of the table lookup
<br />
program are essentially sequences of inputs [i_1,i_2,i_3,...,i_n]. We
<br />
use the mapping M([]) = the initial state, M([i_1,i_2, ..., i_n,i_{n
<br />
+1}]) = I(M([i_1,i_2, ..., i_n]),i_{n+1}). The output for state
<br />
[i_1,i_2, ..., i_n] is whatever the lookup table has for that sequence
<br />
of inputs, which is correct by the assumption that the table lookup
<br />
program gets the behavior right.
<br />
<br />DC:
<br />
<br />&nbsp;&nbsp;&nbsp;&nbsp;You made essentially this argument before, and I responded in a
<br />
message of Feb 28. Here's the relevant material:
<br />
<br />&nbsp;&nbsp;&nbsp;&nbsp;Your complaint about clocks, that they don't support
<br />
counterfactuals, is I think, easily corrected: for example, consider a
<br />
machine M with a state determined by a pair: the time, and the list of
<br />
all inputs ever made (with the times they were made). If
<br />
&quot;implementation&quot; simply means the existence of a mapping from the
<br />
physical system to the FSA, then it seems that such a system M would
<br />
simultaneously implement *every* FSA. Counterfactuals would be
<br />
covered, too.
<br />
<br />&nbsp;&nbsp;&nbsp;&nbsp;This is an interesting example, which also came up in an e-mail
<br />
discussion recently. One trouble with the way you've phrased it is
<br />
that it doesn't support outputs (our FSAs have outputs as well as
<br />
inputs, potentially throughout their operation); but this can be fixed
<br />
by the usual &quot;humongous lookup table&quot; method. So what's to stop us
<br />
saying that a humongous lookup table doesn't implement any FSA to
<br />
which it's I/O equivalent? (You can think of the table as the
<br />
&quot;unrolled&quot; FSA, with new branches being created for each input. To map
<br />
FSA states to (big disjunctions of) table states, simply take the
<br />
image of any FSA state under the unrolling process.) This is a tricky
<br />
question. Perhaps the best answer is that it really doesn't have the
<br />
right state-transitional structure, as it can be in a given state
<br />
without producing the right output and transiting into the appropriate
<br />
next state, namely when it's at the end of the table. Of course this
<br />
won't work for the implementation of halting FSAs (i.e. ones that must
<br />
halt eventually, for any inputs, but one could argue that the FSA
<br />
which describes a human at a given time isn't a halting FSA (the human
<br />
itself might be halting, but that's because of extraneous influences
<br />
on the FSA). Your example above doesn't have the problem at the end of
<br />
the table; it just goes on building up its inputs forever, but at cost
<br />
of being able to produce the right outputs.
<br />
<br />&nbsp;&nbsp;&nbsp;&nbsp;Not that I don't think lookup-tables pose some problems for
<br />
functionalism -- see my long response to Calvin Ostrum. But in any
<br />
case this is far from Putnam's pan-implementationalism.
<br />
<br />DM:
<br />
<br />&nbsp;&nbsp;&nbsp;&nbsp;The conclusion, whether you have silent transitions or not, is
<br />
that functional equivalence doesn't impose any significant constraints
<br />
on a system above and beyond those imposed by behavioral equivalence.
<br />
<br />DC:
<br />
<br />&nbsp;&nbsp;&nbsp;&nbsp;Even if your argument above were valid, this certainly wouldn't
<br />
follow -- the requirement that a system contains a humongous lookup
<br />
table is certainly a significant constraint! I also note that you've
<br />
made no response to my observation that your original example, even
<br />
with the silent transitions, is vastly constrained, about as
<br />
constrained as we'd expect an implementation to be.
<br />
<br />--~--~---------~--~----~------------~-------~--~----~
<br />
You received this message because you are subscribed to the Google Groups &quot;Everything List&quot; group.
<br />
To post to this group, send email to everything-list.domain.name.hidden
<br />
To unsubscribe from this group, send email to everything-list+unsubscribe.domain.name.hidden
<br />
For more options, visit this group at <a href="http://groups.google.com/group/everything-list?hl=en">http://groups.google.com/group/everything-list?hl=en</a>
<br />
-~----------~----~----~----~------~----~------~--~---
<br />
<span id="received"><dfn>Received on</dfn> Thu Sep 03 2009 - 01:32:16 PDT</span>
</div>
<!-- body="end" -->
<div class="foot">
<map id="navbarfoot" name="navbarfoot" title="Related messages">
<ul class="links">
<li><dfn>This message</dfn>: [ <a href="#start17734">Message body</a> ]</li>
<!-- lnext="start" -->
<li><dfn>Next message</dfn>: <a href="17735.html" title="Next message in the list">Quentin Anciaux: "Re: Dreaming On"</a></li>
<li><dfn>Previous message</dfn>: <a href="17733.html" title="Previous message in the list">David Nyman: "Re: Dreaming On"</a></li>
<li><dfn>In reply to</dfn>: <a href="17733.html" title="Message to which this message replies">David Nyman: "Re: Dreaming On"</a></li>
<!-- lnextthread="start" -->
<li><dfn>Next in thread</dfn>: <a href="17735.html" title="Next message in this discussion thread">Quentin Anciaux: "Re: Dreaming On"</a></li>
<li><a name="replies" id="replies"></a>
<dfn>Reply</dfn>: <a href="17735.html" title="Message sent in reply to this message">Quentin Anciaux: "Re: Dreaming On"</a></li>
<li><dfn>Reply</dfn>: <a href="17761.html" title="Message sent in reply to this message">David Nyman: "Re: Dreaming On"</a></li>
<!-- lreply="end" -->
</ul>
<ul class="links">
<li><a name="options3" id="options3"></a><dfn>Contemporary messages sorted</dfn>: [ <a href="date.html#msg17734" title="Contemporary messages by date">by date</a> ] [ <a href="index.html#msg17734" title="Contemporary discussion threads">by thread</a> ] [ <a href="subject.html#msg17734" title="Contemporary messages by subject">by subject</a> ] [ <a href="author.html#msg17734" title="Contemporary messages by author">by author</a> ] [ <a href="attachment.html" title="Contemporary messages by attachment">by messages with attachments</a> ]</li>
</ul>
</map>
</div>
<!-- trailer="footer" -->
<p><small><em>
This archive was generated by <a href="http://www.hypermail-project.org/">hypermail 2.3.0</a>
: Fri Feb 16 2018 - 13:20:16 PST
</em></small></p>
</body>
</html>
