<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1" />
<meta name="generator" content="hypermail 2.3.0, see http://www.hypermail-project.org/" />
<title>Re: MGA 1 from Brent Meeker on 2008-11-21 (everything)</title>
<meta name="Author" content="Brent Meeker (meekerdb.domain.name.hidden)" />
<meta name="Subject" content="Re: MGA 1" />
<meta name="Date" content="2008-11-21" />
<style type="text/css">
/*<![CDATA[*/
/* To be incorporated in the main stylesheet, don't code it in hypermail! */
body {color: black; background: #ffffff}
dfn {font-weight: bold;}
pre { background-color:inherit;}
.head { border-bottom:1px solid black;}
.foot { border-top:1px solid black;}
th {font-style:italic;}
table { margin-left:2em;}map ul {list-style:none;}
#mid { font-size:0.9em;}
#received { float:right;}
address { font-style:inherit ;}
/*]]>*/
.quotelev1 {color : #990099}
.quotelev2 {color : #ff7700}
.quotelev3 {color : #007799}
.quotelev4 {color : #95c500}
.period {font-weight: bold}
</style>
</head>
<body>
<div class="head">
<h1>Re: MGA 1</h1>
<!-- received="Fri Nov 21 13:19:31 2008" -->
<!-- isoreceived="20081121211931" -->
<!-- sent="Fri, 21 Nov 2008 10:19:21 -0800" -->
<!-- isosent="20081121181921" -->
<!-- name="Brent Meeker" -->
<!-- email="meekerdb.domain.name.hidden" -->
<!-- subject="Re: MGA 1" -->
<!-- id="4926FBA9.8020605.domain.name.hidden" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="F614D963-6109-4A6B-968A-06679A606848.domain.name.hidden" -->
<!-- expires="-1" -->
<map id="navbar" name="navbar">
<ul class="links">
<li>
<dfn>This message</dfn>:
[ <a href="#start15334" name="options1" id="options1" tabindex="1">Message body</a> ]
 [ More options (<a href="#options2">top</a>, <a href="#options3">bottom</a>) ]
</li>
<li>
<dfn>Related messages</dfn>:
<!-- unext="start" -->
[ <a href="15335.html" accesskey="d" title="Bruno Marchal: &quot;MGA 2&quot;">Next message</a> ]
[ <a href="15333.html" title="Jason Resch: &quot;Re: MGA 1&quot;">Previous message</a> ]
[ <a href="15324.html" title="Kory Heath: &quot;Re: MGA 1&quot;">In reply to</a> ]
<!-- unextthread="start" -->
[ <a href="15373.html" accesskey="t" title="Bruno Marchal: &quot;Re: MGA 1&quot;">Next in thread</a> ]
<!-- ureply="end" -->
</li>
</ul>
</map>
<ul class="links">
<li><a name="options2" id="options2"></a><dfn>Contemporary messages sorted</dfn>: [ <a href="date.html#msg15334" title="Contemporary messages by date">by date</a> ] [ <a href="index.html#msg15334" title="Contemporary discussion threads">by thread</a> ] [ <a href="subject.html#msg15334" title="Contemporary messages by subject">by subject</a> ] [ <a href="author.html#msg15334" title="Contemporary messages by author">by author</a> ] [ <a href="attachment.html" title="Contemporary messages by attachment">by messages with attachments</a> ]</li>
</ul>
</div>
<!-- body="start" -->
<div class="mail">
<address class="headers">
<span id="from">
<dfn>From</dfn>: Brent Meeker &lt;<a href="mailto:meekerdb.domain.name.hidden?Subject=Re%3A%20MGA%201">meekerdb.domain.name.hidden</a>&gt;
</span><br />
<span id="date"><dfn>Date</dfn>: Fri, 21 Nov 2008 10:19:21 -0800</span><br />
</address>
<br />
Kory Heath wrote:
<br />
<em class="quotelev1">&gt; 
</em><br />
<em class="quotelev1">&gt; On Nov 20, 2008, at 10:52 AM, Bruno Marchal wrote:
</em><br />
<em class="quotelev2">&gt;&gt; I am afraid you are already too much suspect of the contradictory
</em><br />
<em class="quotelev2">&gt;&gt; nature of MEC+MAT.
</em><br />
<em class="quotelev2">&gt;&gt; Take the reasoning has a game. Try to keep both MEC and MAT, the game
</em><br />
<em class="quotelev2">&gt;&gt; consists in showing the more clearly as possible what will go wrong.
</em><br />
<em class="quotelev1">&gt; 
</em><br />
<em class="quotelev1">&gt; I understand what you're saying, and I accept the rules of the game. I  
</em><br />
<em class="quotelev1">&gt; *am* trying to keep both MEC and MAT. But it seems as though we differ  
</em><br />
<em class="quotelev1">&gt; on how we understand MEC and MAT, because in my understanding,  
</em><br />
<em class="quotelev1">&gt; mechanist-materialists should say that Bruno's Lucky Alice is not  
</em><br />
<em class="quotelev1">&gt; conscious (for the same reason that Telmo's Lucky Alice is not  
</em><br />
<em class="quotelev1">&gt; conscious).
</em><br />
<em class="quotelev1">&gt; 
</em><br />
<em class="quotelev2">&gt;&gt; You mean the ALICE of Telmo's solution of MGA 1bis, I guess. The
</em><br />
<em class="quotelev2">&gt;&gt; original Alice, well I mean the one in MGA 1, is functionally
</em><br />
<em class="quotelev2">&gt;&gt; identical at the right level of description (actually she has already
</em><br />
<em class="quotelev2">&gt;&gt; digital brain). The physical instantiation of a computation is
</em><br />
<em class="quotelev2">&gt;&gt; completely realized. No neurons can &quot;know&quot; that the info (correct and
</em><br />
<em class="quotelev2">&gt;&gt; at the right places) does not come from the relevant neurons, but from
</em><br />
<em class="quotelev2">&gt;&gt; a lucky beam.
</em><br />
<em class="quotelev1">&gt; 
</em><br />
<em class="quotelev1">&gt; I agree that the neurons don't &quot;know&quot; or &quot;care&quot; where their inputs are  
</em><br />
<em class="quotelev1">&gt; coming from. They just get their inputs, perform their computations,  
</em><br />
<em class="quotelev1">&gt; and send their outputs. But when it comes to the functional, physical  
</em><br />
<em class="quotelev1">&gt; behavior of Alice's whole brain, the mechanist-materialist is  
</em><br />
<em class="quotelev1">&gt; certainly allowed (indeed, forced) to talk about where each neuron's  
</em><br />
<em class="quotelev1">&gt; input is coming from. That's a part of the computational picture.
</em><br />
<em class="quotelev1">&gt; 
</em><br />
<em class="quotelev1">&gt; I see the point that you're making. Each neuron receives some input,  
</em><br />
<em class="quotelev1">&gt; performs some computation, and then produces some output. We're  
</em><br />
<em class="quotelev1">&gt; imagining that every neuron has been disconnected from its inputs, but  
</em><br />
<em class="quotelev1">&gt; that cosmic rays have luckily produced the exact same input that the  
</em><br />
<em class="quotelev1">&gt; previously connected neurons would have produced. You're arguing that  
</em><br />
<em class="quotelev1">&gt; since every neuron is performing the exact same computations that it  
</em><br />
<em class="quotelev1">&gt; would have performed anyway, the two situations are computationally  
</em><br />
<em class="quotelev1">&gt; identical.
</em><br />
<em class="quotelev1">&gt; 
</em><br />
<em class="quotelev1">&gt; But I don't think that's correct. I think that plain old, garden  
</em><br />
<em class="quotelev1">&gt; variety mechanism-materialism has an easy way of saying that Lucky  
</em><br />
<em class="quotelev1">&gt; Alice's brain, viewed as a whole system, is not performing the same  
</em><br />
<em class="quotelev1">&gt; computations that fully-functioning Alice's brain is. None of the  
</em><br />
<em class="quotelev1">&gt; neurons in Lucky Alice's brain are even causally connected to each  
</em><br />
<em class="quotelev1">&gt; other. That's a pretty big computational difference!
</em><br />
<em class="quotelev1">&gt; 
</em><br />
<em class="quotelev1">&gt; I am arguing, in essence, that for the mechanist-materialist,  
</em><br />
<em class="quotelev1">&gt; &quot;causality&quot; is an important aspect of computation and consciousness.  
</em><br />
<em class="quotelev1">&gt; Maybe your goal is to show that there's something deeply wrong with  
</em><br />
<em class="quotelev1">&gt; that idea, or with the idea of &quot;causality&quot; itself. But we're supposed  
</em><br />
<em class="quotelev1">&gt; to be starting from a foundation of MEC and MAT.
</em><br />
<em class="quotelev1">&gt; 
</em><br />
<em class="quotelev1">&gt; Are you saying that the mechanist-materialist *does* say that Lucky  
</em><br />
<em class="quotelev1">&gt; Alice is conscious, or only that the mechanist-materialist *should*  
</em><br />
<em class="quotelev1">&gt; say it? Because if you're saying the latter, then I'm &quot;playing the  
</em><br />
<em class="quotelev1">&gt; game&quot; better than you are! I'm pretty sure that Dennett (and the other  
</em><br />
<em class="quotelev1">&gt; mechanist-materialists I've read) would say that Lucky Alice is not  
</em><br />
<em class="quotelev1">&gt; conscious, and for them, they have a perfectly straightforward way of  
</em><br />
<em class="quotelev1">&gt; explaining what they *mean* when they say that she's not conscious.  
</em><br />
<em class="quotelev1">&gt; They mean (among other things) that the actions of her neurons are not  
</em><br />
<em class="quotelev1">&gt; being affected at all by the paper lying in front of her on the table,  
</em><br />
<em class="quotelev1">&gt; or the ball flying at her head. For Dennett, it's practically a non- 
</em><br />
<em class="quotelev1">&gt; sequitur to say that she's conscious of a ball that's not affecting  
</em><br />
<em class="quotelev1">&gt; her brain.
</em><br />
<em class="quotelev1">&gt; 
</em><br />
<em class="quotelev2">&gt;&gt; But the physical difference does not play a role.
</em><br />
<em class="quotelev1">&gt; 
</em><br />
<em class="quotelev1">&gt; It depends on what you mean by &quot;play a role&quot;. You're right that the  
</em><br />
<em class="quotelev1">&gt; physical difference (very luckily) didn't change what the neurons did.  
</em><br />
<em class="quotelev1">&gt; It just so happens that the neurons did exactly what they were going  
</em><br />
<em class="quotelev1">&gt; to do anyway. But the *cause* of why the neurons did what they did is  
</em><br />
<em class="quotelev1">&gt; totally different. The action of each individual neuron was caused by  
</em><br />
<em class="quotelev1">&gt; cosmic rays rather than by neighboring neurons. You seem to be asking,  
</em><br />
<em class="quotelev1">&gt; &quot;Why should this difference play any role in whether or not Alice was  
</em><br />
<em class="quotelev1">&gt; conscious?&quot; But for the mechanist-materialist, the difference is  
</em><br />
<em class="quotelev1">&gt; primary. Those kinds of causal connections are a fundamental part of  
</em><br />
<em class="quotelev1">&gt; what they *mean* when they say that something is conscious.
</em><br />
<em class="quotelev1">&gt; 
</em><br />
<em class="quotelev2">&gt;&gt; If you invoke it,
</em><br />
<em class="quotelev2">&gt;&gt; how could you accept saying yes to a doctor, who introduce bigger
</em><br />
<em class="quotelev2">&gt;&gt; difference?
</em><br />
<em class="quotelev1">&gt; 
</em><br />
<em class="quotelev1">&gt; Do you mean the &quot;teleportation doctor&quot;, who makes a copy of me,  
</em><br />
<em class="quotelev1">&gt; destroys me, and then reconstructs me somewhere else using the copied  
</em><br />
<em class="quotelev1">&gt; information? That case is not problematic in the way that Lucky Alice  
</em><br />
<em class="quotelev1">&gt; is, because there is an unbroken causal chain between the &quot;new&quot; me and  
</em><br />
<em class="quotelev1">&gt; the &quot;old&quot; me. What's problematic about Lucky Alice is the fact that  
</em><br />
<em class="quotelev1">&gt; her ducking out of the way of the ball (the movements of her eyes, the  
</em><br />
<em class="quotelev1">&gt; look of surprise, etc.) has nothing to do with the ball, and yet  
</em><br />
<em class="quotelev1">&gt; somehow she's still supposed to be conscious of the ball.
</em><br />
<em class="quotelev1">&gt; 
</em><br />
<em class="quotelev1">&gt; A much closer analogy to Lucky Alice would be if the doctor  
</em><br />
<em class="quotelev1">&gt; accidentally destroys me without making the copy, turns on the  
</em><br />
<em class="quotelev1">&gt; receiving teleporter in desperation, and then the exact copy that  
</em><br />
<em class="quotelev1">&gt; would have appeared anyway steps out, because (luckily!) cosmic rays  
</em><br />
<em class="quotelev1">&gt; hit the receiver's mechanisms in just the right way. I actually find  
</em><br />
<em class="quotelev1">&gt; this thought experiment more persuasive than Lucky Alice (although I'm  
</em><br />
<em class="quotelev1">&gt; sure some will argue that they're identical). At the very least, the  
</em><br />
<em class="quotelev1">&gt; mechanist-materialist has to say that the resulting Lucky Kory is  
</em><br />
<em class="quotelev1">&gt; conscious. I think it's also clear that Lucky Kory's consciousness  
</em><br />
<em class="quotelev1">&gt; must be exactly what it would have been if the teleportation had  
</em><br />
<em class="quotelev1">&gt; worked correctly. This does in fact lead me to feel that maybe  
</em><br />
<em class="quotelev1">&gt; causality shouldn't have any bearing on consciousness after all.
</em><br />
<em class="quotelev1">&gt; 
</em><br />
<em class="quotelev1">&gt; However, the materialist-mechanist still has some grounds to say that  
</em><br />
<em class="quotelev1">&gt; there's something interestingly different about Lucky Kory than  
</em><br />
<em class="quotelev1">&gt; Original Kory. It is a physical fact of the matter that Lucky Kory is  
</em><br />
<em class="quotelev1">&gt; not causally connected to Pre-Teleportation Kory. When someone asks  
</em><br />
<em class="quotelev1">&gt; Lucky Kory, &quot;Why do you tie your shoes that way?&quot;, and Lucky Kory  
</em><br />
<em class="quotelev1">&gt; says, &quot;Because of something I learned when I was ten years old&quot;, Lucky  
</em><br />
<em class="quotelev1">&gt; Kory's statement is quite literally false. Lucky Kory ties his shoes  
</em><br />
<em class="quotelev1">&gt; that way because of some cosmic rays. I actually don't know what the  
</em><br />
<em class="quotelev1">&gt; standard mechanist-materialist way of viewing this situation is. But  
</em><br />
<em class="quotelev1">&gt; it does seem to suggest that maybe breaks in the causal chain  
</em><br />
<em class="quotelev1">&gt; shouldn't affect consciousness after all.
</em><br />
<em class="quotelev1">&gt; 
</em><br />
<em class="quotelev1">&gt; And of course, we can turn the screws in the usual way. If we can do  
</em><br />
<em class="quotelev1">&gt; Lucky Teleportation once, we can do it once a day, and then once an  
</em><br />
<em class="quotelev1">&gt; hour, and then once a second, and so on, until eventually we just have  
</em><br />
<em class="quotelev1">&gt; nothing but random numbers, and if those random numbers happen to look  
</em><br />
<em class="quotelev1">&gt; like Kory, aren't they just as conscious as Lucky Kory was? But this  
</em><br />
<em class="quotelev1">&gt; doesn't convince me (yet) that Lucky Alice should be viewed as  
</em><br />
<em class="quotelev1">&gt; conscious after all. It just convinces me (again) that there's  
</em><br />
<em class="quotelev1">&gt; something weird about the mechanistic-materialist view of  
</em><br />
<em class="quotelev1">&gt; consciousness. Or about the materialist's view of &quot;causality&quot;.
</em><br />
<em class="quotelev1">&gt; 
</em><br />
<em class="quotelev3">&gt;&gt;&gt; But the mechanist-materialist can (and must) claim that
</em><br />
<em class="quotelev3">&gt;&gt;&gt; Lucky Alice did not in fact respond to the ball at all.
</em><br />
<em class="quotelev2">&gt;&gt; Consciously or privately?
</em><br />
<em class="quotelev1">&gt; 
</em><br />
<em class="quotelev1">&gt; Physically! By the definition of the thought experiment, it is a  
</em><br />
<em class="quotelev1">&gt; physical fact that no neuron in Alice's head responded to the ball (in  
</em><br />
<em class="quotelev1">&gt; the indirect way that they normally would have if she were wired  
</em><br />
<em class="quotelev1">&gt; correctly). Whether or not she had a conscious experience of a ball is  
</em><br />
<em class="quotelev1">&gt; a different question.
</em><br />
<em class="quotelev1">&gt; 
</em><br />
<em class="quotelev3">&gt;&gt;&gt; When Alice's brain is working properly, her act of
</em><br />
<em class="quotelev3">&gt;&gt;&gt; ducking *is* causally connected to the movement of the ball. And this
</em><br />
<em class="quotelev3">&gt;&gt;&gt; kind of causal connection is an important part of what the mechanist-
</em><br />
<em class="quotelev3">&gt;&gt;&gt; materialist means by &quot;consciousness&quot;.
</em><br />
<em class="quotelev2">&gt;&gt; Careful:  such kind of causality needs ... MAT.
</em><br />
<em class="quotelev1">&gt; 
</em><br />
<em class="quotelev1">&gt; Yes, of course. But we're *supposed* to be considering the question in  
</em><br />
<em class="quotelev1">&gt; the context of MAT.
</em><br />
<em class="quotelev1">&gt; 
</em><br />
<em class="quotelev2">&gt;&gt; But at that level, a
</em><br />
<em class="quotelev2">&gt;&gt; neurophysiologist looking in the detail would see the neurons doing
</em><br />
<em class="quotelev2">&gt;&gt; their job. Only, he will also see, some neurons breaking down, and
</em><br />
<em class="quotelev2">&gt;&gt; then being fixed, not by an internal biological fixing mechanism (like
</em><br />
<em class="quotelev2">&gt;&gt; it occurs all the time in biological system, but by a lucky beam, but
</em><br />
<em class="quotelev2">&gt;&gt; despite this, and thanks to this, the brain of Alice (MGA 1) does the
</em><br />
<em class="quotelev2">&gt;&gt; entire normal usual work.
</em><br />
<em class="quotelev1">&gt; 
</em><br />
<em class="quotelev1">&gt; What do you mean by &quot;fixed&quot;? If the cosmic rays &quot;fix&quot; the neurons so  
</em><br />
<em class="quotelev1">&gt; that they are able to respond to the input of their neighboring  
</em><br />
<em class="quotelev1">&gt; neurons as they're supposed to, then I've misunderstood the thought  
</em><br />
<em class="quotelev1">&gt; experiment. But if you mean that the cosmic rays &quot;fix&quot; the neurons by  
</em><br />
<em class="quotelev1">&gt; (very luckily) sending them the same inputs that they would have  
</em><br />
<em class="quotelev1">&gt; received from their neighboring neurons, then I don't agree that the  
</em><br />
<em class="quotelev1">&gt; neurophysiologist looking at the details would conclude that the  
</em><br />
<em class="quotelev1">&gt; neurons are doing their job, or that the brain of Alice MGA 1 is doing  
</em><br />
<em class="quotelev1">&gt; its entire normal usual work. He would conclude that the brain is not  
</em><br />
<em class="quotelev1">&gt; physically reacting to the pencil or the paper or the ball at all. For  
</em><br />
<em class="quotelev1">&gt; a mechanist, how can a person be aware of a ball if not a single  
</em><br />
<em class="quotelev1">&gt; neuron in her head is physically reacting to that ball?
</em><br />
<em class="quotelev1">&gt; 
</em><br />
<em class="quotelev3">&gt;&gt;&gt; The mechanist-materialist can only talk about
</em><br />
<em class="quotelev3">&gt;&gt;&gt; consciousness in computational / physical terms. For Dennett, if you
</em><br />
<em class="quotelev3">&gt;&gt;&gt; say that Alice is &quot;aware&quot;, you must be able to translate this into
</em><br />
<em class="quotelev3">&gt;&gt;&gt; mechanistic terms. And I can't see any mechanistic sense in which
</em><br />
<em class="quotelev3">&gt;&gt;&gt; Lucky Alice can be said to be &quot;aware&quot; of anything.
</em><br />
<em class="quotelev2">&gt;&gt; Alice MGA 1 can be said to be aware in the roiginal mechanist sense.
</em><br />
<em class="quotelev2">&gt;&gt; When she thought &quot;Oh the math problem is easy&quot;, she trigged the right
</em><br />
<em class="quotelev2">&gt;&gt; memories in her brain, with the correct physical activity, even if
</em><br />
<em class="quotelev2">&gt;&gt; just luckily in that case.
</em><br />
<em class="quotelev1">&gt; 
</em><br />
<em class="quotelev1">&gt; Memory is notoriously confusing, so lets keep talking about the ball.  
</em><br />
<em class="quotelev1">&gt; What can a mechanist possibly mean by saying that Lucky Alice was  
</em><br />
<em class="quotelev1">&gt; aware of the ball? By the definition of the thought experiment (unless  
</em><br />
<em class="quotelev1">&gt; I've misunderstood it), every single neuron in Lucky Alice's brain is  
</em><br />
<em class="quotelev1">&gt; being triggered by cosmic rays rather than by neighboring neurons. Not  
</em><br />
<em class="quotelev1">&gt; a single action of any neuron (and therefore, not a single movement of  
</em><br />
<em class="quotelev1">&gt; her body) has anything to do with the movement of the ball. All we can  
</em><br />
<em class="quotelev1">&gt; say is that the neurons are (very improbably) being triggered in the  
</em><br />
<em class="quotelev1">&gt; exact same way that they *would* have been triggered if they were  
</em><br />
<em class="quotelev1">&gt; wired up correctly, and they were actually responding (indirectly) to  
</em><br />
<em class="quotelev1">&gt; the light on her retinas, etc.
</em><br />
<em class="quotelev1">&gt; 
</em><br />
<em class="quotelev1">&gt; So what would it mean to say that, nevertheless, Lucky Alice is aware  
</em><br />
<em class="quotelev1">&gt; of the ball? The only sense I can make of this is that, since each  
</em><br />
<em class="quotelev1">&gt; individual neuron is doing exactly what it would have done anyway, the  
</em><br />
<em class="quotelev1">&gt; same &quot;experience&quot; (qualia, whatever) results (or supervenes, or  
</em><br />
<em class="quotelev1">&gt; whatever). But that's exactly the view of consciousness that Dennett  
</em><br />
<em class="quotelev1">&gt; (the archetypical mechanist-materialist) has spent a lifetime arguing  
</em><br />
<em class="quotelev1">&gt; against. For him, that would be a very magical view of consciousness.  
</em><br />
<em class="quotelev1">&gt; For him, the &quot;experience&quot; of being aware of the ball, &quot;deciding&quot; to  
</em><br />
<em class="quotelev1">&gt; duck, etc., is simply what it feels like to be a collection of neurons  
</em><br />
<em class="quotelev1">&gt; responding to that ball. When he says, &quot;This collection of neurons is  
</em><br />
<em class="quotelev1">&gt; aware of that ball&quot;, he is saying, by definition, that that ball is  
</em><br />
<em class="quotelev1">&gt; having causal effects on those neurons. (And not just the causal  
</em><br />
<em class="quotelev1">&gt; effects that any physical object has on any nearby physical object.)
</em><br />
<br /><br />Just to make things more confusing ;-)  We should keep in mind that in current 
<br />
theories of physics the direction of time's arrow, and hence of &quot;causality&quot;, is 
<br />
a mere statistical phenomena and at a fundamental level all physical processes 
<br />
are reversible - along with their causal order.  In physics, causality just 
<br />
means no action-at-a distance.
<br />
<br />It seems to me that the conundrums of these thought experiments about zombies 
<br />
are muddled by invoking possibilities that are so improbable as to be 
<br />
impossible.  It's like considering playing craps with loaded dice that always 
<br />
come up sixes and then asking, &quot;Suppose cosmic rays always happened to come down 
<br />
and strike the dies so that they came up randomly; would playing with them be a 
<br />
fair game?&quot;  &quot;Being a fair game&quot; is an abstract concept, going beyond a 
<br />
particular sequence of events, and so is &quot;coming up randomly&quot;.  So any answer to 
<br />
this question has to fudge the difference between impossible and improbable.
<br />
<br />Questions about zombies seem to have the same character when you hypothesize 
<br />
their behavior is driven by cosmic rays or random number generators.  You're 
<br />
saying suppose something happened that is so improbable that it's impossible, do 
<br />
you now agree that it's possible or not?  If I say it's impossible, you answer 
<br />
that ex hypothesi, it could happen.  If I say it's possible, you can add to the 
<br />
example to make it more and more improbable, e.g. Alice dodges a ball AND she 
<br />
composes a concerto while playing tennis.
<br />
<br />Brent
<br />
<br /><br /><em class="quotelev1">&gt; 
</em><br />
<em class="quotelev2">&gt;&gt; And things will even be more confusing after MGA 2, but that's the
</em><br />
<em class="quotelev2">&gt;&gt; goal. MEC + MAT should give a contradiction, we will extract some
</em><br />
<em class="quotelev2">&gt;&gt; weirder and weirder proposition until the contradiction will be
</em><br />
<em class="quotelev2">&gt;&gt; utterly clear. OK?
</em><br />
<em class="quotelev1">&gt; 
</em><br />
<em class="quotelev1">&gt; Of course I'm entirely on board with the spirit of your thought  
</em><br />
<em class="quotelev1">&gt; experiment. You think MECH and MAT implies that Lucky Alice is  
</em><br />
<em class="quotelev1">&gt; conscious, but I don't think it does. I'm not sure how important that  
</em><br />
<em class="quotelev1">&gt; difference is. It seems substantial. But I can also predict where  
</em><br />
<em class="quotelev1">&gt; you're going with your thought experiment, and it's the exact same  
</em><br />
<em class="quotelev1">&gt; place I go. So by all means, continue on to MGA 2, and we'll see what  
</em><br />
<em class="quotelev1">&gt; happens.
</em><br />
<em class="quotelev1">&gt; 
</em><br />
<em class="quotelev1">&gt; -- Kory
</em><br />
<em class="quotelev1">&gt; 
</em><br />
<em class="quotelev1">&gt; 
</em><br />
<em class="quotelev2">&gt; &gt; 
</em><br />
<em class="quotelev1">&gt; 
</em><br />
<br /><br />--~--~---------~--~----~------------~-------~--~----~
<br />
You received this message because you are subscribed to the Google Groups &quot;Everything List&quot; group.
<br />
To post to this group, send email to everything-list.domain.name.hidden
<br />
To unsubscribe from this group, send email to everything-list+unsubscribe.domain.name.hidden
<br />
For more options, visit this group at <a href="http://groups.google.com/group/everything-list?hl=en">http://groups.google.com/group/everything-list?hl=en</a>
<br />
-~----------~----~----~----~------~----~------~--~---
<br />
<span id="received"><dfn>Received on</dfn> Fri Nov 21 2008 - 13:19:31 PST</span>
</div>
<!-- body="end" -->
<div class="foot">
<map id="navbarfoot" name="navbarfoot" title="Related messages">
<ul class="links">
<li><dfn>This message</dfn>: [ <a href="#start15334">Message body</a> ]</li>
<!-- lnext="start" -->
<li><dfn>Next message</dfn>: <a href="15335.html" title="Next message in the list">Bruno Marchal: "MGA 2"</a></li>
<li><dfn>Previous message</dfn>: <a href="15333.html" title="Previous message in the list">Jason Resch: "Re: MGA 1"</a></li>
<li><dfn>In reply to</dfn>: <a href="15324.html" title="Message to which this message replies">Kory Heath: "Re: MGA 1"</a></li>
<!-- lnextthread="start" -->
<li><dfn>Next in thread</dfn>: <a href="15373.html" title="Next message in this discussion thread">Bruno Marchal: "Re: MGA 1"</a></li>
<!-- lreply="end" -->
</ul>
<ul class="links">
<li><a name="options3" id="options3"></a><dfn>Contemporary messages sorted</dfn>: [ <a href="date.html#msg15334" title="Contemporary messages by date">by date</a> ] [ <a href="index.html#msg15334" title="Contemporary discussion threads">by thread</a> ] [ <a href="subject.html#msg15334" title="Contemporary messages by subject">by subject</a> ] [ <a href="author.html#msg15334" title="Contemporary messages by author">by author</a> ] [ <a href="attachment.html" title="Contemporary messages by attachment">by messages with attachments</a> ]</li>
</ul>
</map>
</div>
<!-- trailer="footer" -->
<p><small><em>
This archive was generated by <a href="http://www.hypermail-project.org/">hypermail 2.3.0</a>
: Fri Feb 16 2018 - 13:20:15 PST
</em></small></p>
</body>
</html>
