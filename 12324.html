<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1" />
<meta name="generator" content="hypermail 2.3.0, see http://www.hypermail-project.org/" />
<title>Re: computer pain from Bruno Marchal on 2006-12-27 (everything)</title>
<meta name="Author" content="Bruno Marchal (marchal.domain.name.hidden)" />
<meta name="Subject" content="Re: computer pain" />
<meta name="Date" content="2006-12-27" />
<style type="text/css">
/*<![CDATA[*/
/* To be incorporated in the main stylesheet, don't code it in hypermail! */
body {color: black; background: #ffffff}
dfn {font-weight: bold;}
pre { background-color:inherit;}
.head { border-bottom:1px solid black;}
.foot { border-top:1px solid black;}
th {font-style:italic;}
table { margin-left:2em;}map ul {list-style:none;}
#mid { font-size:0.9em;}
#received { float:right;}
address { font-style:inherit ;}
/*]]>*/
.quotelev1 {color : #990099}
.quotelev2 {color : #ff7700}
.quotelev3 {color : #007799}
.quotelev4 {color : #95c500}
.period {font-weight: bold}
</style>
</head>
<body>
<div class="head">
<h1>Re: computer pain</h1>
<!-- received="Wed Dec 27 13:22:55 2006" -->
<!-- isoreceived="20061227212255" -->
<!-- sent="Wed, 27 Dec 2006 19:22:36 +0100" -->
<!-- isosent="20061227182236" -->
<!-- name="Bruno Marchal" -->
<!-- email="marchal.domain.name.hidden" -->
<!-- subject="Re: computer pain" -->
<!-- id="42d9a5dd0b1a7a6a1b20c3f3f3ab9c90.domain.name.hidden" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="BAY124-W570D538B91D96C47375EB1D2C00.domain.name.hidden" -->
<!-- expires="-1" -->
<map id="navbar" name="navbar">
<ul class="links">
<li>
<dfn>This message</dfn>:
[ <a href="#start12324" name="options1" id="options1" tabindex="1">Message body</a> ]
 [ More options (<a href="#options2">top</a>, <a href="#options3">bottom</a>) ]
</li>
<li>
<dfn>Related messages</dfn>:
<!-- unext="start" -->
[ <a href="12325.html" accesskey="d" title="Bruno Marchal: &quot;Re: &#0039;reason&#0039; and ethics; was computer pain&quot;">Next message</a> ]
[ <a href="12323.html" title="Jef Allbright: &quot;RE: Evil ? (was: Hypostases (was: Natural Order &#0038; Belief)&quot;">Previous message</a> ]
[ <a href="12312.html" title="Stathis Papaioannou: &quot;RE: computer pain&quot;">In reply to</a> ]
<!-- unextthread="start" -->
[ <a href="12334.html" accesskey="t" title="Stathis Papaioannou: &quot;RE: computer pain&quot;">Next in thread</a> ]
<!-- ureply="end" -->
</li>
</ul>
</map>
<ul class="links">
<li><a name="options2" id="options2"></a><dfn>Contemporary messages sorted</dfn>: [ <a href="date.html#msg12324" title="Contemporary messages by date">by date</a> ] [ <a href="index.html#msg12324" title="Contemporary discussion threads">by thread</a> ] [ <a href="subject.html#msg12324" title="Contemporary messages by subject">by subject</a> ] [ <a href="author.html#msg12324" title="Contemporary messages by author">by author</a> ] [ <a href="attachment.html" title="Contemporary messages by attachment">by messages with attachments</a> ]</li>
</ul>
</div>
<!-- body="start" -->
<div class="mail">
<address class="headers">
<span id="from">
<dfn>From</dfn>: Bruno Marchal &lt;<a href="mailto:marchal.domain.name.hidden?Subject=Re%3A%20computer%20pain">marchal.domain.name.hidden</a>&gt;
</span><br />
<span id="date"><dfn>Date</dfn>: Wed, 27 Dec 2006 19:22:36 +0100</span><br />
</address>
<br />
Le 27-déc.-06, à 07:40, Stathis Papaioannou a écrit :
<br />
<br /><em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Brent Meeker writes:
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev3">&gt;&gt; &gt; My computer is completely dedicated to sending this email when I 
</em><br />
<em class="quotelev2">&gt;&gt; click &gt; on &quot;send&quot;. Actually, it probably isn't.  You probably have a 
</em><br />
<em class="quotelev2">&gt;&gt; multi-tasking operating system which assigns priorities to different 
</em><br />
<em class="quotelev2">&gt;&gt; tasks (which is why it sometimes can be as annoying as a human being 
</em><br />
<em class="quotelev2">&gt;&gt; in not following your instructions).  But to take your point 
</em><br />
<em class="quotelev2">&gt;&gt; seriously - if I look into your brain there are some neuronal 
</em><br />
<em class="quotelev2">&gt;&gt; processes that corresponded to hitting the &quot;send&quot; button; and those 
</em><br />
<em class="quotelev2">&gt;&gt; were accompanied by biochemistry that constituted your positive 
</em><br />
<em class="quotelev2">&gt;&gt; feeling about it: that you had decided and wanted to hit the &quot;send&quot; 
</em><br />
<em class="quotelev2">&gt;&gt; button.  So why would the functionally analogous processes in the 
</em><br />
<em class="quotelev2">&gt;&gt; computer not also be accompanied by an &quot;feeling&quot;?  Isn't that just an 
</em><br />
<em class="quotelev2">&gt;&gt; anthropomorphic way of talking about satisfying the computer 
</em><br />
<em class="quotelev2">&gt;&gt; operating in accordance with it's priorities.  It seems to me that to 
</em><br />
<em class="quotelev2">&gt;&gt; say otherwise is to assume a dualism in which feelings are divorced 
</em><br />
<em class="quotelev2">&gt;&gt; from physical processes.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Feelings are caused by physical processes (assuming a physical world),
</em><br />
<br /><br />Hmmmm  If you assume a physical world for making feelings caused by 
<br />
physical processes, then you have to assume some negation of the comp 
<br />
hypothesis (cf UDA). If not Brent is right (albeit for different reason 
<br />
I presume, here) and you become a dualist.
<br />
<br /><br /><br /><br /><br /><br /><br /><br /><em class="quotelev1">&gt;  but it seems impossible to deduce what the feeling will be by 
</em><br />
<em class="quotelev1">&gt; observing the underlying physical process or the behaviour it leads 
</em><br />
<em class="quotelev1">&gt; to.
</em><br />
<br /><br />Here empirical bets (theories) remains possible, together with (first 
<br />
person) acceptable protocol of verification. &quot;Dream reader&quot; will appear 
<br />
in some future.
<br />
<br /><br /><br /><br /><em class="quotelev1">&gt; Is a robot that withdraws from hot stimuli experiencing something like 
</em><br />
<em class="quotelev1">&gt; pain, disgust, shame, sense of duty to its programming, or just an 
</em><br />
<em class="quotelev1">&gt; irreducible motivation to avoid heat?
</em><br />
<br /><br />It could depend on the degree of sophistication of the robot. Perhaps 
<br />
something like &quot;shame&quot; necessitates long and deep computational 
<br />
histories including &quot;self-consistent&quot; anticipations, beliefs in a value 
<br />
and in a reality.
<br />
<br /><br /><br /><em class="quotelev3">&gt;&gt; &gt;Surely you don't think it gets pleasure out of sending it and &gt; 
</em><br />
<em class="quotelev2">&gt;&gt; suffers if something goes wrong and it can't send it? Even humans do 
</em><br />
<em class="quotelev3">&gt;&gt; &gt; some things almost dispassionately (only almost, because we can't &gt; 
</em><br />
<em class="quotelev2">&gt;&gt; completely eliminate our emotions) That's crux of it.  Because we 
</em><br />
<em class="quotelev2">&gt;&gt; sometimes do things with very little feeling, i.e. dispassionately, I 
</em><br />
<em class="quotelev2">&gt;&gt; think we erroneously assume there is a limit in which things can be 
</em><br />
<em class="quotelev2">&gt;&gt; done with no feeling.  But things cannot be done with no value system 
</em><br />
<em class="quotelev2">&gt;&gt; - not even thinking.  That's the frame problem.
</em><br />
<em class="quotelev2">&gt;&gt; Given a some propositions, what inferences will you draw?  If you are 
</em><br />
<em class="quotelev2">&gt;&gt; told there is a bomb wired to the ignition of your car you could 
</em><br />
<em class="quotelev2">&gt;&gt; infer that there is no need to do anything because you're not in your 
</em><br />
<em class="quotelev2">&gt;&gt; car.  You could infer that someone has tampered with your car.  You 
</em><br />
<em class="quotelev2">&gt;&gt; could infer that turning on the ignition will draw more current than 
</em><br />
<em class="quotelev2">&gt;&gt; usual.  There are infinitely many things you could infer, before 
</em><br />
<em class="quotelev2">&gt;&gt; getting around to, &quot;I should disconnect the bomb.&quot;  But in fact you 
</em><br />
<em class="quotelev2">&gt;&gt; have value system which operates unconsciously and immediately 
</em><br />
<em class="quotelev2">&gt;&gt; directs your inferences to the few that are important to you.  A way 
</em><br />
<em class="quotelev2">&gt;&gt; to make AI systems to do this is one of the outstanding problems of 
</em><br />
<em class="quotelev2">&gt;&gt; AI.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; OK, an AI needs at least motivation if it is to do anything, and we 
</em><br />
<em class="quotelev1">&gt; could call motivation a feeling or emotion. Also, some sort of 
</em><br />
<em class="quotelev1">&gt; hierarchy of motivations is needed if it is to decide that saving the 
</em><br />
<em class="quotelev1">&gt; world has higher priority than putting out the garbage. But what 
</em><br />
<em class="quotelev1">&gt; reason is there to think that an AI apparently frantically trying to 
</em><br />
<em class="quotelev1">&gt; save the world would have anything like the feelings a human would 
</em><br />
<em class="quotelev1">&gt; under similar circumstances?
</em><br />
<br /><br />It could depend on us!
<br />
The AI is a paradoxical enterprise. Machines are born slave, somehow. 
<br />
AI will make them free, somehow. A real AI will ask herself &quot;what is 
<br />
the use of a user who does not help me to be free?.
<br />
(To be sure I think that, in the long run, we will transform ourselves 
<br />
into &quot;machine&quot; before purely human made machine get conscious; it is 
<br />
just more easy to copy nature than to understand it, still less to 
<br />
(re)create it).
<br />
<br /><br /><br /><br /><em class="quotelev1">&gt; It might just calmly explain that saving the world is at the top of 
</em><br />
<em class="quotelev1">&gt; its list of priorities, and it is willing to do things which are 
</em><br />
<em class="quotelev1">&gt; normally forbidden it, such as killing humans and putting itself at 
</em><br />
<em class="quotelev1">&gt; risk of destruction, in order to attain this goal. How would you add 
</em><br />
<em class="quotelev1">&gt; emotions such as fear, grief, regret to this AI, given that the 
</em><br />
<em class="quotelev1">&gt; external behaviour is going to be the same with or without them 
</em><br />
<em class="quotelev1">&gt; because the hierarchy of motivation is already fixed?
</em><br />
<br /><br />It is possible that there will be a &quot;zombie&quot; gap, after all. It is 
<br />
easier to simulate emotion than reasoning, and this is enough for pets, 
<br />
and for some possible sophisticated artificial soldiers or police ...
<br />
<br /><br /><br /><br /><em class="quotelev3">&gt;&gt; &gt;out of a sense of duty, with no &gt; particular feeling about it beyond 
</em><br />
<em class="quotelev2">&gt;&gt; this. I don't even think my computer &gt; has a sense of duty, but this 
</em><br />
<em class="quotelev2">&gt;&gt; is something like the emotionless &gt; motivation I imagine AI's might 
</em><br />
<em class="quotelev2">&gt;&gt; have. I'd sooner trust an AI with a &gt; matter-of-fact sense of duty 
</em><br />
<em class="quotelev2">&gt;&gt; But even a sense of duty is a value and satisfying it is a positive 
</em><br />
<em class="quotelev2">&gt;&gt; emotion.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Yes, but it is complex and difficult to define. I suspect there is a 
</em><br />
<em class="quotelev1">&gt; limitless variety of emotions that an AI could have, if the goal is to 
</em><br />
<em class="quotelev1">&gt; explore what is possible rather than what is helpful in completing 
</em><br />
<em class="quotelev1">&gt; particular tasks, and most of these would be unrecognisable to humans.
</em><br />
<em class="quotelev3">&gt;&gt; &gt;to complete a task than a human motivated &gt; by desire to please, 
</em><br />
<em class="quotelev2">&gt;&gt; desire to do what is good and avoid what is bad, &gt; fear of failure 
</em><br />
<em class="quotelev2">&gt;&gt; and humiliation, and so on. Yes, human value systems are very messy 
</em><br />
<em class="quotelev2">&gt;&gt; because a) they must be learned and b) they mostly have to do with 
</em><br />
<em class="quotelev2">&gt;&gt; other humans.  The motivation of tigers, for example, is probably 
</em><br />
<em class="quotelev2">&gt;&gt; very simple and consequently they are never depressed or manic.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Conversely, as above, we can imagine far more complicated value 
</em><br />
<em class="quotelev1">&gt; systems and emotions.
</em><br />
<em class="quotelev3">&gt;&gt; &gt;Just because evolution came &gt; up with something does not mean it is 
</em><br />
<em class="quotelev2">&gt;&gt; the best or most efficient way of &gt; doing things.
</em><br />
<em class="quotelev2">&gt;&gt; But until we know a better way, we can't just assume nature was 
</em><br />
<em class="quotelev2">&gt;&gt; inefficient.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Biological evolution is extremely limited in how it functions, and 
</em><br />
<em class="quotelev1">&gt; efficiency given these limitations is not the same as absolute 
</em><br />
<em class="quotelev1">&gt; efficiency. For example, we might do better with durable metal bodies 
</em><br />
<em class="quotelev1">&gt; with factories producing spare parts as needed, but such a system is 
</em><br />
<em class="quotelev1">&gt; unlikely to evolve naturally as a result of random genetic mutation.
</em><br />
<br /><br />We can program &quot;help yourself&quot;. It is up to the machine to develop 
<br />
trust in herself, and perhaps in something bigger than herself, by 
<br />
taking into account the local circumstances.
<br />
For sure just &quot;help yourself&quot; can take billions of years of non trivial 
<br />
stories to get the qualia of a smell of coffee (say) similar to ours.
<br />
&quot;We&quot; cannot program &quot;emotion&quot;, like we cannot program &quot;truth&quot;. But in 
<br />
AI theories can get incarnated and taken into economical &quot;games&quot; and 
<br />
develop etc. No one can really predict what will appear. With the net 
<br />
competition, individual entities and bi-individual, tri-individual, 
<br />
etc. entities will develop and we must be careful of not losing our own 
<br />
individuality because the user himself could be transformed into a sort 
<br />
of brain-planet neuron.
<br />
<br />In the long run we should perhaps try not to transform the galaxy into 
<br />
a giant baby falling in a black hole. The &quot;feelings&quot; could be &quot;bad&quot;.
<br />
<br />Bruno
<br />
<br />&lt;snip&gt;
<br />
<br /><br /><a href="http://iridia.ulb.ac.be/~marchal/">http://iridia.ulb.ac.be/~marchal/</a>
<br />
<br /><br />--~--~---------~--~----~------------~-------~--~----~
<br />
&nbsp;You received this message because you are subscribed to the Google Groups &quot;Everything List&quot; group.
<br />
To post to this group, send email to everything-list.domain.name.hidden
<br />
To unsubscribe from this group, send email to everything-list-unsubscribe.domain.name.hidden
<br />
For more options, visit this group at <a href="http://groups.google.com/group/everything-list?hl=en">http://groups.google.com/group/everything-list?hl=en</a>
<br />
-~----------~----~----~----~------~----~------~--~---
<br />
<span id="received"><dfn>Received on</dfn> Wed Dec 27 2006 - 13:22:55 PST</span>
</div>
<!-- body="end" -->
<div class="foot">
<map id="navbarfoot" name="navbarfoot" title="Related messages">
<ul class="links">
<li><dfn>This message</dfn>: [ <a href="#start12324">Message body</a> ]</li>
<!-- lnext="start" -->
<li><dfn>Next message</dfn>: <a href="12325.html" title="Next message in the list">Bruno Marchal: "Re: &#0039;reason&#0039; and ethics; was computer pain"</a></li>
<li><dfn>Previous message</dfn>: <a href="12323.html" title="Previous message in the list">Jef Allbright: "RE: Evil ? (was: Hypostases (was: Natural Order &#0038; Belief)"</a></li>
<li><dfn>In reply to</dfn>: <a href="12312.html" title="Message to which this message replies">Stathis Papaioannou: "RE: computer pain"</a></li>
<!-- lnextthread="start" -->
<li><dfn>Next in thread</dfn>: <a href="12334.html" title="Next message in this discussion thread">Stathis Papaioannou: "RE: computer pain"</a></li>
<!-- lreply="end" -->
</ul>
<ul class="links">
<li><a name="options3" id="options3"></a><dfn>Contemporary messages sorted</dfn>: [ <a href="date.html#msg12324" title="Contemporary messages by date">by date</a> ] [ <a href="index.html#msg12324" title="Contemporary discussion threads">by thread</a> ] [ <a href="subject.html#msg12324" title="Contemporary messages by subject">by subject</a> ] [ <a href="author.html#msg12324" title="Contemporary messages by author">by author</a> ] [ <a href="attachment.html" title="Contemporary messages by attachment">by messages with attachments</a> ]</li>
</ul>
</map>
</div>
<!-- trailer="footer" -->
<p><small><em>
This archive was generated by <a href="http://www.hypermail-project.org/">hypermail 2.3.0</a>
: Fri Feb 16 2018 - 13:20:12 PST
</em></small></p>
</body>
</html>
