<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1" />
<meta name="generator" content="hypermail 2.3.0, see http://www.hypermail-project.org/" />
<title>Re: Dreaming On from Quentin Anciaux on 2009-09-03 (everything)</title>
<meta name="Author" content="Quentin Anciaux (allcolor.domain.name.hidden)" />
<meta name="Subject" content="Re: Dreaming On" />
<meta name="Date" content="2009-09-03" />
<style type="text/css">
/*<![CDATA[*/
/* To be incorporated in the main stylesheet, don't code it in hypermail! */
body {color: black; background: #ffffff}
dfn {font-weight: bold;}
pre { background-color:inherit;}
.head { border-bottom:1px solid black;}
.foot { border-top:1px solid black;}
th {font-style:italic;}
table { margin-left:2em;}map ul {list-style:none;}
#mid { font-size:0.9em;}
#received { float:right;}
address { font-style:inherit ;}
/*]]>*/
.quotelev1 {color : #990099}
.quotelev2 {color : #ff7700}
.quotelev3 {color : #007799}
.quotelev4 {color : #95c500}
.period {font-weight: bold}
</style>
</head>
<body>
<div class="head">
<h1>Re: Dreaming On</h1>
<!-- received="Thu Sep 03 10:41:56 2009" -->
<!-- isoreceived="20090903174156" -->
<!-- sent="Thu, 3 Sep 2009 10:41:56 +0200" -->
<!-- isosent="20090903084156" -->
<!-- name="Quentin Anciaux" -->
<!-- email="allcolor.domain.name.hidden" -->
<!-- subject="Re: Dreaming On" -->
<!-- id="57da2dd30909030141u4d124304sdea46ffafa4383a5.domain.name.hidden" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="36abde5a-654d-4e63-8b16-f3fa86d63a39.domain.name.hidden" -->
<!-- expires="-1" -->
<map id="navbar" name="navbar">
<ul class="links">
<li>
<dfn>This message</dfn>:
[ <a href="#start17735" name="options1" id="options1" tabindex="1">Message body</a> ]
 [ More options (<a href="#options2">top</a>, <a href="#options3">bottom</a>) ]
</li>
<li>
<dfn>Related messages</dfn>:
<!-- unext="start" -->
[ <a href="17736.html" accesskey="d" title="Flammarion: &quot;Re: Dreaming On&quot;">Next message</a> ]
[ <a href="17734.html" title="Flammarion: &quot;Re: Dreaming On&quot;">Previous message</a> ]
[ <a href="17734.html" title="Flammarion: &quot;Re: Dreaming On&quot;">In reply to</a> ]
<!-- unextthread="start" -->
[ <a href="17736.html" accesskey="t" title="Flammarion: &quot;Re: Dreaming On&quot;">Next in thread</a> ]
 [ <a href="#replies">Replies</a> ]
<!-- ureply="end" -->
</li>
</ul>
</map>
<ul class="links">
<li><a name="options2" id="options2"></a><dfn>Contemporary messages sorted</dfn>: [ <a href="date.html#msg17735" title="Contemporary messages by date">by date</a> ] [ <a href="index.html#msg17735" title="Contemporary discussion threads">by thread</a> ] [ <a href="subject.html#msg17735" title="Contemporary messages by subject">by subject</a> ] [ <a href="author.html#msg17735" title="Contemporary messages by author">by author</a> ] [ <a href="attachment.html" title="Contemporary messages by attachment">by messages with attachments</a> ]</li>
</ul>
</div>
<!-- body="start" -->
<div class="mail">
<address class="headers">
<span id="from">
<dfn>From</dfn>: Quentin Anciaux &lt;<a href="mailto:allcolor.domain.name.hidden?Subject=Re%3A%20Dreaming%20On">allcolor.domain.name.hidden</a>&gt;
</span><br />
<span id="date"><dfn>Date</dfn>: Thu, 3 Sep 2009 10:41:56 +0200</span><br />
</address>
<br />
2009/9/3 Flammarion &lt;peterdjones.domain.name.hidden&gt;:
<br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; On 3 Sep, 01:26, David Nyman &lt;david.ny....domain.name.hidden&gt; wrote:
</em><br />
<em class="quotelev2">&gt;&gt; 2009/9/2 Flammarion &lt;peterdjo....domain.name.hidden&gt;:
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev4">&gt;&gt; &gt;&gt; and is thus not any particular physical
</em><br />
<em class="quotelev4">&gt;&gt; &gt;&gt; object.  A specific physical implementation is a token of that
</em><br />
<em class="quotelev4">&gt;&gt; &gt;&gt; computational type, and is indeed a physical object, albeit one whose
</em><br />
<em class="quotelev4">&gt;&gt; &gt;&gt; physical details can be of any variety so long as they continue to
</em><br />
<em class="quotelev4">&gt;&gt; &gt;&gt; instantiate the relevant computational invariance.  Hence it is hard
</em><br />
<em class="quotelev4">&gt;&gt; &gt;&gt; to see how a specific (invariant) example of an experiential state
</em><br />
<em class="quotelev4">&gt;&gt; &gt;&gt; could be justified as being token-identical with all the different
</em><br />
<em class="quotelev4">&gt;&gt; &gt;&gt; physical implementations of a computation.
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev3">&gt;&gt; &gt; I was right.
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev3">&gt;&gt; &gt; A mental type can be associated with a computational
</em><br />
<em class="quotelev3">&gt;&gt; &gt; type.
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev3">&gt;&gt; &gt; Any token of a mental type can be associated with a token
</em><br />
<em class="quotelev3">&gt;&gt; &gt; of the corresponding computational type.
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt; But what difference is that supposed to make?  The type association is
</em><br />
<em class="quotelev2">&gt;&gt; implicit in what I was saying.  All you've said above is that it makes
</em><br />
<em class="quotelev2">&gt;&gt; no difference whether one talks in terms of the mental type or the
</em><br />
<em class="quotelev2">&gt;&gt; associated computational type because their equivalence is a posit of
</em><br />
<em class="quotelev2">&gt;&gt; CTM.  And whether it is plausible that the physical tokens so picked
</em><br />
<em class="quotelev2">&gt;&gt; out possess the causal efficacy presupposed by CTM is precisely what I
</em><br />
<em class="quotelev2">&gt;&gt; was questioning.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; question it then. what's the problem?
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev4">&gt;&gt; &gt;&gt; But even on this basis it still doesn't seem possible to establish any
</em><br />
<em class="quotelev4">&gt;&gt; &gt;&gt; consistent identity between the physical variety of the tokens thus
</em><br />
<em class="quotelev4">&gt;&gt; &gt;&gt; distinguished and a putatively unique experiential state.
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev3">&gt;&gt; &gt; The variety of the physical implementations is reduced by grouping
</em><br />
<em class="quotelev3">&gt;&gt; &gt; them
</em><br />
<em class="quotelev3">&gt;&gt; &gt; as  equivalent computational types. Computation is abstract.
</em><br />
<em class="quotelev3">&gt;&gt; &gt; Abstraction is
</em><br />
<em class="quotelev3">&gt;&gt; &gt; ignoring irrelevant details. Ignoring irrelevant details establishes a
</em><br />
<em class="quotelev3">&gt;&gt; &gt; many-to-one relationship : many possible implementations of one mental
</em><br />
<em class="quotelev3">&gt;&gt; &gt; state.
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt; Again, that's not an argument - you're just reciting the *assumptions*
</em><br />
<em class="quotelev2">&gt;&gt; of CTM, not arguing for their plausibility.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; you're not arguing against its plausibility
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev2">&gt;&gt; The justification of the
</em><br />
<em class="quotelev2">&gt;&gt; supposed irrelevance of particular physical details is that they are
</em><br />
<em class="quotelev2">&gt;&gt; required to be ignored for the supposed efficacy of the type-token
</em><br />
<em class="quotelev2">&gt;&gt; relation to be plausible.  That doesn't make it so.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; why not? we already know they can be ignored to establish
</em><br />
<em class="quotelev1">&gt; computational
</em><br />
<em class="quotelev1">&gt; equivalence.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev4">&gt;&gt; &gt;&gt;  On the
</em><br />
<em class="quotelev4">&gt;&gt; &gt;&gt; contrary, any unbiased a priori prediction would be of experiential
</em><br />
<em class="quotelev4">&gt;&gt; &gt;&gt; variance on the basis of physical variance.
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev3">&gt;&gt; &gt; Yes. The substance of the CTM claim is that physical
</em><br />
<em class="quotelev3">&gt;&gt; &gt; differences do not make  a mental difference unless they
</em><br />
<em class="quotelev3">&gt;&gt; &gt; make a computational difference. That is to say, switching from
</em><br />
<em class="quotelev3">&gt;&gt; &gt; one token of a type of computation to another cannot make
</em><br />
<em class="quotelev3">&gt;&gt; &gt; a difference in mentation. That is not to be expected on an
</em><br />
<em class="quotelev3">&gt;&gt; &gt; &quot;unbiased&quot; basis, just because it is a substantive claim.
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt; Yes it's precisely the claim whose plausibility I've been questioning.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; You haven't said anything specific about what is wrong with it at all.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev3">&gt;&gt; &gt; The variety of the physical implementations is reduced by grouping
</em><br />
<em class="quotelev3">&gt;&gt; &gt; them
</em><br />
<em class="quotelev3">&gt;&gt; &gt; as  equivalent computational types. Computation is abstract.
</em><br />
<em class="quotelev3">&gt;&gt; &gt; Abstraction is
</em><br />
<em class="quotelev3">&gt;&gt; &gt; ignoring irrelevant details. Ignoring irrelevant details establishes a
</em><br />
<em class="quotelev3">&gt;&gt; &gt; many-to-one relationship : many possible implementations of one mental
</em><br />
<em class="quotelev3">&gt;&gt; &gt; state.
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt; Yes thanks, this is indeed the hypothesis.  But simply recapitulating
</em><br />
<em class="quotelev2">&gt;&gt; the assumptions isn't exactly an uncommitted assessment of their
</em><br />
<em class="quotelev2">&gt;&gt; plausibility is it?
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Saying it is not necessarily correct is not a critique
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev2">&gt;&gt;That can only immunise it from criticism.  There
</em><br />
<em class="quotelev2">&gt;&gt; is no whiff in CTM of why it should be considered plausible on
</em><br />
<em class="quotelev2">&gt;&gt; physical grounds alone.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev2">&gt;&gt; Hence counter arguments can legitimately
</em><br />
<em class="quotelev2">&gt;&gt; question the consistency of its claims as a physical theory in the
</em><br />
<em class="quotelev2">&gt;&gt; absence of its type-token presuppositions.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;  If you mean you can criticise the CTM as offering nothing specific
</em><br />
<em class="quotelev1">&gt; to resolve the HP, you are correct. But I *thought* we were
</em><br />
<em class="quotelev1">&gt; discussing the MG/Olympia style of argument, which purportedly
</em><br />
<em class="quotelev1">&gt; still applies even if you restrict yourself to cognition and forget
</em><br />
<em class="quotelev1">&gt; about experience/qualia.
</em><br />
<em class="quotelev1">&gt; Are we?
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev2">&gt;&gt; Look, let me turn this round.  You've said before that you're not a
</em><br />
<em class="quotelev2">&gt;&gt; diehard partisan of CTM.  What in your view would be persuasive
</em><br />
<em class="quotelev2">&gt;&gt; grounds for doubting it?
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; I'll explain below. But the claim I am interested in is that CTM
</em><br />
<em class="quotelev1">&gt; somehow disproves materalism (Maudlin, BTW takes it the other way
</em><br />
<em class="quotelev1">&gt; around--
</em><br />
<em class="quotelev1">&gt; materialism disproves CTM). I have heard not a word in support of
</em><br />
<em class="quotelev1">&gt; *that* claim.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; ust an Artificial Intellence be a Computer ?
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; An AI is not necessarily a computer. Not everything is a computer or
</em><br />
<em class="quotelev1">&gt; computer-emulable. It just needs to be artificial and intelligent!
</em><br />
<br />Then it's no more *CTM*. (C means Computational)
<br />
<br /><em class="quotelev1">&gt; The
</em><br />
<em class="quotelev1">&gt; extra ingredient a conscious system has need not be anything other
</em><br />
<em class="quotelev1">&gt; than the physics (chemistry, biology) of its hardware -- there is no
</em><br />
<em class="quotelev1">&gt; forced choice between ghosts and machines.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; A physical system can never be exactly emulated with different
</em><br />
<em class="quotelev1">&gt; hardware -- the difference has to show up somewhere. It can be hidden
</em><br />
<em class="quotelev1">&gt; by only dealing with a subset of a systems abilities relevant to the
</em><br />
<em class="quotelev1">&gt; job in hand; a brass key can open a door as well as an iron key, but
</em><br />
<em class="quotelev1">&gt; brass cannot be substituted for iron where magnetism is relevant.
</em><br />
<em class="quotelev1">&gt; Physical differences can also be evaded by taking an abstract view of
</em><br />
<em class="quotelev1">&gt; their functioning; two digital circuits might be considered equivalent
</em><br />
<em class="quotelev1">&gt; at the &quot;ones and zeros&quot; level of description even though they
</em><br />
<em class="quotelev1">&gt; physically work at different voltages.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Thus computer-emulability is not a property of physical systems as
</em><br />
<em class="quotelev1">&gt; such. Even if all physical laws are computable, that does not mean
</em><br />
<em class="quotelev1">&gt; that any physical systems can be fully simulated. The reason is that
</em><br />
<em class="quotelev1">&gt; the level of simulation matters. A simulated plane does not actually
</em><br />
<em class="quotelev1">&gt; fly; a simulated game of chess really is chess. There seems to be a
</em><br />
<em class="quotelev1">&gt; distinction between things like chess, which can survive being
</em><br />
<em class="quotelev1">&gt; simulated at a higher level of abstraction, and planes, which can't.
</em><br />
<em class="quotelev1">&gt; Moreover, it seems that chess-like things are in minority, and that
</em><br />
<em class="quotelev1">&gt; they can be turned into an abstract programme and adequately simulated
</em><br />
<em class="quotelev1">&gt; because they are already abstract.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Consciousness. might depend on specific properties of hardware, of
</em><br />
<em class="quotelev1">&gt; matter.
</em><br />
<br />Then it is no more CTM. Specific hardware properties are *totally irrelevant*.
<br />
<br /><em class="quotelev1">&gt; This does not imply parochialism, the attitude that denies
</em><br />
<em class="quotelev1">&gt; consciousness to poor Mr Data just because he is made out of silicon,
</em><br />
<em class="quotelev1">&gt; not protoplasm. We know our own brains are conscious; most of us
</em><br />
<em class="quotelev1">&gt; intuit that rocks and dumb Chinese Rooms are not; all other cases are
</em><br />
<em class="quotelev1">&gt; debatable.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Of course all current research in AI is based on computation in one
</em><br />
<em class="quotelev1">&gt; way or another. If the Searlian idea that consciousness is rooted in
</em><br />
<em class="quotelev1">&gt; physics, strongly emergent, and non-computable is correct, then
</em><br />
<em class="quotelev1">&gt; current AI can only achieve consciousness accidentally. A Searlian
</em><br />
<em class="quotelev1">&gt; research project would understand how brains generate consciousness in
</em><br />
<em class="quotelev1">&gt; the first place -- the aptly-named Hard Problem -- before moving onto
</em><br />
<em class="quotelev1">&gt; possible artificial reproductions, which would have to have the right
</em><br />
<em class="quotelev1">&gt; kind of physics and internal causal activity -- although not
</em><br />
<em class="quotelev1">&gt; necessarily the same kind as humans.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;    &quot;When I say that the brain is a biological organ and consciousness
</em><br />
<em class="quotelev1">&gt; a biological process, I do not, of course, say or imply that it would
</em><br />
<em class="quotelev1">&gt; be impossible to produce an artificial brain out of nonbiological
</em><br />
<em class="quotelev1">&gt; materials that could also cause and sustain consciousness...There is
</em><br />
<em class="quotelev1">&gt; no reason, in principle, why we could not similarly make an artificial
</em><br />
<em class="quotelev1">&gt; brain that causes consciousness. The point that needs to be empnasized
</em><br />
<em class="quotelev1">&gt; is that any such artificial brain would have to duplicate the actual
</em><br />
<em class="quotelev1">&gt; causes of human and animal brains to produce inner, qualitative,
</em><br />
<em class="quotelev1">&gt; subjective states of consciousness. Just producing similar output
</em><br />
<em class="quotelev1">&gt; behavior would not by itself be enough.&quot;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; [Searle, MLS, p. 53]
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; &quot;Is the Brain A Machine?&quot;
</em><br />
<em class="quotelev1">&gt; John Searle thinks so .
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;    The brain is indeed a machine, an organic machine; and its
</em><br />
<em class="quotelev1">&gt; processes, such as neuron firings, are organic machine processes.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; The Mystery of Consciousness. page 17: Is he right ? To give a
</em><br />
<em class="quotelev1">&gt; typically philosophical answer, that depends on what you mean by
</em><br />
<em class="quotelev1">&gt; 'machine'. If 'machine' means an artificial construct, then the answer
</em><br />
<em class="quotelev1">&gt; is obviously 'no'. However. Searle also thinks the the body is a
</em><br />
<em class="quotelev1">&gt; machine, by which he seems to mean that it has been understand in
</em><br />
<em class="quotelev1">&gt; scientific terms, we can explain biology by in terms of to chemistry
</em><br />
<em class="quotelev1">&gt; and chemistry in terms of physics. Is the brain a machine by this
</em><br />
<em class="quotelev1">&gt; definition ? It is being granted that the job of he brain is to
</em><br />
<em class="quotelev1">&gt; implement a conscious mind, just as the job of the stomach is to
</em><br />
<em class="quotelev1">&gt; digest, the problem then is that although our 'mechanical'
</em><br />
<em class="quotelev1">&gt; understanding of the stomach does allow us to understand digestion we
</em><br />
<em class="quotelev1">&gt; do not, according to Searle himself, understand how the brain produces
</em><br />
<em class="quotelev1">&gt; consciousness. He does think that the problem of consciousness is
</em><br />
<em class="quotelev1">&gt; scientifically explicable, so yet another definition of 'machine' is
</em><br />
<em class="quotelev1">&gt; needed, namely 'scientifically explained or scientifically explicable'
</em><br />
<em class="quotelev1">&gt; -- with the brain being explicable rather than explained. The problem
</em><br />
<em class="quotelev1">&gt; with this stretch-to-fit approach to the meaning of the word 'machine'
</em><br />
<em class="quotelev1">&gt; is that every time the definition of brain is broadened, the claim is
</em><br />
<em class="quotelev1">&gt; weakened, made less impactful.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; PDJ 03/02/03
</em><br />
<em class="quotelev1">&gt; The Chinese Room
</em><br />
<em class="quotelev1">&gt; The Chinese Room and Consciousness
</em><br />
<em class="quotelev1">&gt; According to the proponents of Artificial Intelligence, a system is
</em><br />
<em class="quotelev1">&gt; intelligent if it can convince a human interlocutor that it is. This
</em><br />
<em class="quotelev1">&gt; is the famous Turing Test. It focuses on external behaviour and is
</em><br />
<em class="quotelev1">&gt; mute about how that behaviour is produced. A rival idea is that of the
</em><br />
<em class="quotelev1">&gt; Chinese Room, due to John Searle. Searle places himself in the room,
</em><br />
<em class="quotelev1">&gt; manually executing a computer algorithm that implements intelligent-
</em><br />
<em class="quotelev1">&gt; seeming behaviour, in this case getting questions written in Chinese
</em><br />
<em class="quotelev1">&gt; and mechanically producing answers, without himself understanding
</em><br />
<em class="quotelev1">&gt; Chinese. He thereby focuses attention on how the supposedly
</em><br />
<em class="quotelev1">&gt; intelligent behaviour is produced. Although Searle's original idea was
</em><br />
<em class="quotelev1">&gt; aimed at semantics, my variation is going to focus on consciousness.
</em><br />
<em class="quotelev1">&gt; Likewise, although Searle's original specification has him
</em><br />
<em class="quotelev1">&gt; implementing complex rules, I am going to take it that the Chinese
</em><br />
<em class="quotelev1">&gt; Room is implemented as a conceptually simple system --for instance, a
</em><br />
<em class="quotelev1">&gt; Giant Look-Up Table -- in line with the theorem of Computer Science
</em><br />
<em class="quotelev1">&gt; which has it that any computer can be emulated by a Turing Machine.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; If you think a Chinese Room implemented with a simplistic, &quot;dumb&quot;
</em><br />
<em class="quotelev1">&gt; algorithm can still be conscious, you are probably a behaviourist; you
</em><br />
<em class="quotelev1">&gt; only care about that external stimuli get translated into the
</em><br />
<em class="quotelev1">&gt; appropriate responses, not how this happens, let alone what it feels
</em><br />
<em class="quotelev1">&gt; like to the system in question.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; If you think this dumb Chinese Room is not conscious, but a smart one
</em><br />
<em class="quotelev1">&gt; would be, you need to explain why. There are two explanatory routes:
</em><br />
<em class="quotelev1">&gt; one that says consciousness is inessential, and another that says that
</em><br />
<em class="quotelev1">&gt; hardware counts as well as software.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Any smart AI can be implemented as a dumb TM, so the more complex
</em><br />
<em class="quotelev1">&gt; inner workings which supposedly implement consciousness , could be
</em><br />
<em class="quotelev1">&gt; added or subtracted without making any detectable difference. Given
</em><br />
<em class="quotelev1">&gt; the assumption that the computational differences are what matter,
</em><br />
<em class="quotelev1">&gt; this would to add up to epiphenomenalism, the view that consciousness
</em><br />
<em class="quotelev1">&gt; exists but is a bystander that doesn't cause anything, since there is
</em><br />
<em class="quotelev1">&gt; not any computational difference between the simple implementation and
</em><br />
<em class="quotelev1">&gt; the complex one.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; On the other hand, if it is assumed that epiphenomenalism if false,
</em><br />
<em class="quotelev1">&gt; then it follows that implementational differences must matter, since
</em><br />
<em class="quotelev1">&gt; the difference between the complex and the dumb systems is not in
</em><br />
<em class="quotelev1">&gt; their computational properties. That in turn means computationalism is
</em><br />
<em class="quotelev1">&gt; false. The Chinese Room argument then succeeds, but only as
</em><br />
<em class="quotelev1">&gt; interpreted fairly strictly as an argument about the ability of
</em><br />
<em class="quotelev1">&gt; algorithms to implement consciousness. Any actual computational
</em><br />
<em class="quotelev1">&gt; systems, or artificial intelligence construct, will be more than just
</em><br />
<em class="quotelev1">&gt; an algorithm; it will be the concrete implementation of an algorithm.
</em><br />
<em class="quotelev1">&gt; Since it is the implementation that makes the difference between a
</em><br />
<em class="quotelev1">&gt; fully successful AI and a &quot;zombie&quot; (functional enough to pass a Turing
</em><br />
<em class="quotelev1">&gt; test, but lacking real consciousness), and since every AI would have
</em><br />
<em class="quotelev1">&gt; some sort of implementation, the possibility of an actual systems
</em><br />
<em class="quotelev1">&gt; being conscious is far from ruled out. The CR argument only shows that
</em><br />
<em class="quotelev1">&gt; it is not conscious purely by virtue of implementing an algorithm. It
</em><br />
<em class="quotelev1">&gt; is a succesful argument up to that point, the point that why AI may be
</em><br />
<em class="quotelev1">&gt; possible, it will not be pruely due to running the right algorithm.
</em><br />
<em class="quotelev1">&gt; While the success of an AI programme is not ruled out, it is not
</em><br />
<em class="quotelev1">&gt; guaranteed either. It is not clear which implementations are the right
</em><br />
<em class="quotelev1">&gt; ones. A system running the right algorithm on the wrong hardware may
</em><br />
<em class="quotelev1">&gt; well be able to pass a Turing Test, but if the hardware is relevant to
</em><br />
<em class="quotelev1">&gt; consciousness as well, a system with the wrong hardware will be an
</em><br />
<em class="quotelev1">&gt; artificial zombie. It will be cognitively competent, but lacking in
</em><br />
<em class="quotelev1">&gt; genuine phenomenal consciousness. (This is in line with the way robots
</em><br />
<em class="quotelev1">&gt; and the like are often portrayed in science fiction. A further wrinkle
</em><br />
<em class="quotelev1">&gt; is that an exact computational emulation of a real person -- a real
</em><br />
<em class="quotelev1">&gt; person who believes in qualia anyway -- would assert its possession of
</em><br />
<em class="quotelev1">&gt; qualia while quite possibly not possessing any qualia to boast about).
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Thus the success of the CR argument against a software-only approach
</em><br />
<em class="quotelev1">&gt; to AI has the implication that the TT is not adequate to detect the
</em><br />
<em class="quotelev1">&gt; success of a strong AI (artificial consciousness) project. (Of course,
</em><br />
<em class="quotelev1">&gt; all this rests on beahviourism being false; if behaviourism is true
</em><br />
<em class="quotelev1">&gt; there is no problem with a TT, since it is a test of behaviour). We
</em><br />
<em class="quotelev1">&gt; need to peek inside the box; in order to know whether an AI device has
</em><br />
<em class="quotelev1">&gt; full phenomenal, consciousness, we would need a successful theory
</em><br />
<em class="quotelev1">&gt; linking consciousness to physics. Such a theory would be nothing less
</em><br />
<em class="quotelev1">&gt; than an answer to the Hard Problem. So a further implication of the
</em><br />
<em class="quotelev1">&gt; partial success of Searlian arguments is that we cannot bypass the
</em><br />
<em class="quotelev1">&gt; problem of explaining consciousness by some research programme of
</em><br />
<em class="quotelev1">&gt; building AIs. The HP is logically prior. Except for beahviourists.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Peter D Jones 8/6/05
</em><br />
<em class="quotelev1">&gt; Syntax and Semantics. The Circularity Argument as an Alternative
</em><br />
<em class="quotelev1">&gt; Chinese Room
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; The CR concludes that syntax, an abstract set of rules is insufficient
</em><br />
<em class="quotelev1">&gt; for semantics. This conclusions is also needed as a premise for
</em><br />
<em class="quotelev1">&gt; Searle's syllogistic argument
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;   1. Syntax is not sufficient for semantics.
</em><br />
<em class="quotelev1">&gt;   2. Computer programs are entirely defined by their formal, or
</em><br />
<em class="quotelev1">&gt; syntactical, structure.
</em><br />
<em class="quotelev1">&gt;   3. Minds have mental contents; specifically, they have semantic
</em><br />
<em class="quotelev1">&gt; contents.
</em><br />
<em class="quotelev1">&gt;   4. Therefore, No computer program by itself is sufficient to give a
</em><br />
<em class="quotelev1">&gt; system a mind. Programs, in short, are not minds, and they are not by
</em><br />
<em class="quotelev1">&gt; themselves sufficient for having minds.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Premise 01 is the most contentious of the four. The Chinese Room
</em><br />
<em class="quotelev1">&gt; Argument, which Searle puts forward to support itm is highly
</em><br />
<em class="quotelev1">&gt; contentious. We will put forward a different argument to support it.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; An objection to the CR argument goes: &quot;But there must be some kind of
</em><br />
<em class="quotelev1">&gt; information processing structure that implements meaning in our heads.
</em><br />
<em class="quotelev1">&gt; Surely that could be turned into rules for the operator of the Chinese
</em><br />
<em class="quotelev1">&gt; Room&quot;.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; A response, the Circularity Argument goes: a system of syntactic
</em><br />
<em class="quotelev1">&gt; process can only transform one symbol-string into another; it does not
</em><br />
<em class="quotelev1">&gt; have the power to relate the symbols to anything outside the system.
</em><br />
<em class="quotelev1">&gt; It is a circualr, closed system. However, to be meaningful a symbol
</em><br />
<em class="quotelev1">&gt; must stand for something other than itself. (The Symbol must be
</em><br />
<em class="quotelev1">&gt; Grounded). Therefore it must fail to have any real semantics.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; It is plausible that any given term can be given an abstract
</em><br />
<em class="quotelev1">&gt; definition that doesn't depend on direct experience. A dictionary is
</em><br />
<em class="quotelev1">&gt; collection of such definitions. It is much less plausible that every
</em><br />
<em class="quotelev1">&gt; term can be defined that way. Such a system would be circular in the
</em><br />
<em class="quotelev1">&gt; same way as:
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; &quot;present: gift&quot;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; &quot;gift: present&quot;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; ...but on a larger scale.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; A dictionary relates words to each other on a static way. It does not
</em><br />
<em class="quotelev1">&gt; directly have the power to relate words to anything outside itseld. We
</em><br />
<em class="quotelev1">&gt; can understand dictionary definitons because we have already grapsed
</em><br />
<em class="quotelev1">&gt; the meanings of some words. A better analogy for the Symbol Grounding
</em><br />
<em class="quotelev1">&gt; problem is that of trying to learn an entirely unknown langauge for a
</em><br />
<em class="quotelev1">&gt; dictionary. (I have switched from talking about syntacital
</em><br />
<em class="quotelev1">&gt; manipluation processes to static dicitonaries; Searles arguments that
</em><br />
<em class="quotelev1">&gt; syntax cannot lead to semantics have been critices for dealing with
</em><br />
<em class="quotelev1">&gt; &quot;syntax&quot; considered as abstract rules, whereas the computational
</em><br />
<em class="quotelev1">&gt; processes they are aimend are concrete, physcial and dynamic. The
</em><br />
<em class="quotelev1">&gt; Circularity argument does not have that problem. Both abstract syntax
</em><br />
<em class="quotelev1">&gt; and symbol-manipulation processed can be considered as circular).
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; If the Circularity Argument, is correct, the practice of giving
</em><br />
<em class="quotelev1">&gt; abstract definitions, like &quot;equine quadruped&quot; only works because
</em><br />
<em class="quotelev1">&gt; somewhere in the chain of definitions are words that have been defined
</em><br />
<em class="quotelev1">&gt; directly; direct reference has been merely deferred, not avoided
</em><br />
<em class="quotelev1">&gt; altogether.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; The objection continues: &quot;But the information processing structure in
</em><br />
<em class="quotelev1">&gt; our heads has a concrete connection to the real world: so do AI's
</em><br />
<em class="quotelev1">&gt; (although those of the Chinese Room are minimal). Call this is the
</em><br />
<em class="quotelev1">&gt; Portability Assumption.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; But they are not the same concrete connections. The portability of
</em><br />
<em class="quotelev1">&gt; abstract rules is guaranteed by the fact that they are abstract. But
</em><br />
<em class="quotelev1">&gt; concrete causal connections are not-abstract. They are unlikely to be
</em><br />
<em class="quotelev1">&gt; portable -- how can you explain colour to an alien whose senses do not
</em><br />
<em class="quotelev1">&gt; include anything like vision?
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Copying the syntactic rules from one hardware platform to another will
</em><br />
<em class="quotelev1">&gt; not copy the semantics. Therefore,semantics is more than syntax.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; If the Portability Assumption is correct, an AI (particularly a
</em><br />
<em class="quotelev1">&gt; robotic one) could be expected to have some semantics, but there is no
</em><br />
<em class="quotelev1">&gt; reason it should have human semantics. As Wittgenstein said: &quot;if a
</em><br />
<em class="quotelev1">&gt; lion could talk, we could not understand it&quot;.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Peter D Jones 13/11/05
</em><br />
<em class="quotelev1">&gt; The Chinese Room and Computability
</em><br />
<em class="quotelev1">&gt; I casually remarked that mental behaviour 'may not be computable'.
</em><br />
<em class="quotelev1">&gt; This will shock some AI proponents, for whom the Church-Turing thesis
</em><br />
<em class="quotelev1">&gt; proves that everything is computable. More precisely, everything that
</em><br />
<em class="quotelev1">&gt; is mathematically computable is computable by a relatively dumb
</em><br />
<em class="quotelev1">&gt; computer, a Turing that something can be simulated doesn't mean the
</em><br />
<em class="quotelev1">&gt; simulation has all the relevant properties of the original: flight
</em><br />
<em class="quotelev1">&gt; simulators don't take off. Thirdly the mathematical sense of
</em><br />
<em class="quotelev1">&gt; 'computable' doesn't fit well with the idea of computer-simulating
</em><br />
<em class="quotelev1">&gt; fundamental physics. A real number is said to be mathematically
</em><br />
<em class="quotelev1">&gt; computable if the algorithm that churns it out keeps on churning out
</em><br />
<em class="quotelev1">&gt; extra digits of accuracy..indefinitely. Since such a algorithm will
</em><br />
<em class="quotelev1">&gt; never finish churning out a single real number physical value, it is
</em><br />
<em class="quotelev1">&gt; difficult to see how it could simulate an entire universe. Yes, I am
</em><br />
<em class="quotelev1">&gt; assuming the universe is fundamentally made of real numbers. If it is,
</em><br />
<em class="quotelev1">&gt; for instance finite, fundamental physics might be more readily
</em><br />
<em class="quotelev1">&gt; computable, but the computability of physics still depends very much
</em><br />
<em class="quotelev1">&gt; on physics and not just on computer science).
</em><br />
<em class="quotelev1">&gt; The Systems Response and Emergence
</em><br />
<em class="quotelev1">&gt; By far the most common response to the CR argument is that, while the
</em><br />
<em class="quotelev1">&gt; room's operator, Searle himself, does not understand Chinese, the room
</em><br />
<em class="quotelev1">&gt; as a whole does. According to one form of the objection, individual
</em><br />
<em class="quotelev1">&gt; neurons do not understand Chinese either; but this is not a fair
</em><br />
<em class="quotelev1">&gt; comparison. If you were to take a very simple brain and gradually add
</em><br />
<em class="quotelev1">&gt; more neurons to it, the increase in information-processing capacity
</em><br />
<em class="quotelev1">&gt; would keep in line with an increase in causal activity. However, the
</em><br />
<em class="quotelev1">&gt; equivalent procedure of gradually beefing up a CR would bascially
</em><br />
<em class="quotelev1">&gt; consist of adding more and more rules to the rule book while the
</em><br />
<em class="quotelev1">&gt; single &quot;neuron&quot;, the single causally active constituent, the operator
</em><br />
<em class="quotelev1">&gt; of the room did all the work. It is hard to attribute understanding to
</em><br />
<em class="quotelev1">&gt; a passive rulebook, and hard to attribute it to an operator performing
</em><br />
<em class="quotelev1">&gt; simple rote actions. It is also hard to see how the whole can be more
</em><br />
<em class="quotelev1">&gt; than the sum of the parts. It is very much a characteristic of a
</em><br />
<em class="quotelev1">&gt; computer, or other mechanism, that there is no mysterious emegence
</em><br />
<em class="quotelev1">&gt; going on; the behavour of the whole is always explicable in term sof
</em><br />
<em class="quotelev1">&gt; the behaviour of the parts. There is no mystery, by contrast, in more
</em><br />
<em class="quotelev1">&gt; neurons being able to do more work. Searle doesn't think you can put
</em><br />
<em class="quotelev1">&gt; two dumbs together and get a smart. That is no barrier to putting 100
</em><br />
<em class="quotelev1">&gt; billion dumbs together to get a smart. Or to putting two almost-smarts
</em><br />
<em class="quotelev1">&gt; together to get a smart.
</em><br />
<em class="quotelev1">&gt; The Chinese Room and Speed
</em><br />
<em class="quotelev1">&gt; Of course, if we burden the room's operator with more and more rules,
</em><br />
<em class="quotelev1">&gt; he will go slower and slower. Dennett thinks a slow chinese room would
</em><br />
<em class="quotelev1">&gt; not count as conscious at all. Nature, he notes, requires conscious
</em><br />
<em class="quotelev1">&gt; beings to react within a certain timescale in order to survive. That
</em><br />
<em class="quotelev1">&gt; is true, but it does not suggest any absolute speed requirement.
</em><br />
<em class="quotelev1">&gt; Nature accomodates the tortoise and the mayfly alike. The idea that a
</em><br />
<em class="quotelev1">&gt; uselessly slow consciousness would not be actually be a concsiousness
</em><br />
<em class="quotelev1">&gt; at all is also rather idiosyncratic. We generally credit a useless
</em><br />
<em class="quotelev1">&gt; vestigal limb with being a loimb, at least.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Anyway, Dennett's speed objection is designed to lead into one of his
</em><br />
<em class="quotelev1">&gt; favourite ideas: the need for massive parallelism. One Searle might
</em><br />
<em class="quotelev1">&gt; lack conscious semantics, but a million might do the trick. Or so he
</em><br />
<em class="quotelev1">&gt; says. But what would parallelism bring us except speed?
</em><br />
<em class="quotelev1">&gt; The Chinese Room and complexity.
</em><br />
<em class="quotelev1">&gt; The Dennettians make two claims; that zombies are impossible, and that
</em><br />
<em class="quotelev1">&gt; the problem with the Chinese room is that it is too simple. We will
</em><br />
<em class="quotelev1">&gt; show that both claims cannot be true.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; What kind of complexity does the Chinese Room lack? By hypothesis it
</em><br />
<em class="quotelev1">&gt; can pass a Turing test: it has that much complexity in the sense of
</em><br />
<em class="quotelev1">&gt; outward performance. There is another way of thinking about
</em><br />
<em class="quotelev1">&gt; complexity: complexity of implementation. . So would the Chinese Room
</em><br />
<em class="quotelev1">&gt; be more convincing if it had a more complex algorithm? The problem
</em><br />
<em class="quotelev1">&gt; here is that there is a well-founded principle of computer science
</em><br />
<em class="quotelev1">&gt; according to which a computer programme of any complexity can emulated
</em><br />
<em class="quotelev1">&gt; by a particular type of essentially simple machine called a Turing
</em><br />
<em class="quotelev1">&gt; Machine. As it happens, the Chinese Room scenario matches a Turing
</em><br />
<em class="quotelev1">&gt; Machine pretty well. A Turing Machine has a simple active element, the
</em><br />
<em class="quotelev1">&gt; read-write head and a complex instruction table. In the Chinese Room
</em><br />
<em class="quotelev1">&gt; the sole active element is the operator, performing instruction by
</em><br />
<em class="quotelev1">&gt; rote; any further complexity is in the rulebooks. Since there is no
</em><br />
<em class="quotelev1">&gt; stated limit to the &quot;hardware&quot; of the Chinese Room -- the size of the
</em><br />
<em class="quotelev1">&gt; rulebook, the speed of the operator -- the CR could be modified to
</em><br />
<em class="quotelev1">&gt; implement more complex algorithms without changing any of the
</em><br />
<em class="quotelev1">&gt; essential features.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Of course differences in implementation could make all sorts of non-
</em><br />
<em class="quotelev1">&gt; computational differences. Dennett might think no amount of
</em><br />
<em class="quotelev1">&gt; computation will make a flight simulator fly. He might think that the
</em><br />
<em class="quotelev1">&gt; Chinese Room lack sensor and effectuators to interact with its
</em><br />
<em class="quotelev1">&gt; environment, and that such interactions are needed to solve the symbol-
</em><br />
<em class="quotelev1">&gt; grounding problem. He might think that implementational complexity,
</em><br />
<em class="quotelev1">&gt; hardware over software is what makes the difference between real
</em><br />
<em class="quotelev1">&gt; consciousness and zombiehood. And Searle might well agree with him on
</em><br />
<em class="quotelev1">&gt; all those points: he may not be a computationalist, but he is a
</em><br />
<em class="quotelev1">&gt; naturalist. The dichotomy is this: Denett's appeal to complexity is
</em><br />
<em class="quotelev1">&gt; either based on software, in which case it is implausible, being
</em><br />
<em class="quotelev1">&gt; undermined by Turing equivalence; or it is based in hardware, in which
</em><br />
<em class="quotelev1">&gt; case it is no disproof of Searle. Rather, Searle's argument can be
</em><br />
<em class="quotelev1">&gt; seen as a successful disproof of computationalism(ie the only-software-
</em><br />
<em class="quotelev1">&gt; matters approach) and Dennett's theory of consciousness is a proposal
</em><br />
<em class="quotelev1">&gt; for a non-computationalistic, hardware-based, robotic approach of the
</em><br />
<em class="quotelev1">&gt; kind Searle favours.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Some Denettians think a particular kind of hardware issue matters:
</em><br />
<em class="quotelev1">&gt; parallelism. The Chinese room is &quot;too simple&quot; in that it is a serial
</em><br />
<em class="quotelev1">&gt; processor. Parallel processors cannot in fact computer anything --
</em><br />
<em class="quotelev1">&gt; cannot solve any problem -- that single processors can't. So parallel
</em><br />
<em class="quotelev1">&gt; processing is a difference in implementation, not computation. What
</em><br />
<em class="quotelev1">&gt; parallel-processing hardware can do that serial hardware cannot is
</em><br />
<em class="quotelev1">&gt; perform opertations simultaneously. Whatever &quot;extra factor&quot; is added
</em><br />
<em class="quotelev1">&gt; by genuine simultaneity is not computational. Presumably that means it
</em><br />
<em class="quotelev1">&gt; would not show up in a Turing test -- it would be indetectable from
</em><br />
<em class="quotelev1">&gt; the outside. So the extra factor added by simultaneity is something
</em><br />
<em class="quotelev1">&gt; that works just like phenomenality. It is indescernable from the
</em><br />
<em class="quotelev1">&gt; outside, and it is capable of going missing while external
</em><br />
<em class="quotelev1">&gt; functionality is preserved. (We could switch a parallel processor off
</em><br />
<em class="quotelev1">&gt; during a TT and replace it with a computationally equivalent serial
</em><br />
<em class="quotelev1">&gt; one. According to the parallel processing claim, any genuine
</em><br />
<em class="quotelev1">&gt; cosnciousness would vanish, although the external examiner preforming
</em><br />
<em class="quotelev1">&gt; the TT would be none the wiser). In short, simulateneity implies
</em><br />
<em class="quotelev1">&gt; zombies.
</em><br />
<em class="quotelev1">&gt; The Chinese Room and Abstraction
</em><br />
<em class="quotelev1">&gt; Consider the argument that computer programmes are too abstract to
</em><br />
<em class="quotelev1">&gt; cause consciousness. Consider the counter-argument that a running
</em><br />
<em class="quotelev1">&gt; computer programme is a physical process and therefore not abstract at
</em><br />
<em class="quotelev1">&gt; all.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;   1. Computationalism in general associates that consciousness with a
</em><br />
<em class="quotelev1">&gt; specific comptuer programme, programme C let's say.
</em><br />
<em class="quotelev1">&gt;   2. Let us combine that with the further claim that programme C
</em><br />
<em class="quotelev1">&gt; causes cosnciousness, somehow leveraging the physical causality of the
</em><br />
<em class="quotelev1">&gt; hardware it is running on.
</em><br />
<em class="quotelev1">&gt;   3. A corrolary of that is that running programme C will always
</em><br />
<em class="quotelev1">&gt; cause the same effect.
</em><br />
<em class="quotelev1">&gt;   4. Running a programme on hardware is a physical process with
</em><br />
<em class="quotelev1">&gt; physical effects.
</em><br />
<em class="quotelev1">&gt;   5. It is in the nature of causality that the same kind of cause
</em><br />
<em class="quotelev1">&gt; produces the same kind of effects-- that is, causaliy attaches to
</em><br />
<em class="quotelev1">&gt; types not tokens.
</em><br />
<em class="quotelev1">&gt;   6. Running a programme on hardware will cause physical effects, and
</em><br />
<em class="quotelev1">&gt; these will be determined by the kind of physical hardware. (Valve
</em><br />
<em class="quotelev1">&gt; computers will generate heat, cogwheel computers will generate noise,
</em><br />
<em class="quotelev1">&gt; etc).
</em><br />
<em class="quotelev1">&gt;   7. Therefore, running programme C on different kinds of hardware
</em><br />
<em class="quotelev1">&gt; will not produce a uniform effect as required by 1.
</em><br />
<em class="quotelev1">&gt;   8. Programmes do not have a physical typology: they are not natural
</em><br />
<em class="quotelev1">&gt; kinds. In that sense they are abstract. (Arguably, that is not as
</em><br />
<em class="quotelev1">&gt; abstract as the square root of two, since they still have physical
</em><br />
<em class="quotelev1">&gt; tokens. There may be more than one kind or level of abstraction).
</em><br />
<em class="quotelev1">&gt;   9. Conclusion: even running programmes are not apt to cause
</em><br />
<em class="quotelev1">&gt; consciousness. They are still too abstract.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Computational Zombies
</em><br />
<em class="quotelev1">&gt; This argument explores the consequenes of two assumptions:
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;   1. We agree that Searle is right in his claim that software alone
</em><br />
<em class="quotelev1">&gt; is not able to bring about genuine intelligence,
</em><br />
<em class="quotelev1">&gt;   2. But continue to insist that AI research should nonetheless be
</em><br />
<em class="quotelev1">&gt; pursued with computers.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; In other words, we expect the success or failure of our AI to be
</em><br />
<em class="quotelev1">&gt; dependent on the choice of software in combination with the choice of
</em><br />
<em class="quotelev1">&gt; hardware.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; The external behaviour of a computational system -- software and
</em><br />
<em class="quotelev1">&gt; hardware taken together -- is basically detemined by the software it
</em><br />
<em class="quotelev1">&gt; is running; that is to say, while running a programme on different
</em><br />
<em class="quotelev1">&gt; hardware will make some kind of external differences, they tend to be
</em><br />
<em class="quotelev1">&gt; irrelevant and uninteresting differences such as the amount of heat
</em><br />
<em class="quotelev1">&gt; and noise generated. Behaviourstic tests like the Turing Test are
</em><br />
<em class="quotelev1">&gt; specifically designed to filter out such differences (so that the
</em><br />
<em class="quotelev1">&gt; examiner's prejudices about what kind of system could be conscious are
</em><br />
<em class="quotelev1">&gt; excluded). The questions and responses in a TT are just the inputs and
</em><br />
<em class="quotelev1">&gt; outputs of the software.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Abandoning the software-only approach for a combined software-and-
</em><br />
<em class="quotelev1">&gt; hardware approach has a peculiar consequence: that it is entirely
</em><br />
<em class="quotelev1">&gt; possible that out of two identically programmed systems running on
</em><br />
<em class="quotelev1">&gt; different hardware, one will be genuinely intelligent (or have genuine
</em><br />
<em class="quotelev1">&gt; consciousness, or genuine semantic comprehension, etc) and the other
</em><br />
<em class="quotelev1">&gt; will not. Yet, as we have seen above, these differences will be --
</em><br />
<em class="quotelev1">&gt; must be -- indiscernable in a Turing Test. Thus, if hardware is
</em><br />
<em class="quotelev1">&gt; involved in the implementation of AI in computers, the Turing Test
</em><br />
<em class="quotelev1">&gt; must be unreliable. There is a high probability that it will give
</em><br />
<em class="quotelev1">&gt; &quot;false positives&quot;, telling us that unconscious AIs are actually
</em><br />
<em class="quotelev1">&gt; conscious -- a probability that rises with the number of different
</em><br />
<em class="quotelev1">&gt; systems tested.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; To expand on the last point: suppose you get a positive TT result for
</em><br />
<em class="quotelev1">&gt; one system, A. Then suppose you duplicate the software onto a whole
</em><br />
<em class="quotelev1">&gt; bunch of different hardware platforms, B, C, D....
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; (Obviously, they are all assumed to be capable of running the software
</em><br />
<em class="quotelev1">&gt; in the first place). They must give the same results to the TT for A,
</em><br />
<em class="quotelev1">&gt; since they all run the same software, and since the software
</em><br />
<em class="quotelev1">&gt; determines the responses to a TT, as we established above, they must
</em><br />
<em class="quotelev1">&gt; give positive results. But eventually you will hit the wrong hardware
</em><br />
<em class="quotelev1">&gt; -- it would be too unlikely to always hit on the right hardware by
</em><br />
<em class="quotelev1">&gt; sheer chance, like throwing an endless series of heads. When you do
</em><br />
<em class="quotelev1">&gt; hit the wrong hardware, you get a false positive. (Actually you don't
</em><br />
<em class="quotelev1">&gt; know you got a true positive with A in the first place...)
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Thus, some AIs would be &quot;zombies&quot; in a restricted sense of &quot;zombie&quot;.
</em><br />
<em class="quotelev1">&gt; Whereas a zombie is normally thought of a physical duplicate lacking
</em><br />
<em class="quotelev1">&gt; consciousness, these are software duplicates lacking appropriate
</em><br />
<em class="quotelev1">&gt; hardware.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; This peculiar sitation comes about because of the separability of
</em><br />
<em class="quotelev1">&gt; software and hardware in a computational approach, and the further
</em><br />
<em class="quotelev1">&gt; separation of relevant and irrelvant behaviour in the Turing Test.
</em><br />
<em class="quotelev1">&gt; (The separability of software simply means the ability to run the same
</em><br />
<em class="quotelev1">&gt; software on differenet hardware). Physical systems in general -- non
</em><br />
<em class="quotelev1">&gt; computers, not susceptible to separate descriptions of hardware and
</em><br />
<em class="quotelev1">&gt; software -- do not have that separability. Their total behaviour is
</em><br />
<em class="quotelev1">&gt; determined by their total physical makeup. A kind of Articial
</em><br />
<em class="quotelev1">&gt; Intelligence that was basically non-computational would not be subject
</em><br />
<em class="quotelev1">&gt; to the Compuational Zombie problem. Searle is therefore correct to
</em><br />
<em class="quotelev1">&gt; maintain, as he does, that AI is broadly possible.
</em><br />
<em class="quotelev1">&gt; Neuron-silicon replacement scenarios
</em><br />
<em class="quotelev1">&gt; Chalmers claims that replacing neurons with silicon will preserve
</em><br />
<em class="quotelev1">&gt; qualia so long as it preserves function -- by which he means not just
</em><br />
<em class="quotelev1">&gt; outward, behavioural function but also the internal organisation that
</em><br />
<em class="quotelev1">&gt; produces it. Obviously, he has to make that stipulation because it is
</em><br />
<em class="quotelev1">&gt; possible to think of cases, such as Searle's Chinese Room, where
</em><br />
<em class="quotelev1">&gt; outward behaviour is generated by a very simplistic mechanism, such as
</em><br />
<em class="quotelev1">&gt; a lookup table. In fact, if one takes the idea that consciousness
</em><br />
<em class="quotelev1">&gt; supervenes on the functional to the extreme, it becomes practically
</em><br />
<em class="quotelev1">&gt; tautologous. The most fine-grained possible functional description
</em><br />
<em class="quotelev1">&gt; just is a physical description (assuming physics does not deliver
</em><br />
<em class="quotelev1">&gt; intrinsic properties, only structural/behavioural ones) , and the
</em><br />
<em class="quotelev1">&gt; mental supervenes in some sense on the physical, so consciousness can
</em><br />
<em class="quotelev1">&gt; hardly fail to supervene on an ultimately fine-grained functional
</em><br />
<em class="quotelev1">&gt; simulation. So the interesting question is what happens between these
</em><br />
<em class="quotelev1">&gt; two extremes at, say, the neuronal level.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; One could imagine a variation of the thought-experiment where one's
</em><br />
<em class="quotelev1">&gt; brain is first replaced at the fine-grained level, and the replaced
</em><br />
<em class="quotelev1">&gt; again with a coarser-grained version, and so, on, finishing in a Giant
</em><br />
<em class="quotelev1">&gt; Look Up Table. Since hardly anyone thinks a GLUT would have phenomenal
</em><br />
<em class="quotelev1">&gt; properties, phenomenality would presumably fade out. So there is no
</em><br />
<em class="quotelev1">&gt; rigid rule that phenomenality is preserved where functionality is
</em><br />
<em class="quotelev1">&gt; preserved.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; It is natural suppose that one's functional dispositions are in line
</em><br />
<em class="quotelev1">&gt; with one's qualia. One claims to see red because one is actually
</em><br />
<em class="quotelev1">&gt; seeing red. But an intuition that is founded on naturalness cannot be
</em><br />
<em class="quotelev1">&gt; readily carried across to the very unnaturual situation of having
</em><br />
<em class="quotelev1">&gt; one's brain gradually replaced.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; What is it like to have one's qualia fade away ? If one had ever been
</em><br />
<em class="quotelev1">&gt; a qualiaphile, one would continue to claim to have qualia, without
</em><br />
<em class="quotelev1">&gt; actually doing so. That is, one would be under an increasing series of
</em><br />
<em class="quotelev1">&gt; delusions. It is not difficult to imagine thought-experiments where
</em><br />
<em class="quotelev1">&gt; the victim's true beliefs are changed into false ones. For instance,
</em><br />
<em class="quotelev1">&gt; the Mad Scientist could transport the victim from their bedroom to a
</em><br />
<em class="quotelev1">&gt; &quot;Truman Show&quot; replica while they slept. Thus the victim's belief that
</em><br />
<em class="quotelev1">&gt; they were still in their own bedroom would be falsified. Since beliefs
</em><br />
<em class="quotelev1">&gt; refer to states-of-afairs outside the head, you don't even need to
</em><br />
<em class="quotelev1">&gt; change anything about someone's psychology to change the truth of
</em><br />
<em class="quotelev1">&gt; their beliefs. So there is no great problem with the idea that
</em><br />
<em class="quotelev1">&gt; rummaging in someone's head does change their beliefs -- any such
</em><br />
<em class="quotelev1">&gt; process must change beliefs relating to what is physically inside the
</em><br />
<em class="quotelev1">&gt; victims head. Since the victim is funtionally identical, they must
</em><br />
<em class="quotelev1">&gt; carry on believing they have neural tissue in their head, even after
</em><br />
<em class="quotelev1">&gt; it has all been replaced. It doesn't follow from this that replacing a
</em><br />
<em class="quotelev1">&gt; brain with silicon must destroy qualia, but there is definitely a
</em><br />
<em class="quotelev1">&gt; precedent for having false beliefs about one's own qualia after one's
</em><br />
<em class="quotelev1">&gt; brain has been tampered with.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; A GLUT of Turings
</em><br />
<em class="quotelev1">&gt; An old programmer's trick is to store &quot;potted&quot; results rather than
</em><br />
<em class="quotelev1">&gt; calculating them afresh each time. This saves tiem at the expense of
</em><br />
<em class="quotelev1">&gt; using up memory. Earlier, we used the idea of a &quot;Giant Look-Up Table&quot;
</em><br />
<em class="quotelev1">&gt; to implement, in an essentially dumb way, the whole of an extremely
</em><br />
<em class="quotelev1">&gt; coplicated system, such as a human brain.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Can a (Giant) Look-Up Table emulate any Turing Machine (and therefore,
</em><br />
<em class="quotelev1">&gt; any computer, and therefore, if computationalism is true, any brain).
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; The usual objection to LUT's is that they are stateless. But that is
</em><br />
<em class="quotelev1">&gt; easy get round. Add a timestamp as an additional input.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Or includde with the fresh input each time a record of all previous
</em><br />
<em class="quotelev1">&gt; conversations it has had, with the total table size limiting the
</em><br />
<em class="quotelev1">&gt; &quot;lifespan&quot; of the machine
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; The feedback of the old conversation gives the machine a state memory,
</em><br />
<em class="quotelev1">&gt; very straightforwardly is voluminously encoded
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; What is the LUT for a sorting algorithm?
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; It is a table which matches lists of unsorted numbers against sorted
</em><br />
<em class="quotelev1">&gt; numbers. it doesn't even need to be stateful. And, yes, if it is
</em><br />
<em class="quotelev1">&gt; finite it will only sort lists of up to some size limit. But then any
</em><br />
<em class="quotelev1">&gt; algorithm has to run for a finite length of time, and will not be able
</em><br />
<em class="quotelev1">&gt; to sort some lists in the time allowed. So time limits are just being
</em><br />
<em class="quotelev1">&gt; traded for space limits.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; If you want to pass a Turing test with a glut, you only need a coarse-
</em><br />
<em class="quotelev1">&gt; grained (but still huge) GLUT, that matches verbal resonses to verbal
</em><br />
<em class="quotelev1">&gt; inputs. (A GLUT that always produced the same response to the same
</em><br />
<em class="quotelev1">&gt; query would be quickly detected as a machine, so it would need the
</em><br />
<em class="quotelev1">&gt; statefullness trick, making it even larger...). However, it is
</em><br />
<em class="quotelev1">&gt; counterinutitive that such a GLUT would simulate thought since nothing
</em><br />
<em class="quotelev1">&gt; goes on between stimulus and response. Well, it is counterintuitive
</em><br />
<em class="quotelev1">&gt; that any GLUT would think or feel anything. Daryl McCullough and Dave
</em><br />
<em class="quotelev1">&gt; Chalmers chew the issue over in this extract from a Newsgroup
</em><br />
<em class="quotelev1">&gt; discussion.
</em><br />
<em class="quotelev1">&gt; Computationalism
</em><br />
<em class="quotelev1">&gt; Computationalism is the claim that the human mind is essentially a
</em><br />
<em class="quotelev1">&gt; computer. It can be picturesquely expressed in the &quot;yes, doctor&quot;
</em><br />
<em class="quotelev1">&gt; hypothesis -- the idea that, faced with a terminal disease, you would
</em><br />
<em class="quotelev1">&gt; consent to having your consciousness downloaded to a computer.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; There are two ambiguities in &quot;computationalism&quot; -- consciousness vs.
</em><br />
<em class="quotelev1">&gt; cognition, process vs programme -- leading to a total of four possible
</em><br />
<em class="quotelev1">&gt; meanings.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Most people would not say &quot;yes doctor&quot; to a process that recorded
</em><br />
<em class="quotelev1">&gt; their brain on a tape a left it in a filing cabinet. Yet, that is all
</em><br />
<em class="quotelev1">&gt; you can get out of the timeless world of Plato's heaven (programme vs
</em><br />
<em class="quotelev1">&gt; process).
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; That intuition is, I think, rather stronger than the intuition that
</em><br />
<em class="quotelev1">&gt; Maudlin's argument relies on: that consciousness supervenes only on
</em><br />
<em class="quotelev1">&gt; brain activity, not on counterfactuals.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; But the other ambiguity in computationalism offers another way out. If
</em><br />
<em class="quotelev1">&gt; only cognition supervenes on computational (and hence counterfactual)
</em><br />
<em class="quotelev1">&gt; activity, then consciousness could supervene on non-counterfactual
</em><br />
<em class="quotelev1">&gt; activity -- i.e they could both supervene on physical processes, but
</em><br />
<em class="quotelev1">&gt; in different ways.
</em><br />
<em class="quotelev1">&gt; Aritifical intelligence and emotion
</em><br />
<em class="quotelev1">&gt; AI enthusiasts are much taken with the analogy between the brain's
</em><br />
<em class="quotelev1">&gt; (electro) chemical activity and the electrical nature of most current
</em><br />
<em class="quotelev1">&gt; computers. But brains are not entirely electrical. Neurons sit in a
</em><br />
<em class="quotelev1">&gt; bath of chemicals which effects their behaviour, too. Adrenaline, sex
</em><br />
<em class="quotelev1">&gt; hormones, recreational drugs all affect the brain. Why are AI
</em><br />
<em class="quotelev1">&gt; proponents so unconcerned about brain chemistry? Is it because they
</em><br />
<em class="quotelev1">&gt; are so enamoured with the electrical analogy? Or because they just
</em><br />
<em class="quotelev1">&gt; aren't that interested in emotion?
</em><br />
<em class="quotelev1">&gt; Platonic computationalism -- are computers numbers?
</em><br />
<em class="quotelev1">&gt; Any computer programme (in a particular computer) is a long sequence
</em><br />
<em class="quotelev1">&gt; of 1's and 0's, and therefore, a long number. According to Platonism,
</em><br />
<em class="quotelev1">&gt; numbers exist immaterially in &quot;Plato's Heaven&quot;. If programmes are
</em><br />
<em class="quotelev1">&gt; numbers, does that mean Plato's heaven is populated with computer
</em><br />
<em class="quotelev1">&gt; programmes?
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; The problem, as we shall see is the &quot;in a a particular computer&quot;
</em><br />
<em class="quotelev1">&gt; clause.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; As Bruno Marchal states the claim in a more formal language:
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; &quot;Of course I can [identify programmes with numbers ]. This is a key
</em><br />
<em class="quotelev1">&gt; point, and it is not obvious. But I can, and the main reason is Church
</em><br />
<em class="quotelev1">&gt; Thesis (CT). Fix any universal machine, then, by CT, all partial
</em><br />
<em class="quotelev1">&gt; computable function can be arranged in a recursively enumerable list
</em><br />
<em class="quotelev1">&gt; F1, F2, F3, F4, F5, etc. &quot;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Of course you can count or enumerate machines or algorithms, i.e.
</em><br />
<em class="quotelev1">&gt; attach unique numerical labels to them. The problem is in your &quot;Fix
</em><br />
<em class="quotelev1">&gt; any universal machine&quot;. Given a string of 1's and 0s wihouta universal
</em><br />
<em class="quotelev1">&gt; machine, and you have no idea of which algorithm (non-universal
</em><br />
<em class="quotelev1">&gt; machine) it is. Two things are only identical if they have all*their
</em><br />
<em class="quotelev1">&gt; properties in common (Leibniz's law). But none of the propeties of the
</em><br />
<em class="quotelev1">&gt; &quot;machine&quot; are detectable in the number itself.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; (You can also count the even numbers off against the odd numbers , but
</em><br />
<em class="quotelev1">&gt; that hardly means that even numbers are identical to odd numbers!)
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; &quot;In computer science, a fixed universal machine plays the role of a
</em><br />
<em class="quotelev1">&gt; coordinate system in geometry. That's all. With Church Thesis, we
</em><br />
<em class="quotelev1">&gt; don't even have to name the particular universal machine, it could be
</em><br />
<em class="quotelev1">&gt; a universal cellular automaton (like the game of life), or Python,
</em><br />
<em class="quotelev1">&gt; Robinson Aritmetic, Matiyasevich Diophantine universal polynomial,
</em><br />
<em class="quotelev1">&gt; Java, ... rational complex unitary matrices, universal recursive group
</em><br />
<em class="quotelev1">&gt; or ring, billiard ball, whatever.&quot;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Ye-e-es. But if all this is taking place in Platonia, the only thing
</em><br />
<em class="quotelev1">&gt; it can be is a number. But that number can't be associated with a
</em><br />
<em class="quotelev1">&gt; computaiton by another machine, or you get infinite regress.
</em><br />
<em class="quotelev1">&gt; Is the computationalist claim trivial -- are all systems computers?
</em><br />
<em class="quotelev1">&gt; It can be argued that any physical theory involving real numbers poses
</em><br />
<em class="quotelev1">&gt; problems (and all major theories do, at the time of writing). Known
</em><br />
<em class="quotelev1">&gt; physics is held to be computable, but that statement needs to be
</em><br />
<em class="quotelev1">&gt; qualified in various ways. A number thinking particularly of a real
</em><br />
<em class="quotelev1">&gt; number, one with an infinite number of digits -- is said to be
</em><br />
<em class="quotelev1">&gt; computable if a Turing machine will continue to spit out digits
</em><br />
<em class="quotelev1">&gt; endlessly. In other words, there is no question of getting to the
</em><br />
<em class="quotelev1">&gt; &quot;last digit&quot;. But this sits uncomfortably with the idea of simulating
</em><br />
<em class="quotelev1">&gt; physics in real time (or any plausible kind of time). Known physical
</em><br />
<em class="quotelev1">&gt; laws (including those of quantum mechanics) are very much infused with
</em><br />
<em class="quotelev1">&gt; real numbers and continua.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;    &quot;So ordinary computational descriptions do not have a cardinality
</em><br />
<em class="quotelev1">&gt; of states and state space trajectories hat is sufficient for them to
</em><br />
<em class="quotelev1">&gt; map onto ordinary mathematical descriptions of natural systems. Thus,
</em><br />
<em class="quotelev1">&gt; from the point of view of strict mathematical description, the thesis
</em><br />
<em class="quotelev1">&gt; that everything is a computing system in this second sense cannot be
</em><br />
<em class="quotelev1">&gt; supported&quot;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Moreover, the universe seems to be able decide on their values on a
</em><br />
<em class="quotelev1">&gt; moment-by-moment basis. As Richard Feynman put it: &quot;It always bothers
</em><br />
<em class="quotelev1">&gt; me that, according to the laws as we understand them today, it takes a
</em><br />
<em class="quotelev1">&gt; computing machine an infinite number of logical operations to figure
</em><br />
<em class="quotelev1">&gt; out what goes on in no matter how tiny a region of space, and no
</em><br />
<em class="quotelev1">&gt; matter how tiny a region of time. How can all that be going on in that
</em><br />
<em class="quotelev1">&gt; tiny space? Why should it take an infinite amount of logic to figure
</em><br />
<em class="quotelev1">&gt; out what one tiny piece of space/time is going to do?
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; However, he went on to say:
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; So I have often made the hypotheses that ultimately physics will not
</em><br />
<em class="quotelev1">&gt; require a mathematical statement, that in the end the machinery will
</em><br />
<em class="quotelev1">&gt; be revealed, and the laws will turn out to be simple, like the chequer
</em><br />
<em class="quotelev1">&gt; board with all its apparent complexities. But this speculation is of
</em><br />
<em class="quotelev1">&gt; the same nature as those other people make I like it, I dont like it,
</em><br />
<em class="quotelev1">&gt; and it is not good to be prejudiced about these things&quot;.
</em><br />
<em class="quotelev1">&gt; Is no physical system a computer, except in the eye of the beholder
</em><br />
<em class="quotelev1">&gt; Comsider the claim that &quot;computation&quot; may not correctly be ascribed to
</em><br />
<em class="quotelev1">&gt; the physics per se. Maybe it can be ascribed as an heuristic device as
</em><br />
<em class="quotelev1">&gt; physical explanation has an algorithmic component as Wolfram suggests.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Whether everything physical is computational or whether specific
</em><br />
<em class="quotelev1">&gt; physical systems are computational are two quite different questions.
</em><br />
<em class="quotelev1">&gt; As far as I can see, a NAND's gate being a NAND gate is just as
</em><br />
<em class="quotelev1">&gt; objective as a square thing's being square.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Are the computations themselves part of the purely physical story of
</em><br />
<em class="quotelev1">&gt; what is going on inside a compter?
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Seen mathematically, they have to be part of the physical story. They
</em><br />
<em class="quotelev1">&gt; are not some non-physical aura hanging over it. A computer doing
</em><br />
<em class="quotelev1">&gt; something semantic like word-processing needs external interpretation
</em><br />
<em class="quotelev1">&gt; in the way anything semantic does: there is nothing intrinsic and
</em><br />
<em class="quotelev1">&gt; objective about a mark that makes it a sign standing for something.
</em><br />
<em class="quotelev1">&gt; But that is down to semantics, not computation. Whilst we don't expect
</em><br />
<em class="quotelev1">&gt; the sign &quot;dog&quot; to be understood universally, we regard mathematics as
</em><br />
<em class="quotelev1">&gt; a universal language, so we put things like
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; | || ||| ||||
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; on space probes, expecting them to be understood. But an entity that
</em><br />
<em class="quotelev1">&gt; can understand a basic numeric sequence could understand a basic
</em><br />
<em class="quotelev1">&gt; mathematical function. So taking our best guesses about
</em><br />
<em class="quotelev1">&gt; intersubjective comprehensibility to stand for objectivity,
</em><br />
<em class="quotelev1">&gt; mathematical computation is objective.
</em><br />
<em class="quotelev1">&gt; Is hypercomoutation a testable hypothesis? We can decide between non-
</em><br />
<em class="quotelev1">&gt; computable physics (CM) and computable physics (QM). What the question
</em><br />
<em class="quotelev1">&gt; hinges on is the different kinds and levels of proof used in emprical
</em><br />
<em class="quotelev1">&gt; science and maths/logic.
</em><br />
<em class="quotelev1">&gt; Is Reality real ? Nick Bostrom's Simulation Argument
</em><br />
<em class="quotelev1">&gt; The Simulation Argument seeks to show that it is not just possible
</em><br />
<em class="quotelev1">&gt; that we are living inside a simulation, but likely.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; 1 You cannot simulate a world of X complexity inside a world of X
</em><br />
<em class="quotelev1">&gt; complexity.(quart-into-a-pint-pot-problem).
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; 02 Therefore, if we are in a simulation the 'real' world outside the
</em><br />
<em class="quotelev1">&gt; simulation is much more complex and quite possibly completely
</em><br />
<em class="quotelev1">&gt; different to the simulated world.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; 3 In which case, we cannot make sound inferences from the world we are
</em><br />
<em class="quotelev1">&gt; appear to be in to alleged real world in which the simulation is
</em><br />
<em class="quotelev1">&gt; running
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; 04 Therefore we cannot appeal to an argumentative apparatus of advanced
</em><br />
<em class="quotelev1">&gt; races, simulations etc, since all those concepts are derived from the
</em><br />
<em class="quotelev1">&gt; world as we see it -- which, by hypothesis is a mere simulation.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; 05 Therefore, the simulation argument pulls the metaphysical rug from
</em><br />
<em class="quotelev1">&gt; under its epistemological feet.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; The counterargument does not show that we are not living in a
</em><br />
<em class="quotelev1">&gt; simulation, but if we are , we have no way of knowing whether it is
</em><br />
<em class="quotelev1">&gt; likely or not. Even if it seems likely that we will go on to create
</em><br />
<em class="quotelev1">&gt; (sub) simulations, that does not mean we are living in a simulation
</em><br />
<em class="quotelev1">&gt; that is likely for the same reasons, since our simulation might be
</em><br />
<em class="quotelev1">&gt; rare and peculiar. In particular, it might have the peculiarity that
</em><br />
<em class="quotelev1">&gt; sub-simulations are easy to create in it. For all we know our
</em><br />
<em class="quotelev1">&gt; simulators had extreme difficulty in creating our universe. In this
</em><br />
<em class="quotelev1">&gt; case, the fact that it is easy to create sub simulations within our
</em><br />
<em class="quotelev1">&gt; (supposed) simulation, does not mean it is easy to creae simulations
</em><br />
<em class="quotelev1">&gt; per se.
</em><br />
<em class="quotelev1">&gt; Computational counterfactuals, and the Computational-Platonic Argument
</em><br />
<em class="quotelev1">&gt; for Immaterial Minds
</em><br />
<em class="quotelev1">&gt; For one, there is the argument that: A computer programme is just a
</em><br />
<em class="quotelev1">&gt; long number, a string of 1's and 0's.
</em><br />
<em class="quotelev1">&gt; (All) numbers exist Platonically (according to Platonism)
</em><br />
<em class="quotelev1">&gt; Therefore, all programmes exist Platonically.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; A mind is special kind of programme (According to computaionalism)
</em><br />
<em class="quotelev1">&gt; All programmes exist Platonically (previous argument)
</em><br />
<em class="quotelev1">&gt; Therefore, all possible minds exist Platonically
</em><br />
<em class="quotelev1">&gt; Therefore, a physical universe is unnecessary -- our minds exist
</em><br />
<em class="quotelev1">&gt; already in the Platonic realm
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; The argument has a number of problems even allowing the assumptions of
</em><br />
<em class="quotelev1">&gt; Platonism, and computationalism.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; A programme is not the same thing as a process.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Computationalism refers to real, physical processes running on
</em><br />
<em class="quotelev1">&gt; material computers. Proponents of the argument need to show that the
</em><br />
<em class="quotelev1">&gt; causality and dynamism are inessential (that there is no relevant
</em><br />
<em class="quotelev1">&gt; difference between process and programme) before you can have
</em><br />
<em class="quotelev1">&gt; consciousness implemented Platonically.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; To exist Platonically is to exist eternally and necessarily. There is
</em><br />
<em class="quotelev1">&gt; no time or change in Plato's heave. Therefore, to &quot;gain entry&quot;, a
</em><br />
<em class="quotelev1">&gt; computational mind will have to be translated from a running process
</em><br />
<em class="quotelev1">&gt; into something static and acausal.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; One route is to replace the process with a programme. let's call this
</em><br />
<em class="quotelev1">&gt; the Programme approach.. After all, the programme does specify all the
</em><br />
<em class="quotelev1">&gt; possible counterfactual behaviour, and it is basically a string of 1's
</em><br />
<em class="quotelev1">&gt; and 0's, and therefore a suitable occupant of Plato's heaven. But a
</em><br />
<em class="quotelev1">&gt; specification of counterfactual behaviour is not actual counterfactual
</em><br />
<em class="quotelev1">&gt; behaviour. The information is the same, but they are not the same
</em><br />
<em class="quotelev1">&gt; thing.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; No-one would believe that a brain-scan, however detailed, is
</em><br />
<em class="quotelev1">&gt; conscious, so not computationalist, however ardent, is required to
</em><br />
<em class="quotelev1">&gt; believe that a progamme on a disk, gathering dust on a shelf, is
</em><br />
<em class="quotelev1">&gt; sentient, however good a piece of AI code it may be!
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Another route is &quot;record&quot; the actual behaviour, under some
</em><br />
<em class="quotelev1">&gt; circumstances of a process, into a stream of data (ultimately, a
</em><br />
<em class="quotelev1">&gt; string of numbers, and therefore something already in Plato's heaven).
</em><br />
<em class="quotelev1">&gt; Let's call this the Movie approach. This route loses the conditional
</em><br />
<em class="quotelev1">&gt; structure, the counterfactuals that are vital to computer programmes
</em><br />
<em class="quotelev1">&gt; and therefore to computationalism.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Computer programmes contain conditional (if-then) statements. A given
</em><br />
<em class="quotelev1">&gt; run of the programme will in general not explore every branch. yet the
</em><br />
<em class="quotelev1">&gt; unexplored branches are part of the programme. A branch of an if-then
</em><br />
<em class="quotelev1">&gt; statement that is not executed on a particular run of a programme will
</em><br />
<em class="quotelev1">&gt; constitute a counterfactual, a situation that could have happened but
</em><br />
<em class="quotelev1">&gt; didn't. Without counterfactuals you cannot tell which programme
</em><br />
<em class="quotelev1">&gt; (algorithm) a process is implementing because two algorithms could
</em><br />
<em class="quotelev1">&gt; have the same execution path but different unexecuted branches.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Since a &quot;recording&quot; is not computation as such, the computationalist
</em><br />
<em class="quotelev1">&gt; need not attribute mentality to it -- it need not have a mind of its
</em><br />
<em class="quotelev1">&gt; own, any more than the characters in a movie.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; (Another way of looking at this is via the Turing Test; a mere
</em><br />
<em class="quotelev1">&gt; recording would never pass a TT since it has no condiitonal/
</em><br />
<em class="quotelev1">&gt; counterfactual behaviour and therfore cannot answer unexpected
</em><br />
<em class="quotelev1">&gt; questions).
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; A third approach is make a movie of all possible computational
</em><br />
<em class="quotelev1">&gt; histories, and not just one. Let's call thsi the Many-Movie approach.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; In this case a computation would have to be associated with all
</em><br />
<em class="quotelev1">&gt; related branches in order to bring all the counterfactuals (or rather
</em><br />
<em class="quotelev1">&gt; conditionals) into a single computation.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; (IOW treating branches individually would fall back into the problems
</em><br />
<em class="quotelev1">&gt; of the Movie approach)
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; If a computation is associated with all branches, consciousness will
</em><br />
<em class="quotelev1">&gt; also be according to computationalism. That will bring on a White
</em><br />
<em class="quotelev1">&gt; Rabbit problem with a vengeance.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; However, it is not that computation cannot be associated with
</em><br />
<em class="quotelev1">&gt; counterfactuals in single-universe theories -- in the form of
</em><br />
<em class="quotelev1">&gt; unrealised possibilities, dispositions and so on. If consciousness
</em><br />
<em class="quotelev1">&gt; supervenes on computation , then it supervenes on such counterfactuals
</em><br />
<em class="quotelev1">&gt; too; this amounts to the response to Maudlin's argument in wihch the
</em><br />
<em class="quotelev1">&gt; physicalist abandons the claim that consciousness supervenes on
</em><br />
<em class="quotelev1">&gt; activity.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Of ocurse, unactualised possibilities in a single universe are never
</em><br />
<em class="quotelev1">&gt; going to lead to any White Rabbits!
</em><br />
<em class="quotelev1">&gt; Turing and Other Machines
</em><br />
<em class="quotelev1">&gt; Turing machines are the classical model of computation, but it is
</em><br />
<em class="quotelev1">&gt; doubtful whether they are the best model for human (or other organic)
</em><br />
<em class="quotelev1">&gt; intelligence. Turing machines take a fixed input, take as much time as
</em><br />
<em class="quotelev1">&gt; necessary to calculate a result, and produce a perfect result (in some
</em><br />
<em class="quotelev1">&gt; cases, they will carry on refining a result forever). Biological
</em><br />
<em class="quotelev1">&gt; survival is all about coming up with good-enough answers to a tight
</em><br />
<em class="quotelev1">&gt; timescale. Mistaking a shadow for a sabre-tooth tiger is a msitake,
</em><br />
<em class="quotelev1">&gt; but it is more accpetable than standing stock still calculating the
</em><br />
<em class="quotelev1">&gt; perfect interpretation of your visual information, only to ge eaten.
</em><br />
<em class="quotelev1">&gt; This doesn't put natural cognition beyone the bounds of computation,
</em><br />
<em class="quotelev1">&gt; but it does mean that the Turing Machine is not the ideal model.
</em><br />
<em class="quotelev1">&gt; Biological systems are more like real time systems, which have to
</em><br />
<em class="quotelev1">&gt; &quot;keep up&quot; with external events, at the expense of doing some things
</em><br />
<em class="quotelev1">&gt; imprefectly.
</em><br />
<em class="quotelev1">&gt; Quantum and Classical Computers
</em><br />
<em class="quotelev1">&gt; (Regarding David Deutsch's FoR)
</em><br />
<em class="quotelev1">&gt; To simulate a general quantum system with a classical computer you
</em><br />
<em class="quotelev1">&gt; need a number of bits that scales exponentially with the number of
</em><br />
<em class="quotelev1">&gt; qubits in the system. For a universal quantum computer the number of
</em><br />
<em class="quotelev1">&gt; qubits needed to simulate a system scales linearly with the number of
</em><br />
<em class="quotelev1">&gt; qubits in the system. So simulating quantum systems classically is
</em><br />
<em class="quotelev1">&gt; intractable, simulating quantum systems with a universal quantum
</em><br />
<em class="quotelev1">&gt; computer is tractable.
</em><br />
<em class="quotelev1">&gt; Time and Causality in Physics and Computation
</em><br />
<em class="quotelev1">&gt; The sum total of all the positions of particles of matter specififies
</em><br />
<em class="quotelev1">&gt; a (classical) physical state, but not how the state evolves. Thus it
</em><br />
<em class="quotelev1">&gt; seems that the universe cannot be built out of 0-width (in temporal
</em><br />
<em class="quotelev1">&gt; terms) slices alone. Physics needs to appeal to something else.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; There is one dualistic and two monistic solutions to this.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; The dualistic solution is that the universe consists (separately) of
</em><br />
<em class="quotelev1">&gt; states+the laws of universe. It is like a computer, where the data
</em><br />
<em class="quotelev1">&gt; (state) evolves according to the programme (laws).
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; One of the monistic solutions is to put more information into states.
</em><br />
<em class="quotelev1">&gt; Physics has an age old &quot;cheat&quot; of &quot;instantaneous velocities&quot;. This
</em><br />
<em class="quotelev1">&gt; gives more information about how the state will evolve. But the state
</em><br />
<em class="quotelev1">&gt; is no longer 0-width, it is infinitessimal.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Another example of states-without-laws is Julian Barbour's Platonia.
</em><br />
<em class="quotelev1">&gt; Full Newtonian mechanics cannot be recovered from his &quot;Machian&quot;
</em><br />
<em class="quotelev1">&gt; approach, but he thinks that what is lost (universes with overall
</em><br />
<em class="quotelev1">&gt; rotation and movement) is no loss.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; The other dualistic solution is the opposite of the second: laws-
</em><br />
<em class="quotelev1">&gt; without-states. For instance, Stephen Hawking's No Boundary Conditions
</em><br />
<em class="quotelev1">&gt; proposal
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Maudlin's Argument and Counterfactuals
</em><br />
<em class="quotelev1">&gt; We have already mentioned a parallel with computation. There is also
</em><br />
<em class="quotelev1">&gt; relevance to Tim Maudlin's claim that computationalism is incompatible
</em><br />
<em class="quotelev1">&gt; with physicalism. His argument hinges on serparating the activity of a
</em><br />
<em class="quotelev1">&gt; comptuaitonal system from its causal dispositions. Consciousness, says
</em><br />
<em class="quotelev1">&gt; Maudlin supervened on activity alone. Parts of an AI mechansim that
</em><br />
<em class="quotelev1">&gt; are not triggered into activity can be disabled without changing
</em><br />
<em class="quotelev1">&gt; consciousness. However, such disabling changes the computation being
</em><br />
<em class="quotelev1">&gt; performed, because programmes contain if-then statements only one
</em><br />
<em class="quotelev1">&gt; branch of which can be executed at a time. The other branch is a
</em><br />
<em class="quotelev1">&gt; &quot;counterfactual&quot;, as situation that could have happened but didn't.
</em><br />
<em class="quotelev1">&gt; Nonetheless, these counterfactuals are part of the algorithm. If
</em><br />
<em class="quotelev1">&gt; changing the algorithm doesn't change the conscious state (because it
</em><br />
<em class="quotelev1">&gt; only supervenes on the active parts of the process, not the unrealised
</em><br />
<em class="quotelev1">&gt; counterfactuals), consciousness does not supervene on computation.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; However, If causal dispositions are inextricably part of a physical
</em><br />
<em class="quotelev1">&gt; state, you can't separate activity from counterfactuals. Maudlin's
</em><br />
<em class="quotelev1">&gt; argument would then have to rely on disabling counterfactuals of a
</em><br />
<em class="quotelev1">&gt; specifically computational sort.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; We earlier stated that the dualistic solution is like the separation
</em><br />
<em class="quotelev1">&gt; between programme and data in a (conventional) computer programme.
</em><br />
<em class="quotelev1">&gt; However, AI-type programmes are typified by the fact that there is not
</em><br />
<em class="quotelev1">&gt; a barrier between code and programme -- AI software is self-modifying,
</em><br />
<em class="quotelev1">&gt; so it is its own data. Just as it is not physically necessary that
</em><br />
<em class="quotelev1">&gt; there is a clear distinction between states and laws (and thus a
</em><br />
<em class="quotelev1">&gt; separability of physical counterfactuals), so it isn't necessarily the
</em><br />
<em class="quotelev1">&gt; case that there is a clear distinciton between programme and data, and
</em><br />
<em class="quotelev1">&gt; thus a separability of computational counterfactuals. PDJ 19/8/06
</em><br />
<em class="quotelev1">&gt; Chalmers on GLUTS
</em><br />
<em class="quotelev1">&gt; Daryl McCullough writes:
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;    I made the split to satisfy *you*, Dave. In our discussion about
</em><br />
<em class="quotelev1">&gt; the table lookup program, your main argument against the table lookup
</em><br />
<em class="quotelev1">&gt; being conscious was the &quot;lack of richness&quot; of its thinking process.
</em><br />
<em class="quotelev1">&gt; And this lack of richness was revealed by the fact that it took zero
</em><br />
<em class="quotelev1">&gt; time to &quot;think&quot; about its inputs before it made its outputs. So I have
</em><br />
<em class="quotelev1">&gt; patched up this discrepancy by allowing &quot;silent&quot; transitions where
</em><br />
<em class="quotelev1">&gt; there is thinking, but no inputs. However, as I thought my example
</em><br />
<em class="quotelev1">&gt; showed, this silent, internal thinking can be perfectly trivial; as
</em><br />
<em class="quotelev1">&gt; simple as counting. It is therefore not clear to me in what sense
</em><br />
<em class="quotelev1">&gt; there can be more &quot;richness&quot; in some FSA's than there is in a table
</em><br />
<em class="quotelev1">&gt; lookup.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Dave Chalmers writes:
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;    I made it abundantly clear that the problem with the lookup table
</em><br />
<em class="quotelev1">&gt; is not the mere lack of silent transitions -- see my response to your
</em><br />
<em class="quotelev1">&gt; message about the brain that beeps upon every step. Rather, the
</em><br />
<em class="quotelev1">&gt; objection is that (a) a lot of conscious experience goes on between
</em><br />
<em class="quotelev1">&gt; any two statements I make in a conversation; and (b) it's very
</em><br />
<em class="quotelev1">&gt; implausible that a single state-transition could be responsible for
</em><br />
<em class="quotelev1">&gt; all that conscious experience.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;    Like the beeping brain, ordinary FSAs with null inputs and outputs
</em><br />
<em class="quotelev1">&gt; aren't vulnerable to this argument, as in those cases the richness of
</em><br />
<em class="quotelev1">&gt; such conscious experience need not result from a single state-
</em><br />
<em class="quotelev1">&gt; transition, but from a combination of many.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; DM:
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;    If you allow a &quot;null input&quot; to be a possible input, then the
</em><br />
<em class="quotelev1">&gt; humongous table lookup program becomes functionally equivalent to a
</em><br />
<em class="quotelev1">&gt; human brain. To see this, note that the states of the table lookup
</em><br />
<em class="quotelev1">&gt; program are essentially sequences of inputs [i_1,i_2,i_3,...,i_n]. We
</em><br />
<em class="quotelev1">&gt; use the mapping M([]) = the initial state, M([i_1,i_2, ..., i_n,i_{n
</em><br />
<em class="quotelev1">&gt; +1}]) = I(M([i_1,i_2, ..., i_n]),i_{n+1}). The output for state
</em><br />
<em class="quotelev1">&gt; [i_1,i_2, ..., i_n] is whatever the lookup table has for that sequence
</em><br />
<em class="quotelev1">&gt; of inputs, which is correct by the assumption that the table lookup
</em><br />
<em class="quotelev1">&gt; program gets the behavior right.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; DC:
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;    You made essentially this argument before, and I responded in a
</em><br />
<em class="quotelev1">&gt; message of Feb 28. Here's the relevant material:
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;    Your complaint about clocks, that they don't support
</em><br />
<em class="quotelev1">&gt; counterfactuals, is I think, easily corrected: for example, consider a
</em><br />
<em class="quotelev1">&gt; machine M with a state determined by a pair: the time, and the list of
</em><br />
<em class="quotelev1">&gt; all inputs ever made (with the times they were made). If
</em><br />
<em class="quotelev1">&gt; &quot;implementation&quot; simply means the existence of a mapping from the
</em><br />
<em class="quotelev1">&gt; physical system to the FSA, then it seems that such a system M would
</em><br />
<em class="quotelev1">&gt; simultaneously implement *every* FSA. Counterfactuals would be
</em><br />
<em class="quotelev1">&gt; covered, too.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;    This is an interesting example, which also came up in an e-mail
</em><br />
<em class="quotelev1">&gt; discussion recently. One trouble with the way you've phrased it is
</em><br />
<em class="quotelev1">&gt; that it doesn't support outputs (our FSAs have outputs as well as
</em><br />
<em class="quotelev1">&gt; inputs, potentially throughout their operation); but this can be fixed
</em><br />
<em class="quotelev1">&gt; by the usual &quot;humongous lookup table&quot; method. So what's to stop us
</em><br />
<em class="quotelev1">&gt; saying that a humongous lookup table doesn't implement any FSA to
</em><br />
<em class="quotelev1">&gt; which it's I/O equivalent? (You can think of the table as the
</em><br />
<em class="quotelev1">&gt; &quot;unrolled&quot; FSA, with new branches being created for each input. To map
</em><br />
<em class="quotelev1">&gt; FSA states to (big disjunctions of) table states, simply take the
</em><br />
<em class="quotelev1">&gt; image of any FSA state under the unrolling process.) This is a tricky
</em><br />
<em class="quotelev1">&gt; question. Perhaps the best answer is that it really doesn't have the
</em><br />
<em class="quotelev1">&gt; right state-transitional structure, as it can be in a given state
</em><br />
<em class="quotelev1">&gt; without producing the right output and transiting into the appropriate
</em><br />
<em class="quotelev1">&gt; next state, namely when it's at the end of the table. Of course this
</em><br />
<em class="quotelev1">&gt; won't work for the implementation of halting FSAs (i.e. ones that must
</em><br />
<em class="quotelev1">&gt; halt eventually, for any inputs, but one could argue that the FSA
</em><br />
<em class="quotelev1">&gt; which describes a human at a given time isn't a halting FSA (the human
</em><br />
<em class="quotelev1">&gt; itself might be halting, but that's because of extraneous influences
</em><br />
<em class="quotelev1">&gt; on the FSA). Your example above doesn't have the problem at the end of
</em><br />
<em class="quotelev1">&gt; the table; it just goes on building up its inputs forever, but at cost
</em><br />
<em class="quotelev1">&gt; of being able to produce the right outputs.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;    Not that I don't think lookup-tables pose some problems for
</em><br />
<em class="quotelev1">&gt; functionalism -- see my long response to Calvin Ostrum. But in any
</em><br />
<em class="quotelev1">&gt; case this is far from Putnam's pan-implementationalism.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; DM:
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;    The conclusion, whether you have silent transitions or not, is
</em><br />
<em class="quotelev1">&gt; that functional equivalence doesn't impose any significant constraints
</em><br />
<em class="quotelev1">&gt; on a system above and beyond those imposed by behavioral equivalence.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; DC:
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;    Even if your argument above were valid, this certainly wouldn't
</em><br />
<em class="quotelev1">&gt; follow -- the requirement that a system contains a humongous lookup
</em><br />
<em class="quotelev1">&gt; table is certainly a significant constraint! I also note that you've
</em><br />
<em class="quotelev1">&gt; made no response to my observation that your original example, even
</em><br />
<em class="quotelev1">&gt; with the silent transitions, is vastly constrained, about as
</em><br />
<em class="quotelev1">&gt; constrained as we'd expect an implementation to be.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev2">&gt; &gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<br /><br /><br /><pre>
-- 
All those moments will be lost in time, like tears in rain.
--~--~---------~--~----~------------~-------~--~----~
You received this message because you are subscribed to the Google Groups &quot;Everything List&quot; group.
To post to this group, send email to everything-list.domain.name.hidden
To unsubscribe from this group, send email to everything-list+unsubscribe.domain.name.hidden
For more options, visit this group at <a href="http://groups.google.com/group/everything-list?hl=en">http://groups.google.com/group/everything-list?hl=en</a>
-~----------~----~----~----~------~----~------~--~---
</pre>
<span id="received"><dfn>Received on</dfn> Thu Sep 03 2009 - 10:41:56 PDT</span>
</div>
<!-- body="end" -->
<div class="foot">
<map id="navbarfoot" name="navbarfoot" title="Related messages">
<ul class="links">
<li><dfn>This message</dfn>: [ <a href="#start17735">Message body</a> ]</li>
<!-- lnext="start" -->
<li><dfn>Next message</dfn>: <a href="17736.html" title="Next message in the list">Flammarion: "Re: Dreaming On"</a></li>
<li><dfn>Previous message</dfn>: <a href="17734.html" title="Previous message in the list">Flammarion: "Re: Dreaming On"</a></li>
<li><dfn>In reply to</dfn>: <a href="17734.html" title="Message to which this message replies">Flammarion: "Re: Dreaming On"</a></li>
<!-- lnextthread="start" -->
<li><dfn>Next in thread</dfn>: <a href="17736.html" title="Next message in this discussion thread">Flammarion: "Re: Dreaming On"</a></li>
<li><a name="replies" id="replies"></a>
<dfn>Reply</dfn>: <a href="17736.html" title="Message sent in reply to this message">Flammarion: "Re: Dreaming On"</a></li>
<!-- lreply="end" -->
</ul>
<ul class="links">
<li><a name="options3" id="options3"></a><dfn>Contemporary messages sorted</dfn>: [ <a href="date.html#msg17735" title="Contemporary messages by date">by date</a> ] [ <a href="index.html#msg17735" title="Contemporary discussion threads">by thread</a> ] [ <a href="subject.html#msg17735" title="Contemporary messages by subject">by subject</a> ] [ <a href="author.html#msg17735" title="Contemporary messages by author">by author</a> ] [ <a href="attachment.html" title="Contemporary messages by attachment">by messages with attachments</a> ]</li>
</ul>
</map>
</div>
<!-- trailer="footer" -->
<p><small><em>
This archive was generated by <a href="http://www.hypermail-project.org/">hypermail 2.3.0</a>
: Fri Feb 16 2018 - 13:20:16 PST
</em></small></p>
</body>
</html>
